{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d77e4e",
   "metadata": {},
   "source": [
    "# Legal Document Processing Pipeline - Text Refinement & JSON Conversion\n",
    "\n",
    "## **Overview**\n",
    "This notebook implements a comprehensive pipeline for processing legal documents (laws, decrees, and transitional provisions) from PDF format to structured JSON. The pipeline handles Mexican state legislation, performing text extraction, cleaning, structural analysis, and hierarchical parsing.\n",
    "\n",
    "## **Processing Workflow**\n",
    "1. **PDF Text Extraction** - Extract raw text from PDF documents\n",
    "2. **Text Cleaning** - Clean and normalize extracted text  \n",
    "3. **Document Splitting** - Separate decreto, ley, and transitorios sections\n",
    "4. **JSON Structuring** - Parse hierarchical structure (libros, títulos, capítulos, secciones, artículos)\n",
    "5. **Validation & Error Handling** - Detect structural issues and validate article sequences\n",
    "\n",
    "## **Input/Output**\n",
    "- **Input**: PDF files in `Raw/` directory with corresponding catalog CSV\n",
    "- **Output**: Structured JSON files with hierarchical legal document representation\n",
    "- **Logs**: Comprehensive error tracking and validation reports\n",
    "\n",
    "---\n",
    "\n",
    "## **Required Dependencies & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2097ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "from __future__ import annotations          # Enable forward type references\n",
    "import json                                 # JSON serialization for output files\n",
    "import re                                   # Regular expressions for text pattern matching\n",
    "import statistics                           # Statistical calculations for text analysis\n",
    "\n",
    "# Data structures and type hints\n",
    "from dataclasses import dataclass, field    # Structured data classes for law metadata\n",
    "from pathlib import Path                    # Cross-platform file system path handling\n",
    "from typing import Dict, List, Optional, Tuple, Union, Set, Any  # Type annotations\n",
    "\n",
    "# External libraries\n",
    "#%pip install PyMuPDF pandas unidecode\n",
    "import fitz                                 # PyMuPDF - PDF text extraction library\n",
    "import pandas as pd                         # Data manipulation and CSV handling\n",
    "from unidecode import unidecode             # Unicode normalization and accent removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85be6cf",
   "metadata": {},
   "source": [
    "## **Directory Structure & Data Flow Configuration**\n",
    "\n",
    "### **Input Directories**\n",
    "- **`Raw/`** - Source PDF files containing legal documents\n",
    "- **`index.csv`** - Catalog mapping file numbers to law metadata\n",
    "\n",
    "### **Processing Pipeline Directories**\n",
    "- **`temp/raw_txt/`** - Step 1: Raw text extracted from PDFs  \n",
    "- **`temp/clean/`** - Step 2: Cleaned and normalized text\n",
    "- **`leyes/`** - Step 3: Law sections (main legal content)\n",
    "- **`decretos/`** - Step 3: Decree sections (government decisions)\n",
    "- **`transitorios/`** - Step 3: Transitional provisions\n",
    "\n",
    "### **Output Directories**  \n",
    "- **`json/`** - Final structured JSON files\n",
    "- **`errores/`** - Error logs and validation reports\n",
    "\n",
    "This configuration ensures a clear separation of processing stages and enables easy debugging and quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88067e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created successfully!\n",
      "Base directory: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\n",
      "Input PDFs: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Raw\n",
      "Catalog: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Raw\\index.csv\n",
      "Final JSON output: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Refined\\json\n"
     ]
    }
   ],
   "source": [
    "# ============== Directory Configuration ==============\n",
    "\n",
    "# Base working directory (current notebook location)\n",
    "BASE_DIR     = Path.cwd()\n",
    "\n",
    "# === INPUT PATHS ===\n",
    "RAW_DIR      = BASE_DIR / \"Raw\"                 # Source PDF files location\n",
    "CATALOG_CSV  = RAW_DIR / \"index.csv\"           # Metadata catalog for PDFs\n",
    "\n",
    "# === OUTPUT ROOT ===\n",
    "OUTPUT_DIR   = BASE_DIR / \"Refined\"             # All processed outputs go here\n",
    "TEMP_DIR     = BASE_DIR / \"temp\"                # Temporary processing files\n",
    "\n",
    "# === INTERMEDIATE PROCESSING DIRECTORIES ===\n",
    "RAW_TXT_DIR  = OUTPUT_DIR / TEMP_DIR / \"raw_txt\"   # Step 1: Raw PDF text extraction\n",
    "CLEAN_DIR    = OUTPUT_DIR / TEMP_DIR / \"clean\"     # Step 2: Cleaned text files\n",
    "\n",
    "# === DOCUMENT TYPE SEPARATION (Step 3 outputs) ===\n",
    "LEY_DIR      = OUTPUT_DIR / \"leyes\"             # Main law content\n",
    "DECR_DIR     = OUTPUT_DIR / \"decretos\"          # Government decree sections  \n",
    "TRANS_DIR    = OUTPUT_DIR / \"transitorios\"      # Transitional provisions\n",
    "\n",
    "# === FINAL OUTPUTS ===\n",
    "JSON_DIR     = OUTPUT_DIR / \"json\"              # Structured JSON documents\n",
    "ERRORES_DIR  = OUTPUT_DIR / \"errores\"           # Error logs and validation reports\n",
    "\n",
    "# Create all necessary directories (parents=True creates nested paths)\n",
    "for d in [OUTPUT_DIR, RAW_TXT_DIR, TEMP_DIR, CLEAN_DIR, LEY_DIR, DECR_DIR, TRANS_DIR, JSON_DIR, ERRORES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directory structure created successfully!\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Input PDFs: {RAW_DIR}\")  \n",
    "print(f\"Catalog: {CATALOG_CSV}\")\n",
    "print(f\"Final JSON output: {JSON_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a13c62",
   "metadata": {},
   "source": [
    "## **Utility Functions & Text Processing Tools**\n",
    "\n",
    "### **Core Functionality**\n",
    "- **Text Normalization** - Clean and standardize text for consistent processing\n",
    "- **File I/O Operations** - Safe file writing with UTF-8 encoding\n",
    "- **Error Logging** - Structured error reporting with contextual information  \n",
    "- **Statistical Analysis** - Count lines, words, and characters for quality metrics\n",
    "- **String Sanitization** - Create safe filenames and normalize accented characters\n",
    "\n",
    "These utilities ensure robust text processing and provide comprehensive error tracking throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5059ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Text Processing Utilities ==============\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to URL-safe slug format.\n",
    "    - Removes accents using unidecode\n",
    "    - Converts to lowercase\n",
    "    - Replaces non-alphanumeric chars with underscores]\n",
    "    - Collapses multiple underscores to single ones\n",
    "    \"\"\"\n",
    "    s = unidecode(s).lower()                    # Remove accents, convert to lowercase\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)          # Replace non-alphanumeric with underscores\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"x\"  # Clean up multiple underscores\n",
    "\n",
    "def norm_lower(s: str) -> str:\n",
    "    \"\"\"Normalize text: remove accents, lowercase, collapse whitespace.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", unidecode(s).lower().strip())\n",
    "\n",
    "def caps_line(s: str) -> str:\n",
    "    \"\"\"Convert text to uppercase and remove accents for header matching.\"\"\"\n",
    "    return unidecode(s).upper().strip()\n",
    "\n",
    "def write_text(path: Path, content: str) -> None:\n",
    "    \"\"\"Safely write text content to file with UTF-8 encoding.\"\"\"\n",
    "    path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "def write_error(base: str, kind: str, message: str, extra: Dict | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Log structured error information to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        base: File identifier (e.g., '0001')\n",
    "        kind: Error category (e.g., 'catalog_missing', 'parse_error')\n",
    "        message: Human-readable error description\n",
    "        extra: Additional context data\n",
    "    \"\"\"\n",
    "    rec = {\"file\": base, \"kind\": kind, \"message\": message}\n",
    "    if extra:\n",
    "        rec.update(extra)\n",
    "    \n",
    "    # Create safe filename for error log\n",
    "    error_filename = f\"{slugify(base)}_{slugify(kind)}.json\"\n",
    "    (ERRORES_DIR / error_filename).write_text(\n",
    "        json.dumps(rec, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def count_stats(text: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Calculate text statistics for quality metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'lines', 'words', and 'chars' counts\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"lines\": text.count(\"\\n\") + (1 if text else 0),  # Count newlines + 1\n",
    "        \"words\": len(re.findall(r\"\\S+\", text)),           # Count non-whitespace sequences\n",
    "        \"chars\": len(text)                                # Total character count\n",
    "    }\n",
    "\n",
    "def _norm_caps(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Internal helper: normalize text to uppercase alphanumeric with spaces.\n",
    "    Used for header pattern matching.\n",
    "    \"\"\"\n",
    "    t = unidecode(s).upper()                    # Remove accents, uppercase\n",
    "    t = re.sub(r\"[^A-Z0-9]+\", \" \", t)          # Keep only letters/numbers\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()       # Collapse whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdd51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Law Metadata & Catalog Management ==============\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LawMeta:\n",
    "    \"\"\"\n",
    "    Immutable metadata container for legal documents.\n",
    "    \n",
    "    Attributes:\n",
    "        num_est: State number identifier\n",
    "        file_num: Zero-padded file number (e.g., '0001')\n",
    "        law_name: Full name of the law\n",
    "        link: Source URL or reference link\n",
    "        first_two_caps: Auto-generated uppercase version of first two words\n",
    "                       (used for document structure detection)\n",
    "    \"\"\"\n",
    "    num_est: str\n",
    "    file_num: str  \n",
    "    law_name: str\n",
    "    link: str\n",
    "    first_two_caps: str = field(init=False)  # Computed automatically\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Automatically extract and normalize the first two words of law name.\n",
    "        This is used for identifying the law title within document text.\n",
    "        \"\"\"\n",
    "        # Tokenize and normalize the law name\n",
    "        toks = [t for t in norm_lower(self.law_name).split() if t]\n",
    "        first_two = \" \".join(toks[:2]) if toks else \"\"\n",
    "        \n",
    "        # Set the computed field (frozen dataclass requires object.__setattr__)\n",
    "        object.__setattr__(self, \"first_two_caps\", first_two.upper())\n",
    "\n",
    "def load_catalog(path: Path) -> Dict[str, LawMeta]:\n",
    "    \"\"\"\n",
    "    Load law metadata from CSV catalog file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to catalog CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping file_num to LawMeta objects\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing from CSV\n",
    "    \"\"\"\n",
    "    # Read CSV with string dtype to preserve leading zeros\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
    "    df.columns = [c.lower() for c in df.columns]  # Normalize column names\n",
    "    \n",
    "    # Verify required columns exist\n",
    "    req = {\"num_est\", \"file_num\", \"law_name\", \"link\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV missing required columns: {miss}\")\n",
    "    \n",
    "    # Build catalog dictionary\n",
    "    out: Dict[str, LawMeta] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        meta = LawMeta(\n",
    "            num_est=(r[\"num_est\"] or \"\").strip(),\n",
    "            file_num=(r[\"file_num\"] or \"\").strip().zfill(4),  # Ensure 4-digit padding\n",
    "            law_name=(r[\"law_name\"] or \"\").strip(),\n",
    "            link=(r[\"link\"] or \"\").strip(),\n",
    "        )\n",
    "        out[meta.file_num] = meta\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9794bc0",
   "metadata": {},
   "source": [
    "## **Step 1: PDF Text Extraction**\n",
    "\n",
    "### **Purpose**\n",
    "Extract raw text content from PDF legal documents using PyMuPDF (fitz). This step converts binary PDF files into plain text while preserving layout and structure as much as possible.\n",
    "\n",
    "### **Process**\n",
    "1. **PDF Reading** - Open each PDF file in the Raw directory\n",
    "2. **Page-by-Page Extraction** - Extract text from each page sequentially  \n",
    "3. **Text Concatenation** - Combine all pages with double newlines as separators\n",
    "4. **File Output** - Save raw text to `temp/raw_txt/` directory\n",
    "5. **Statistics Tracking** - Record file processing metrics and errors\n",
    "\n",
    "### **Quality Control**\n",
    "- Validates files against catalog metadata\n",
    "- Tracks processing statistics (lines, words, characters)\n",
    "- Logs missing files and extraction errors\n",
    "- Generates manifest for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file using PyMuPDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file to process\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated text content from all pages\n",
    "        \n",
    "    Note:\n",
    "        - Processes pages sequentially to maintain document order\n",
    "        - Adds double newlines between pages for section separation\n",
    "        - Handles multi-page documents automatically\n",
    "    \"\"\"\n",
    "    with fitz.open(pdf_path) as pdf_file:\n",
    "        text_content = \"\"\n",
    "        \n",
    "        # Process each page in order\n",
    "        for page_num in range(len(pdf_file)):\n",
    "            page = pdf_file[page_num]\n",
    "            text = page.get_text()                    # Extract plain text\n",
    "            text_content += text + \"\\n\\n\"             # Add page separator\n",
    "\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa138b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Step 1: PDF Text Extraction...\n",
      "Found 301 PDF files to process\n",
      "Catalog contains 301 entries\n",
      "Processing 0001.pdf...\n",
      "Processing 0002.pdf...\n",
      "Processing 0003.pdf...\n",
      "Processing 0004.pdf...\n",
      "Processing 0005.pdf...\n",
      "Processing 0006.pdf...\n",
      "Processing 0007.pdf...\n",
      "Processing 0008.pdf...\n",
      "Processing 0009.pdf...\n",
      "Processing 0010.pdf...\n",
      "Processing 0011.pdf...\n",
      "Processing 0012.pdf...\n",
      "Processing 0013.pdf...\n",
      "Processing 0014.pdf...\n",
      "Processing 0015.pdf...\n",
      "Processing 0016.pdf...\n",
      "Processing 0017.pdf...\n",
      "Processing 0018.pdf...\n",
      "Processing 0019.pdf...\n",
      "Processing 0020.pdf...\n",
      "Processing 0021.pdf...\n",
      "Processing 0022.pdf...\n",
      "Processing 0023.pdf...\n",
      "Processing 0024.pdf...\n",
      "Processing 0025.pdf...\n",
      "Processing 0026.pdf...\n",
      "Processing 0027.pdf...\n",
      "Processing 0028.pdf...\n",
      "Processing 0029.pdf...\n",
      "Processing 0030.pdf...\n",
      "Processing 0031.pdf...\n",
      "Processing 0032.pdf...\n",
      "Processing 0033.pdf...\n",
      "Processing 0034.pdf...\n",
      "Processing 0035.pdf...\n",
      "Processing 0036.pdf...\n",
      "Processing 0037.pdf...\n",
      "Processing 0038.pdf...\n",
      "Processing 0039.pdf...\n",
      "Processing 0040.pdf...\n",
      "Processing 0041.pdf...\n",
      "Processing 0042.pdf...\n",
      "Processing 0043.pdf...\n",
      "Processing 0044.pdf...\n",
      "Processing 0045.pdf...\n",
      "Processing 0046.pdf...\n",
      "Processing 0047.pdf...\n",
      "Processing 0048.pdf...\n",
      "Processing 0049.pdf...\n",
      "Processing 0050.pdf...\n",
      "Processing 0051.pdf...\n",
      "Processing 0052.pdf...\n",
      "Processing 0053.pdf...\n",
      "Processing 0054.pdf...\n",
      "Processing 0055.pdf...\n",
      "Processing 0056.pdf...\n",
      "Processing 0057.pdf...\n",
      "Processing 0058.pdf...\n",
      "Processing 0059.pdf...\n",
      "Processing 0060.pdf...\n",
      "Processing 0061.pdf...\n",
      "Processing 0062.pdf...\n",
      "Processing 0063.pdf...\n",
      "Processing 0064.pdf...\n",
      "Processing 0065.pdf...\n",
      "Processing 0066.pdf...\n",
      "Processing 0067.pdf...\n",
      "Processing 0068.pdf...\n",
      "Processing 0069.pdf...\n",
      "Processing 0070.pdf...\n",
      "Processing 0071.pdf...\n",
      "Processing 0072.pdf...\n",
      "Processing 0073.pdf...\n",
      "Processing 0074.pdf...\n",
      "Processing 0075.pdf...\n",
      "Processing 0076.pdf...\n",
      "Processing 0077.pdf...\n",
      "Processing 0078.pdf...\n",
      "Processing 0079.pdf...\n",
      "Processing 0080.pdf...\n",
      "Processing 0081.pdf...\n",
      "Processing 0082.pdf...\n",
      "Processing 0083.pdf...\n",
      "Processing 0084.pdf...\n",
      "Processing 0085.pdf...\n",
      "Processing 0086.pdf...\n",
      "Processing 0087.pdf...\n",
      "Processing 0088.pdf...\n",
      "Processing 0089.pdf...\n",
      "Processing 0090.pdf...\n",
      "Processing 0091.pdf...\n",
      "Processing 0092.pdf...\n",
      "Processing 0093.pdf...\n",
      "Processing 0094.pdf...\n",
      "Processing 0095.pdf...\n",
      "Processing 0096.pdf...\n",
      "Processing 0097.pdf...\n",
      "Processing 0098.pdf...\n",
      "Processing 0099.pdf...\n",
      "Processing 0100.pdf...\n",
      "Processing 0101.pdf...\n",
      "Processing 0102.pdf...\n",
      "Processing 0103.pdf...\n",
      "Processing 0104.pdf...\n",
      "Processing 0105.pdf...\n",
      "Processing 0106.pdf...\n",
      "Processing 0107.pdf...\n",
      "Processing 0108.pdf...\n",
      "Processing 0109.pdf...\n",
      "Processing 0110.pdf...\n",
      "Processing 0111.pdf...\n",
      "Processing 0112.pdf...\n",
      "Processing 0113.pdf...\n",
      "Processing 0114.pdf...\n",
      "Processing 0115.pdf...\n",
      "Processing 0116.pdf...\n",
      "Processing 0117.pdf...\n",
      "Processing 0118.pdf...\n",
      "Processing 0119.pdf...\n",
      "Processing 0120.pdf...\n",
      "Processing 0121.pdf...\n",
      "Processing 0122.pdf...\n",
      "Processing 0123.pdf...\n",
      "Processing 0124.pdf...\n",
      "Processing 0125.pdf...\n",
      "Processing 0126.pdf...\n",
      "Processing 0127.pdf...\n",
      "Processing 0128.pdf...\n",
      "Processing 0129.pdf...\n",
      "Processing 0130.pdf...\n",
      "Processing 0131.pdf...\n",
      "Processing 0132.pdf...\n",
      "Processing 0133.pdf...\n",
      "Processing 0134.pdf...\n",
      "Processing 0135.pdf...\n",
      "Processing 0136.pdf...\n",
      "Processing 0137.pdf...\n",
      "Processing 0138.pdf...\n",
      "Processing 0139.pdf...\n",
      "Processing 0140.pdf...\n",
      "Processing 0141.pdf...\n",
      "Processing 0142.pdf...\n",
      "Processing 0143.pdf...\n",
      "Processing 0144.pdf...\n",
      "Processing 0145.pdf...\n",
      "Processing 0146.pdf...\n",
      "Processing 0147.pdf...\n",
      "Processing 0148.pdf...\n",
      "Processing 0149.pdf...\n",
      "Processing 0150.pdf...\n",
      "Processing 0151.pdf...\n",
      "Processing 0152.pdf...\n",
      "Processing 0153.pdf...\n",
      "Processing 0154.pdf...\n",
      "Processing 0155.pdf...\n",
      "Processing 0156.pdf...\n",
      "Processing 0157.pdf...\n",
      "Processing 0158.pdf...\n",
      "Processing 0159.pdf...\n",
      "Processing 0160.pdf...\n",
      "Processing 0161.pdf...\n",
      "Processing 0162.pdf...\n",
      "Processing 0163.pdf...\n",
      "Processing 0164.pdf...\n",
      "Processing 0165.pdf...\n",
      "Processing 0166.pdf...\n",
      "Processing 0167.pdf...\n",
      "Processing 0168.pdf...\n",
      "Processing 0169.pdf...\n",
      "Processing 0170.pdf...\n",
      "Processing 0171.pdf...\n",
      "Processing 0172.pdf...\n",
      "Processing 0173.pdf...\n",
      "Processing 0174.pdf...\n",
      "Processing 0175.pdf...\n",
      "Processing 0176.pdf...\n",
      "Processing 0177.pdf...\n",
      "Processing 0178.pdf...\n",
      "Processing 0179.pdf...\n",
      "Processing 0180.pdf...\n",
      "Processing 0181.pdf...\n",
      "Processing 0182.pdf...\n",
      "Processing 0183.pdf...\n",
      "Processing 0184.pdf...\n",
      "Processing 0185.pdf...\n",
      "Processing 0186.pdf...\n",
      "Processing 0187.pdf...\n",
      "Processing 0188.pdf...\n",
      "Processing 0189.pdf...\n",
      "Processing 0190.pdf...\n",
      "Processing 0191.pdf...\n",
      "Processing 0192.pdf...\n",
      "Processing 0193.pdf...\n",
      "Processing 0194.pdf...\n",
      "Processing 0195.pdf...\n",
      "Processing 0196.pdf...\n",
      "Processing 0197.pdf...\n",
      "Processing 0198.pdf...\n",
      "Processing 0199.pdf...\n",
      "Processing 0200.pdf...\n",
      "Processing 0201.pdf...\n",
      "Processing 0202.pdf...\n",
      "Processing 0203.pdf...\n",
      "Processing 0204.pdf...\n",
      "Processing 0205.pdf...\n",
      "Processing 0206.pdf...\n",
      "Processing 0207.pdf...\n",
      "Processing 0208.pdf...\n",
      "Processing 0209.pdf...\n",
      "Processing 0210.pdf...\n",
      "Processing 0211.pdf...\n",
      "Processing 0212.pdf...\n",
      "Processing 0213.pdf...\n",
      "Processing 0214.pdf...\n",
      "Processing 0215.pdf...\n",
      "Processing 0216.pdf...\n",
      "Processing 0217.pdf...\n",
      "Processing 0218.pdf...\n",
      "Processing 0219.pdf...\n",
      "Processing 0220.pdf...\n",
      "Processing 0221.pdf...\n",
      "Processing 0222.pdf...\n",
      "Processing 0223.pdf...\n",
      "Processing 0224.pdf...\n",
      "Processing 0225.pdf...\n",
      "Processing 0226.pdf...\n",
      "Processing 0227.pdf...\n",
      "Processing 0228.pdf...\n",
      "Processing 0229.pdf...\n",
      "Processing 0230.pdf...\n",
      "Processing 0231.pdf...\n",
      "Processing 0232.pdf...\n",
      "Processing 0233.pdf...\n",
      "Processing 0234.pdf...\n",
      "Processing 0235.pdf...\n",
      "Processing 0236.pdf...\n",
      "Processing 0237.pdf...\n",
      "Processing 0238.pdf...\n",
      "Processing 0239.pdf...\n",
      "Processing 0240.pdf...\n",
      "Processing 0241.pdf...\n",
      "Processing 0242.pdf...\n",
      "Processing 0243.pdf...\n",
      "Processing 0244.pdf...\n",
      "Processing 0245.pdf...\n",
      "Processing 0246.pdf...\n",
      "Processing 0247.pdf...\n",
      "Processing 0248.pdf...\n",
      "Processing 0249.pdf...\n",
      "Processing 0250.pdf...\n",
      "Processing 0251.pdf...\n",
      "Processing 0252.pdf...\n",
      "Processing 0253.pdf...\n",
      "Processing 0254.pdf...\n",
      "Processing 0255.pdf...\n",
      "Processing 0256.pdf...\n",
      "Processing 0257.pdf...\n",
      "Processing 0258.pdf...\n",
      "Processing 0259.pdf...\n",
      "Processing 0260.pdf...\n",
      "Processing 0261.pdf...\n",
      "Processing 0262.pdf...\n",
      "Processing 0263.pdf...\n",
      "Processing 0264.pdf...\n",
      "Processing 0265.pdf...\n",
      "Processing 0266.pdf...\n",
      "Processing 0267.pdf...\n",
      "Processing 0268.pdf...\n",
      "Processing 0269.pdf...\n",
      "Processing 0270.pdf...\n",
      "Processing 0271.pdf...\n",
      "Processing 0272.pdf...\n",
      "Processing 0273.pdf...\n",
      "Processing 0274.pdf...\n",
      "Processing 0275.pdf...\n",
      "Processing 0276.pdf...\n",
      "Processing 0277.pdf...\n",
      "Processing 0278.pdf...\n",
      "Processing 0279.pdf...\n",
      "Processing 0280.pdf...\n",
      "Processing 0281.pdf...\n",
      "Processing 0282.pdf...\n",
      "Processing 0283.pdf...\n",
      "Processing 0284.pdf...\n",
      "Processing 0285.pdf...\n",
      "Processing 0286.pdf...\n",
      "Processing 0287.pdf...\n",
      "Processing 0288.pdf...\n",
      "Processing 0289.pdf...\n",
      "Processing 0290.pdf...\n",
      "Processing 0291.pdf...\n",
      "Processing 0292.pdf...\n",
      "Processing 0293.pdf...\n",
      "Processing 0294.pdf...\n",
      "Processing 0295.pdf...\n",
      "Processing 0296.pdf...\n",
      "Processing 0297.pdf...\n",
      "Processing 0298.pdf...\n",
      "Processing 0299.pdf...\n",
      "Processing 0300.pdf...\n",
      "Processing 0301.pdf...\n",
      "Manifest saved to c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Refined\\manifest_raw.csv\n",
      "Step 1 Complete: 301 raw txt files written to c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\temp\\raw_txt\n",
      "============================================================\n",
      "\n",
      "**Raw Text Extraction Summary:**\n",
      "   Total files processed: 301\n",
      "   Average lines per file: 4770.6\n",
      "   Average words per file: 21217.1\n",
      "   Total characters extracted: 44,660,830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>base</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>0001.pdf</td>\n",
       "      <td>0001</td>\n",
       "      <td>5445</td>\n",
       "      <td>52675</td>\n",
       "      <td>344041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>0002.pdf</td>\n",
       "      <td>0002</td>\n",
       "      <td>20438</td>\n",
       "      <td>170028</td>\n",
       "      <td>1073667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw</td>\n",
       "      <td>0003.pdf</td>\n",
       "      <td>0003</td>\n",
       "      <td>2140</td>\n",
       "      <td>15764</td>\n",
       "      <td>105782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw</td>\n",
       "      <td>0004.pdf</td>\n",
       "      <td>0004</td>\n",
       "      <td>1253</td>\n",
       "      <td>7528</td>\n",
       "      <td>51263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>0005.pdf</td>\n",
       "      <td>0005</td>\n",
       "      <td>1711</td>\n",
       "      <td>13228</td>\n",
       "      <td>89434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stage source_pdf  base  lines   words    chars\n",
       "0   raw   0001.pdf  0001   5445   52675   344041\n",
       "1   raw   0002.pdf  0002  20438  170028  1073667\n",
       "2   raw   0003.pdf  0003   2140   15764   105782\n",
       "3   raw   0004.pdf  0004   1253    7528    51263\n",
       "4   raw   0005.pdf  0005   1711   13228    89434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step1_extract_raw(catalog_csv: Path = CATALOG_CSV) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 1: Extract raw text from all PDF files in the Raw directory.\n",
    "    \n",
    "    Process:\n",
    "    1. Load catalog metadata for file validation\n",
    "    2. Find all PDF files in Raw directory  \n",
    "    3. Extract text from each PDF using PyMuPDF\n",
    "    4. Save raw text files to temp/raw_txt/ directory\n",
    "    5. Generate processing statistics and manifest\n",
    "    \n",
    "    Args:\n",
    "        catalog_csv: Path to catalog file for metadata validation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with processing statistics for each file\n",
    "    \"\"\"\n",
    "    print(\"Starting Step 1: PDF Text Extraction...\")\n",
    "    \n",
    "    # Load catalog for file validation and metadata\n",
    "    catalog = load_catalog(catalog_csv)\n",
    "    pdfs = sorted(RAW_DIR.glob(\"*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdfs)} PDF files to process\")\n",
    "    print(f\"Catalog contains {len(catalog)} entries\")\n",
    "\n",
    "    all_recs: List[Dict] = []\n",
    "    \n",
    "    for pdf_path in pdfs:\n",
    "        # Extract base filename (e.g., '0001.pdf' -> '0001')\n",
    "        base = pdf_path.stem.zfill(4) \n",
    "        \n",
    "        # Validate against catalog\n",
    "        if base not in catalog:\n",
    "            write_error(base, \"catalog_missing\", \n",
    "                       \"file_num not found in catalog\", \n",
    "                       {\"pdf\": pdf_path.name})\n",
    "            print(f\"WARNING: {pdf_path.name} not found in catalog - skipping\")\n",
    "            continue\n",
    "\n",
    "        # Extract text from PDF\n",
    "        print(f\"Processing {pdf_path.name}...\")\n",
    "        raw_layout = read_pdf(pdf_path)\n",
    "\n",
    "        # Save raw text output\n",
    "        raw_out = RAW_TXT_DIR / f\"raw_{base}.txt\"\n",
    "        write_text(raw_out, raw_layout)\n",
    "\n",
    "        # Record processing statistics\n",
    "        rec_raw = {\n",
    "            \"stage\": \"raw\",\n",
    "            \"source_pdf\": pdf_path.name,\n",
    "            \"base\": base\n",
    "        }\n",
    "        rec_raw.update(count_stats(raw_layout))  # Add line/word/char counts\n",
    "        all_recs.append(rec_raw)\n",
    "\n",
    "    # Create processing manifest\n",
    "    df = pd.DataFrame(all_recs)\n",
    "    if not df.empty:\n",
    "        manifest_path = OUTPUT_DIR / \"manifest_raw.csv\"\n",
    "        df.to_csv(manifest_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Manifest saved to {manifest_path}\")\n",
    "    \n",
    "    print(f\"Step 1 Complete: {len(df)} raw txt files written to {RAW_TXT_DIR}\")\n",
    "    return df\n",
    "\n",
    "# === EXECUTE STEP 1 ===\n",
    "print(\"=\" * 60)\n",
    "df_raw = step1_extract_raw(CATALOG_CSV)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display processing summary\n",
    "if not df_raw.empty:\n",
    "    print(\"\\n**Raw Text Extraction Summary:**\")\n",
    "    print(f\"   Total files processed: {len(df_raw)}\")\n",
    "    print(f\"   Average lines per file: {df_raw['lines'].mean():.1f}\")\n",
    "    print(f\"   Average words per file: {df_raw['words'].mean():.1f}\")\n",
    "    print(f\"   Total characters extracted: {df_raw['chars'].sum():,}\")\n",
    "    \n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5070",
   "metadata": {},
   "source": [
    "## **Step 2: Text Cleaning & Normalization**\n",
    "\n",
    "### **Purpose**\n",
    "Clean and normalize raw text extracted from PDFs to prepare for structural analysis. This step removes PDF artifacts, standardizes formatting, and improves text quality for downstream processing.\n",
    "\n",
    "### **Cleaning Operations**\n",
    "- **Line Break Normalization** - Convert Windows/Mac line endings to Unix format\n",
    "- **Whitespace Consolidation** - Collapse multiple spaces and excessive line breaks\n",
    "- **Paragraph Preservation** - Maintain paragraph structure while removing artifacts\n",
    "- **Title Matching** - Remove duplicate title lines that match catalog metadata\n",
    "\n",
    "### **Quality Improvements**\n",
    "- Removes PDF extraction artifacts and formatting inconsistencies\n",
    "- Preserves document structure and readability\n",
    "- Standardizes text encoding and character representation\n",
    "- Prepares text for accurate structural parsing in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f4576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Step 2: Text Cleaning & Normalization (one paragraph = one line)...\n",
      "[INFO] Catalog loaded: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Raw\\index.csv\n",
      "[INFO] Found 301 raw text files in c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\temp\\raw_txt\n",
      "[INFO] Cleaning raw_0001.txt …\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote clean_0001.txt (lines=1477, words=52266, chars=333390)\n",
      "[INFO] Cleaning raw_0002.txt …\n",
      "[INFO] Wrote clean_0002.txt (lines=6019, words=171856, chars=1037046)\n",
      "[INFO] Cleaning raw_0003.txt …\n",
      "[INFO] Wrote clean_0003.txt (lines=652, words=15815, chars=102309)\n",
      "[INFO] Cleaning raw_0004.txt …\n",
      "[INFO] Wrote clean_0004.txt (lines=300, words=7379, chars=48703)\n",
      "[INFO] Cleaning raw_0005.txt …\n",
      "[INFO] Wrote clean_0005.txt (lines=459, words=13085, chars=86294)\n",
      "[INFO] Cleaning raw_0006.txt …\n",
      "[INFO] Wrote clean_0006.txt (lines=2802, words=92858, chars=560122)\n",
      "[INFO] Cleaning raw_0007.txt …\n",
      "[INFO] Wrote clean_0007.txt (lines=3228, words=77099, chars=496470)\n",
      "[INFO] Cleaning raw_0008.txt …\n",
      "[INFO] Wrote clean_0008.txt (lines=1240, words=41662, chars=261846)\n",
      "[INFO] Cleaning raw_0009.txt …\n",
      "[INFO] Wrote clean_0009.txt (lines=2219, words=69867, chars=424016)\n",
      "[INFO] Cleaning raw_0010.txt …\n",
      "[INFO] Wrote clean_0010.txt (lines=2182, words=65971, chars=432822)\n",
      "[INFO] Cleaning raw_0011.txt …\n",
      "[INFO] Wrote clean_0011.txt (lines=727, words=20780, chars=141118)\n",
      "[INFO] Cleaning raw_0012.txt …\n",
      "[INFO] Wrote clean_0012.txt (lines=111, words=3480, chars=21841)\n",
      "[INFO] Cleaning raw_0013.txt …\n",
      "[INFO] Wrote clean_0013.txt (lines=697, words=19649, chars=125178)\n",
      "[INFO] Cleaning raw_0014.txt …\n",
      "[INFO] Wrote clean_0014.txt (lines=748, words=20459, chars=132855)\n",
      "[INFO] Cleaning raw_0015.txt …\n",
      "[INFO] Wrote clean_0015.txt (lines=36, words=825, chars=5217)\n",
      "[INFO] Cleaning raw_0016.txt …\n",
      "[INFO] Wrote clean_0016.txt (lines=28, words=497, chars=3088)\n",
      "[INFO] Cleaning raw_0017.txt …\n",
      "[INFO] Wrote clean_0017.txt (lines=704, words=19041, chars=126778)\n",
      "[INFO] Cleaning raw_0018.txt …\n",
      "[INFO] Wrote clean_0018.txt (lines=394, words=9848, chars=66755)\n",
      "[INFO] Cleaning raw_0019.txt …\n",
      "[INFO] Wrote clean_0019.txt (lines=465, words=22257, chars=142853)\n",
      "[INFO] Cleaning raw_0020.txt …\n",
      "[INFO] Wrote clean_0020.txt (lines=118, words=3426, chars=22243)\n",
      "[INFO] Cleaning raw_0021.txt …\n",
      "[INFO] Wrote clean_0021.txt (lines=97, words=2729, chars=16579)\n",
      "[INFO] Cleaning raw_0022.txt …\n",
      "[INFO] Wrote clean_0022.txt (lines=150, words=3138, chars=20502)\n",
      "[INFO] Cleaning raw_0023.txt …\n",
      "[INFO] Wrote clean_0023.txt (lines=519, words=13434, chars=86248)\n",
      "[INFO] Cleaning raw_0024.txt …\n",
      "[INFO] Wrote clean_0024.txt (lines=429, words=11329, chars=75151)\n",
      "[INFO] Cleaning raw_0025.txt …\n",
      "[INFO] Wrote clean_0025.txt (lines=840, words=28511, chars=184427)\n",
      "[INFO] Cleaning raw_0026.txt …\n",
      "[INFO] Wrote clean_0026.txt (lines=142, words=3336, chars=20927)\n",
      "[INFO] Cleaning raw_0027.txt …\n",
      "[INFO] Wrote clean_0027.txt (lines=294, words=9133, chars=57572)\n",
      "[INFO] Cleaning raw_0028.txt …\n",
      "[INFO] Wrote clean_0028.txt (lines=445, words=11820, chars=78220)\n",
      "[INFO] Cleaning raw_0029.txt …\n",
      "[INFO] Wrote clean_0029.txt (lines=160, words=3865, chars=25083)\n",
      "[INFO] Cleaning raw_0030.txt …\n",
      "[INFO] Wrote clean_0030.txt (lines=403, words=11376, chars=74383)\n",
      "[INFO] Cleaning raw_0031.txt …\n",
      "[INFO] Wrote clean_0031.txt (lines=304, words=7069, chars=47435)\n",
      "[INFO] Cleaning raw_0032.txt …\n",
      "[INFO] Wrote clean_0032.txt (lines=575, words=16192, chars=109655)\n",
      "[INFO] Cleaning raw_0033.txt …\n",
      "[INFO] Wrote clean_0033.txt (lines=435, words=9906, chars=63764)\n",
      "[INFO] Cleaning raw_0034.txt …\n",
      "[INFO] Wrote clean_0034.txt (lines=360, words=14950, chars=99442)\n",
      "[INFO] Cleaning raw_0035.txt …\n",
      "[INFO] Wrote clean_0035.txt (lines=1101, words=32667, chars=217223)\n",
      "[INFO] Cleaning raw_0036.txt …\n",
      "[INFO] Wrote clean_0036.txt (lines=486, words=20350, chars=135831)\n",
      "[INFO] Cleaning raw_0037.txt …\n",
      "[INFO] Wrote clean_0037.txt (lines=340, words=8376, chars=54496)\n",
      "[INFO] Cleaning raw_0038.txt …\n",
      "[INFO] Wrote clean_0038.txt (lines=133, words=3687, chars=23476)\n",
      "[INFO] Cleaning raw_0039.txt …\n",
      "[INFO] Wrote clean_0039.txt (lines=292, words=10173, chars=66135)\n",
      "[INFO] Cleaning raw_0040.txt …\n",
      "[INFO] Wrote clean_0040.txt (lines=76, words=2549, chars=15505)\n",
      "[INFO] Cleaning raw_0041.txt …\n",
      "[INFO] Wrote clean_0041.txt (lines=293, words=7225, chars=48421)\n",
      "[INFO] Cleaning raw_0042.txt …\n",
      "[INFO] Wrote clean_0042.txt (lines=222, words=5471, chars=35821)\n",
      "[INFO] Cleaning raw_0043.txt …\n",
      "[INFO] Wrote clean_0043.txt (lines=655, words=17497, chars=113154)\n",
      "[INFO] Cleaning raw_0044.txt …\n",
      "[INFO] Wrote clean_0044.txt (lines=420, words=9684, chars=61882)\n",
      "[INFO] Cleaning raw_0045.txt …\n",
      "[INFO] Wrote clean_0045.txt (lines=322, words=7043, chars=45732)\n",
      "[INFO] Cleaning raw_0046.txt …\n",
      "[INFO] Wrote clean_0046.txt (lines=399, words=9368, chars=59912)\n",
      "[INFO] Cleaning raw_0047.txt …\n",
      "[INFO] Wrote clean_0047.txt (lines=637, words=19627, chars=128108)\n",
      "[INFO] Cleaning raw_0048.txt …\n",
      "[INFO] Wrote clean_0048.txt (lines=540, words=14274, chars=93909)\n",
      "[INFO] Cleaning raw_0049.txt …\n",
      "[INFO] Wrote clean_0049.txt (lines=484, words=13148, chars=82427)\n",
      "[INFO] Cleaning raw_0050.txt …\n",
      "[INFO] Wrote clean_0050.txt (lines=1470, words=42875, chars=269208)\n",
      "[INFO] Cleaning raw_0051.txt …\n",
      "[INFO] Wrote clean_0051.txt (lines=74, words=1955, chars=12656)\n",
      "[INFO] Cleaning raw_0052.txt …\n",
      "[INFO] Wrote clean_0052.txt (lines=1539, words=35842, chars=237225)\n",
      "[INFO] Cleaning raw_0053.txt …\n",
      "[INFO] Wrote clean_0053.txt (lines=2010, words=26900, chars=168958)\n",
      "[INFO] Cleaning raw_0054.txt …\n",
      "[INFO] Wrote clean_0054.txt (lines=2409, words=28410, chars=180464)\n",
      "[INFO] Cleaning raw_0055.txt …\n",
      "[INFO] Wrote clean_0055.txt (lines=2179, words=34215, chars=214307)\n",
      "[INFO] Cleaning raw_0056.txt …\n",
      "[INFO] Wrote clean_0056.txt (lines=1622, words=21091, chars=133696)\n",
      "[INFO] Cleaning raw_0057.txt …\n",
      "[INFO] Wrote clean_0057.txt (lines=1534, words=26220, chars=166157)\n",
      "[INFO] Cleaning raw_0058.txt …\n",
      "[INFO] Wrote clean_0058.txt (lines=1613, words=26987, chars=172014)\n",
      "[INFO] Cleaning raw_0059.txt …\n",
      "[INFO] Wrote clean_0059.txt (lines=1752, words=29896, chars=188344)\n",
      "[INFO] Cleaning raw_0060.txt …\n",
      "[INFO] Wrote clean_0060.txt (lines=1614, words=26149, chars=161085)\n",
      "[INFO] Cleaning raw_0061.txt …\n",
      "[INFO] Wrote clean_0061.txt (lines=1617, words=24686, chars=158212)\n",
      "[INFO] Cleaning raw_0062.txt …\n",
      "[INFO] Wrote clean_0062.txt (lines=1675, words=22237, chars=140771)\n",
      "[INFO] Cleaning raw_0063.txt …\n",
      "[INFO] Wrote clean_0063.txt (lines=1977, words=27398, chars=174115)\n",
      "[INFO] Cleaning raw_0064.txt …\n",
      "[INFO] Wrote clean_0064.txt (lines=1613, words=24225, chars=152711)\n",
      "[INFO] Cleaning raw_0065.txt …\n",
      "[INFO] Wrote clean_0065.txt (lines=2438, words=31881, chars=202416)\n",
      "[INFO] Cleaning raw_0066.txt …\n",
      "[INFO] Wrote clean_0066.txt (lines=1795, words=24267, chars=151274)\n",
      "[INFO] Cleaning raw_0067.txt …\n",
      "[INFO] Wrote clean_0067.txt (lines=2043, words=26924, chars=170217)\n",
      "[INFO] Cleaning raw_0068.txt …\n",
      "[INFO] Wrote clean_0068.txt (lines=1959, words=22599, chars=141198)\n",
      "[INFO] Cleaning raw_0069.txt …\n",
      "[INFO] Wrote clean_0069.txt (lines=1725, words=23532, chars=148850)\n",
      "[INFO] Cleaning raw_0070.txt …\n",
      "[INFO] Wrote clean_0070.txt (lines=1885, words=23803, chars=148578)\n",
      "[INFO] Cleaning raw_0071.txt …\n",
      "[INFO] Wrote clean_0071.txt (lines=1047, words=21136, chars=133845)\n",
      "[INFO] Cleaning raw_0072.txt …\n",
      "[INFO] Wrote clean_0072.txt (lines=1672, words=36118, chars=229207)\n",
      "[INFO] Cleaning raw_0073.txt …\n",
      "[INFO] Wrote clean_0073.txt (lines=1416, words=22508, chars=141534)\n",
      "[INFO] Cleaning raw_0074.txt …\n",
      "[INFO] Wrote clean_0074.txt (lines=1723, words=21475, chars=134717)\n",
      "[INFO] Cleaning raw_0075.txt …\n",
      "[INFO] Wrote clean_0075.txt (lines=1344, words=23489, chars=149730)\n",
      "[INFO] Cleaning raw_0076.txt …\n",
      "[INFO] Wrote clean_0076.txt (lines=1773, words=25698, chars=161553)\n",
      "[INFO] Cleaning raw_0077.txt …\n",
      "[INFO] Wrote clean_0077.txt (lines=1693, words=22859, chars=141831)\n",
      "[INFO] Cleaning raw_0078.txt …\n",
      "[INFO] Wrote clean_0078.txt (lines=1446, words=23914, chars=151186)\n",
      "[INFO] Cleaning raw_0079.txt …\n",
      "[INFO] Wrote clean_0079.txt (lines=1779, words=23696, chars=150128)\n",
      "[INFO] Cleaning raw_0080.txt …\n",
      "[INFO] Wrote clean_0080.txt (lines=1374, words=22022, chars=137140)\n",
      "[INFO] Cleaning raw_0081.txt …\n",
      "[INFO] Wrote clean_0081.txt (lines=1627, words=23256, chars=145836)\n",
      "[INFO] Cleaning raw_0082.txt …\n",
      "[INFO] Wrote clean_0082.txt (lines=1795, words=22026, chars=135898)\n",
      "[INFO] Cleaning raw_0083.txt …\n",
      "[INFO] Wrote clean_0083.txt (lines=1547, words=20420, chars=129371)\n",
      "[INFO] Cleaning raw_0084.txt …\n",
      "[INFO] Wrote clean_0084.txt (lines=1548, words=26074, chars=162355)\n",
      "[INFO] Cleaning raw_0085.txt …\n",
      "[INFO] Wrote clean_0085.txt (lines=1719, words=23387, chars=148431)\n",
      "[INFO] Cleaning raw_0086.txt …\n",
      "[INFO] Wrote clean_0086.txt (lines=1644, words=21746, chars=136402)\n",
      "[INFO] Cleaning raw_0087.txt …\n",
      "[INFO] Wrote clean_0087.txt (lines=2451, words=48529, chars=305994)\n",
      "[INFO] Cleaning raw_0088.txt …\n",
      "[INFO] Wrote clean_0088.txt (lines=1491, words=22676, chars=144190)\n",
      "[INFO] Cleaning raw_0089.txt …\n",
      "[INFO] Wrote clean_0089.txt (lines=2648, words=43532, chars=273186)\n",
      "[INFO] Cleaning raw_0090.txt …\n",
      "[INFO] Wrote clean_0090.txt (lines=1691, words=28761, chars=183265)\n",
      "[INFO] Cleaning raw_0091.txt …\n",
      "[INFO] Wrote clean_0091.txt (lines=1244, words=19917, chars=125848)\n",
      "[INFO] Cleaning raw_0092.txt …\n",
      "[INFO] Wrote clean_0092.txt (lines=4046, words=99469, chars=637053)\n",
      "[INFO] Cleaning raw_0093.txt …\n",
      "[INFO] Wrote clean_0093.txt (lines=1734, words=25060, chars=156762)\n",
      "[INFO] Cleaning raw_0094.txt …\n",
      "[INFO] Wrote clean_0094.txt (lines=1405, words=22176, chars=139352)\n",
      "[INFO] Cleaning raw_0095.txt …\n",
      "[INFO] Wrote clean_0095.txt (lines=1614, words=22303, chars=140722)\n",
      "[INFO] Cleaning raw_0096.txt …\n",
      "[INFO] Wrote clean_0096.txt (lines=2481, words=44266, chars=278929)\n",
      "[INFO] Cleaning raw_0097.txt …\n",
      "[INFO] Wrote clean_0097.txt (lines=1372, words=22035, chars=138947)\n",
      "[INFO] Cleaning raw_0098.txt …\n",
      "[INFO] Wrote clean_0098.txt (lines=1582, words=22962, chars=147716)\n",
      "[INFO] Cleaning raw_0099.txt …\n",
      "[INFO] Wrote clean_0099.txt (lines=1758, words=25943, chars=159594)\n",
      "[INFO] Cleaning raw_0100.txt …\n",
      "[INFO] Wrote clean_0100.txt (lines=1646, words=23680, chars=148100)\n",
      "[INFO] Cleaning raw_0101.txt …\n",
      "[INFO] Wrote clean_0101.txt (lines=1486, words=23524, chars=146079)\n",
      "[INFO] Cleaning raw_0102.txt …\n",
      "[INFO] Wrote clean_0102.txt (lines=2632, words=42469, chars=267703)\n",
      "[INFO] Cleaning raw_0103.txt …\n",
      "[INFO] Wrote clean_0103.txt (lines=2325, words=27370, chars=172190)\n",
      "[INFO] Cleaning raw_0104.txt …\n",
      "[INFO] Wrote clean_0104.txt (lines=1545, words=24576, chars=154082)\n",
      "[INFO] Cleaning raw_0105.txt …\n",
      "[INFO] Wrote clean_0105.txt (lines=2466, words=31737, chars=201549)\n",
      "[INFO] Cleaning raw_0106.txt …\n",
      "[INFO] Wrote clean_0106.txt (lines=1902, words=26971, chars=166603)\n",
      "[INFO] Cleaning raw_0107.txt …\n",
      "[INFO] Wrote clean_0107.txt (lines=1542, words=21831, chars=136966)\n",
      "[INFO] Cleaning raw_0108.txt …\n",
      "[INFO] Wrote clean_0108.txt (lines=1522, words=29028, chars=183256)\n",
      "[INFO] Cleaning raw_0109.txt …\n",
      "[INFO] Wrote clean_0109.txt (lines=1740, words=24892, chars=153886)\n",
      "[INFO] Cleaning raw_0110.txt …\n",
      "[INFO] Wrote clean_0110.txt (lines=1573, words=24286, chars=151718)\n",
      "[INFO] Cleaning raw_0111.txt …\n",
      "[INFO] Wrote clean_0111.txt (lines=1494, words=23977, chars=151240)\n",
      "[INFO] Cleaning raw_0112.txt …\n",
      "[INFO] Wrote clean_0112.txt (lines=1693, words=22100, chars=139844)\n",
      "[INFO] Cleaning raw_0113.txt …\n",
      "[INFO] Wrote clean_0113.txt (lines=1414, words=22158, chars=140573)\n",
      "[INFO] Cleaning raw_0114.txt …\n",
      "[INFO] Wrote clean_0114.txt (lines=1664, words=21108, chars=132883)\n",
      "[INFO] Cleaning raw_0115.txt …\n",
      "[INFO] Wrote clean_0115.txt (lines=1857, words=39203, chars=250723)\n",
      "[INFO] Cleaning raw_0116.txt …\n",
      "[INFO] Wrote clean_0116.txt (lines=1630, words=24126, chars=150202)\n",
      "[INFO] Cleaning raw_0117.txt …\n",
      "[INFO] Wrote clean_0117.txt (lines=1463, words=22717, chars=146011)\n",
      "[INFO] Cleaning raw_0118.txt …\n",
      "[INFO] Wrote clean_0118.txt (lines=1805, words=26796, chars=168187)\n",
      "[INFO] Cleaning raw_0119.txt …\n",
      "[INFO] Wrote clean_0119.txt (lines=1561, words=42462, chars=277696)\n",
      "[INFO] Cleaning raw_0120.txt …\n",
      "[INFO] Wrote clean_0120.txt (lines=1463, words=21424, chars=133915)\n",
      "[INFO] Cleaning raw_0121.txt …\n",
      "[INFO] Wrote clean_0121.txt (lines=1560, words=24962, chars=156702)\n",
      "[INFO] Cleaning raw_0122.txt …\n",
      "[INFO] Wrote clean_0122.txt (lines=1576, words=23657, chars=147825)\n",
      "[INFO] Cleaning raw_0123.txt …\n",
      "[INFO] Wrote clean_0123.txt (lines=1519, words=22062, chars=137935)\n",
      "[INFO] Cleaning raw_0124.txt …\n",
      "[INFO] Wrote clean_0124.txt (lines=1336, words=24638, chars=154421)\n",
      "[INFO] Cleaning raw_0125.txt …\n",
      "[INFO] Wrote clean_0125.txt (lines=2317, words=35908, chars=225599)\n",
      "[INFO] Cleaning raw_0126.txt …\n",
      "[INFO] Wrote clean_0126.txt (lines=1922, words=23209, chars=144421)\n",
      "[INFO] Cleaning raw_0127.txt …\n",
      "[INFO] Wrote clean_0127.txt (lines=1632, words=22425, chars=142014)\n",
      "[INFO] Cleaning raw_0128.txt …\n",
      "[INFO] Wrote clean_0128.txt (lines=1691, words=23176, chars=143772)\n",
      "[INFO] Cleaning raw_0129.txt …\n",
      "[INFO] Wrote clean_0129.txt (lines=1583, words=20852, chars=131217)\n",
      "[INFO] Cleaning raw_0130.txt …\n",
      "[INFO] Wrote clean_0130.txt (lines=2037, words=26483, chars=165117)\n",
      "[INFO] Cleaning raw_0131.txt …\n",
      "[INFO] Wrote clean_0131.txt (lines=2127, words=23927, chars=149347)\n",
      "[INFO] Cleaning raw_0132.txt …\n",
      "[INFO] Wrote clean_0132.txt (lines=5093, words=85155, chars=529928)\n",
      "[INFO] Cleaning raw_0133.txt …\n",
      "[INFO] Wrote clean_0133.txt (lines=1460, words=23039, chars=154009)\n",
      "[INFO] Cleaning raw_0134.txt …\n",
      "[INFO] Wrote clean_0134.txt (lines=1488, words=21277, chars=134211)\n",
      "[INFO] Cleaning raw_0135.txt …\n",
      "[INFO] Wrote clean_0135.txt (lines=1207, words=19243, chars=122691)\n",
      "[INFO] Cleaning raw_0136.txt …\n",
      "[INFO] Wrote clean_0136.txt (lines=2364, words=30788, chars=194069)\n",
      "[INFO] Cleaning raw_0137.txt …\n",
      "[INFO] Wrote clean_0137.txt (lines=4115, words=63234, chars=403773)\n",
      "[INFO] Cleaning raw_0138.txt …\n",
      "[INFO] Wrote clean_0138.txt (lines=2664, words=41397, chars=261850)\n",
      "[INFO] Cleaning raw_0139.txt …\n",
      "[INFO] Wrote clean_0139.txt (lines=3779, words=65835, chars=410428)\n",
      "[INFO] Cleaning raw_0140.txt …\n",
      "[INFO] Wrote clean_0140.txt (lines=2023, words=27463, chars=173646)\n",
      "[INFO] Cleaning raw_0141.txt …\n",
      "[INFO] Wrote clean_0141.txt (lines=1560, words=23792, chars=148750)\n",
      "[INFO] Cleaning raw_0142.txt …\n",
      "[INFO] Wrote clean_0142.txt (lines=1600, words=23880, chars=149321)\n",
      "[INFO] Cleaning raw_0143.txt …\n",
      "[INFO] Wrote clean_0143.txt (lines=1505, words=25978, chars=160502)\n",
      "[INFO] Cleaning raw_0144.txt …\n",
      "[INFO] Wrote clean_0144.txt (lines=1975, words=23391, chars=146218)\n",
      "[INFO] Cleaning raw_0145.txt …\n",
      "[INFO] Wrote clean_0145.txt (lines=1695, words=26504, chars=165441)\n",
      "[INFO] Cleaning raw_0146.txt …\n",
      "[INFO] Wrote clean_0146.txt (lines=1930, words=24502, chars=153844)\n",
      "[INFO] Cleaning raw_0147.txt …\n",
      "[INFO] Wrote clean_0147.txt (lines=3466, words=41771, chars=266219)\n",
      "[INFO] Cleaning raw_0148.txt …\n",
      "[INFO] Wrote clean_0148.txt (lines=1707, words=38754, chars=250306)\n",
      "[INFO] Cleaning raw_0149.txt …\n",
      "[INFO] Wrote clean_0149.txt (lines=1940, words=23209, chars=146292)\n",
      "[INFO] Cleaning raw_0150.txt …\n",
      "[INFO] Wrote clean_0150.txt (lines=1705, words=28759, chars=179300)\n",
      "[INFO] Cleaning raw_0151.txt …\n",
      "[INFO] Wrote clean_0151.txt (lines=4802, words=81953, chars=511755)\n",
      "[INFO] Cleaning raw_0152.txt …\n",
      "[INFO] Wrote clean_0152.txt (lines=1459, words=23515, chars=147078)\n",
      "[INFO] Cleaning raw_0153.txt …\n",
      "[INFO] Wrote clean_0153.txt (lines=1802, words=23359, chars=147867)\n",
      "[INFO] Cleaning raw_0154.txt …\n",
      "[INFO] Wrote clean_0154.txt (lines=3786, words=54233, chars=342015)\n",
      "[INFO] Cleaning raw_0155.txt …\n",
      "[INFO] Wrote clean_0155.txt (lines=1917, words=23549, chars=150664)\n",
      "[INFO] Cleaning raw_0156.txt …\n",
      "[INFO] Wrote clean_0156.txt (lines=2005, words=24116, chars=151164)\n",
      "[INFO] Cleaning raw_0157.txt …\n",
      "[INFO] Wrote clean_0157.txt (lines=1388, words=22605, chars=141809)\n",
      "[INFO] Cleaning raw_0158.txt …\n",
      "[INFO] Wrote clean_0158.txt (lines=1872, words=27589, chars=175158)\n",
      "[INFO] Cleaning raw_0159.txt …\n",
      "[INFO] Wrote clean_0159.txt (lines=1726, words=23862, chars=150548)\n",
      "[INFO] Cleaning raw_0160.txt …\n",
      "[INFO] Wrote clean_0160.txt (lines=1813, words=25263, chars=159106)\n",
      "[INFO] Cleaning raw_0161.txt …\n",
      "[INFO] Wrote clean_0161.txt (lines=2029, words=37495, chars=236405)\n",
      "[INFO] Cleaning raw_0162.txt …\n",
      "[INFO] Wrote clean_0162.txt (lines=1938, words=23406, chars=148029)\n",
      "[INFO] Cleaning raw_0163.txt …\n",
      "[INFO] Wrote clean_0163.txt (lines=1736, words=27684, chars=173544)\n",
      "[INFO] Cleaning raw_0164.txt …\n",
      "[INFO] Wrote clean_0164.txt (lines=1605, words=24976, chars=153954)\n",
      "[INFO] Cleaning raw_0165.txt …\n",
      "[INFO] Wrote clean_0165.txt (lines=1647, words=21939, chars=138029)\n",
      "[INFO] Cleaning raw_0166.txt …\n",
      "[INFO] Wrote clean_0166.txt (lines=2223, words=27322, chars=174272)\n",
      "[INFO] Cleaning raw_0167.txt …\n",
      "[INFO] Wrote clean_0167.txt (lines=1594, words=22085, chars=139755)\n",
      "[INFO] Cleaning raw_0168.txt …\n",
      "[INFO] Wrote clean_0168.txt (lines=1778, words=23924, chars=150184)\n",
      "[INFO] Cleaning raw_0169.txt …\n",
      "[INFO] Wrote clean_0169.txt (lines=1788, words=26484, chars=167390)\n",
      "[INFO] Cleaning raw_0170.txt …\n",
      "[INFO] Wrote clean_0170.txt (lines=1372, words=21952, chars=139301)\n",
      "[INFO] Cleaning raw_0171.txt …\n",
      "[INFO] Wrote clean_0171.txt (lines=1607, words=23977, chars=152239)\n",
      "[INFO] Cleaning raw_0172.txt …\n",
      "[INFO] Wrote clean_0172.txt (lines=3382, words=77646, chars=487396)\n",
      "[INFO] Cleaning raw_0173.txt …\n",
      "[INFO] Wrote clean_0173.txt (lines=2194, words=26408, chars=168516)\n",
      "[INFO] Cleaning raw_0174.txt …\n",
      "[INFO] Wrote clean_0174.txt (lines=1837, words=22600, chars=142157)\n",
      "[INFO] Cleaning raw_0175.txt …\n",
      "[INFO] Wrote clean_0175.txt (lines=1354, words=21505, chars=136450)\n",
      "[INFO] Cleaning raw_0176.txt …\n",
      "[INFO] Wrote clean_0176.txt (lines=2098, words=51410, chars=327889)\n",
      "[INFO] Cleaning raw_0177.txt …\n",
      "[INFO] Wrote clean_0177.txt (lines=2221, words=36524, chars=231239)\n",
      "[INFO] Cleaning raw_0178.txt …\n",
      "[INFO] Wrote clean_0178.txt (lines=598, words=21151, chars=131079)\n",
      "[INFO] Cleaning raw_0179.txt …\n",
      "[INFO] Wrote clean_0179.txt (lines=464, words=11418, chars=72407)\n",
      "[INFO] Cleaning raw_0180.txt …\n",
      "[INFO] Wrote clean_0180.txt (lines=488, words=13296, chars=85186)\n",
      "[INFO] Cleaning raw_0181.txt …\n",
      "[INFO] Wrote clean_0181.txt (lines=101, words=3493, chars=22634)\n",
      "[INFO] Cleaning raw_0182.txt …\n",
      "[INFO] Wrote clean_0182.txt (lines=836, words=22660, chars=146822)\n",
      "[INFO] Cleaning raw_0183.txt …\n",
      "[INFO] Wrote clean_0183.txt (lines=330, words=8160, chars=53708)\n",
      "[INFO] Cleaning raw_0184.txt …\n",
      "[INFO] Wrote clean_0184.txt (lines=118, words=2028, chars=12111)\n",
      "[INFO] Cleaning raw_0185.txt …\n",
      "[INFO] Wrote clean_0185.txt (lines=53, words=1297, chars=8060)\n",
      "[INFO] Cleaning raw_0186.txt …\n",
      "[INFO] Wrote clean_0186.txt (lines=639, words=15382, chars=101857)\n",
      "[INFO] Cleaning raw_0187.txt …\n",
      "[INFO] Wrote clean_0187.txt (lines=2409, words=84281, chars=548420)\n",
      "[INFO] Cleaning raw_0188.txt …\n",
      "[INFO] Wrote clean_0188.txt (lines=243, words=7817, chars=51822)\n",
      "[INFO] Cleaning raw_0189.txt …\n",
      "[INFO] Wrote clean_0189.txt (lines=1329, words=35801, chars=227232)\n",
      "[INFO] Cleaning raw_0190.txt …\n",
      "[INFO] Wrote clean_0190.txt (lines=360, words=8927, chars=59146)\n",
      "[INFO] Cleaning raw_0191.txt …\n",
      "[INFO] Wrote clean_0191.txt (lines=995, words=31501, chars=207903)\n",
      "[INFO] Cleaning raw_0192.txt …\n",
      "[INFO] Wrote clean_0192.txt (lines=427, words=12685, chars=84907)\n",
      "[INFO] Cleaning raw_0193.txt …\n",
      "[INFO] Wrote clean_0193.txt (lines=165, words=3900, chars=25961)\n",
      "[INFO] Cleaning raw_0194.txt …\n",
      "[INFO] Wrote clean_0194.txt (lines=252, words=5698, chars=37997)\n",
      "[INFO] Cleaning raw_0195.txt …\n",
      "[INFO] Wrote clean_0195.txt (lines=680, words=18485, chars=119511)\n",
      "[INFO] Cleaning raw_0196.txt …\n",
      "[INFO] Wrote clean_0196.txt (lines=159, words=3211, chars=20442)\n",
      "[INFO] Cleaning raw_0197.txt …\n",
      "[INFO] Wrote clean_0197.txt (lines=140, words=4261, chars=26822)\n",
      "[INFO] Cleaning raw_0198.txt …\n",
      "[INFO] Wrote clean_0198.txt (lines=999, words=23072, chars=150097)\n",
      "[INFO] Cleaning raw_0199.txt …\n",
      "[INFO] Wrote clean_0199.txt (lines=88, words=1957, chars=12617)\n",
      "[INFO] Cleaning raw_0200.txt …\n",
      "[INFO] Wrote clean_0200.txt (lines=323, words=8296, chars=53556)\n",
      "[INFO] Cleaning raw_0201.txt …\n",
      "[INFO] Wrote clean_0201.txt (lines=426, words=13318, chars=85793)\n",
      "[INFO] Cleaning raw_0202.txt …\n",
      "[INFO] Wrote clean_0202.txt (lines=135, words=4288, chars=27567)\n",
      "[INFO] Cleaning raw_0203.txt …\n",
      "[INFO] Wrote clean_0203.txt (lines=475, words=14085, chars=90563)\n",
      "[INFO] Cleaning raw_0204.txt …\n",
      "[INFO] Wrote clean_0204.txt (lines=1958, words=44284, chars=290575)\n",
      "[INFO] Cleaning raw_0205.txt …\n",
      "[INFO] Wrote clean_0205.txt (lines=371, words=10562, chars=70282)\n",
      "[INFO] Cleaning raw_0206.txt …\n",
      "[INFO] Wrote clean_0206.txt (lines=169, words=4474, chars=28839)\n",
      "[INFO] Cleaning raw_0207.txt …\n",
      "[INFO] Wrote clean_0207.txt (lines=1656, words=33724, chars=219459)\n",
      "[INFO] Cleaning raw_0208.txt …\n",
      "[INFO] Wrote clean_0208.txt (lines=364, words=7255, chars=48282)\n",
      "[INFO] Cleaning raw_0209.txt …\n",
      "[INFO] Wrote clean_0209.txt (lines=456, words=12843, chars=83963)\n",
      "[INFO] Cleaning raw_0210.txt …\n",
      "[INFO] Wrote clean_0210.txt (lines=783, words=23587, chars=152884)\n",
      "[INFO] Cleaning raw_0211.txt …\n",
      "[INFO] Wrote clean_0211.txt (lines=204, words=4534, chars=28454)\n",
      "[INFO] Cleaning raw_0212.txt …\n",
      "[INFO] Wrote clean_0212.txt (lines=1181, words=31163, chars=199353)\n",
      "[INFO] Cleaning raw_0213.txt …\n",
      "[INFO] Wrote clean_0213.txt (lines=788, words=23075, chars=146620)\n",
      "[INFO] Cleaning raw_0214.txt …\n",
      "[INFO] Wrote clean_0214.txt (lines=1009, words=30641, chars=191715)\n",
      "[INFO] Cleaning raw_0215.txt …\n",
      "[INFO] Wrote clean_0215.txt (lines=280, words=7115, chars=45318)\n",
      "[INFO] Cleaning raw_0216.txt …\n",
      "[INFO] Wrote clean_0216.txt (lines=134, words=2965, chars=18907)\n",
      "[INFO] Cleaning raw_0217.txt …\n",
      "[INFO] Wrote clean_0217.txt (lines=150, words=3615, chars=23311)\n",
      "[INFO] Cleaning raw_0218.txt …\n",
      "[INFO] Wrote clean_0218.txt (lines=116, words=2146, chars=13604)\n",
      "[INFO] Cleaning raw_0219.txt …\n",
      "[INFO] Wrote clean_0219.txt (lines=503, words=19978, chars=128793)\n",
      "[INFO] Cleaning raw_0220.txt …\n",
      "[INFO] Wrote clean_0220.txt (lines=524, words=14039, chars=90088)\n",
      "[INFO] Cleaning raw_0221.txt …\n",
      "[INFO] Wrote clean_0221.txt (lines=559, words=18125, chars=111216)\n",
      "[INFO] Cleaning raw_0222.txt …\n",
      "[INFO] Wrote clean_0222.txt (lines=617, words=15698, chars=98890)\n",
      "[INFO] Cleaning raw_0223.txt …\n",
      "[INFO] Wrote clean_0223.txt (lines=87, words=2722, chars=17638)\n",
      "[INFO] Cleaning raw_0224.txt …\n",
      "[INFO] Wrote clean_0224.txt (lines=194, words=6411, chars=42514)\n",
      "[INFO] Cleaning raw_0225.txt …\n",
      "[INFO] Wrote clean_0225.txt (lines=327, words=9260, chars=60278)\n",
      "[INFO] Cleaning raw_0226.txt …\n",
      "[INFO] Wrote clean_0226.txt (lines=956, words=22101, chars=140572)\n",
      "[INFO] Cleaning raw_0227.txt …\n",
      "[INFO] Wrote clean_0227.txt (lines=1205, words=33443, chars=217906)\n",
      "[INFO] Cleaning raw_0228.txt …\n",
      "[INFO] Wrote clean_0228.txt (lines=388, words=8363, chars=54782)\n",
      "[INFO] Cleaning raw_0229.txt …\n",
      "[INFO] Wrote clean_0229.txt (lines=1133, words=36540, chars=240919)\n",
      "[INFO] Cleaning raw_0230.txt …\n",
      "[INFO] Wrote clean_0230.txt (lines=309, words=9051, chars=58142)\n",
      "[INFO] Cleaning raw_0231.txt …\n",
      "[INFO] Wrote clean_0231.txt (lines=237, words=5817, chars=37095)\n",
      "[INFO] Cleaning raw_0232.txt …\n",
      "[INFO] Wrote clean_0232.txt (lines=363, words=10473, chars=69219)\n",
      "[INFO] Cleaning raw_0233.txt …\n",
      "[INFO] Wrote clean_0233.txt (lines=167, words=3982, chars=26177)\n",
      "[INFO] Cleaning raw_0234.txt …\n",
      "[INFO] Wrote clean_0234.txt (lines=258, words=5556, chars=36336)\n",
      "[INFO] Cleaning raw_0235.txt …\n",
      "[INFO] Wrote clean_0235.txt (lines=252, words=6281, chars=41828)\n",
      "[INFO] Cleaning raw_0236.txt …\n",
      "[INFO] Wrote clean_0236.txt (lines=192, words=4255, chars=27575)\n",
      "[INFO] Cleaning raw_0237.txt …\n",
      "[INFO] Wrote clean_0237.txt (lines=318, words=8501, chars=54934)\n",
      "[INFO] Cleaning raw_0238.txt …\n",
      "[INFO] Wrote clean_0238.txt (lines=296, words=6495, chars=42101)\n",
      "[INFO] Cleaning raw_0239.txt …\n",
      "[INFO] Wrote clean_0239.txt (lines=757, words=15719, chars=103308)\n",
      "[INFO] Cleaning raw_0240.txt …\n",
      "[INFO] Wrote clean_0240.txt (lines=355, words=9045, chars=60929)\n",
      "[INFO] Cleaning raw_0241.txt …\n",
      "[INFO] Wrote clean_0241.txt (lines=219, words=4296, chars=28171)\n",
      "[INFO] Cleaning raw_0242.txt …\n",
      "[INFO] Wrote clean_0242.txt (lines=188, words=4027, chars=26414)\n",
      "[INFO] Cleaning raw_0243.txt …\n",
      "[INFO] Wrote clean_0243.txt (lines=195, words=3978, chars=26155)\n",
      "[INFO] Cleaning raw_0244.txt …\n",
      "[INFO] Wrote clean_0244.txt (lines=240, words=5095, chars=32672)\n",
      "[INFO] Cleaning raw_0245.txt …\n",
      "[INFO] Wrote clean_0245.txt (lines=238, words=5290, chars=34110)\n",
      "[INFO] Cleaning raw_0246.txt …\n",
      "[INFO] Wrote clean_0246.txt (lines=259, words=5648, chars=36373)\n",
      "[INFO] Cleaning raw_0247.txt …\n",
      "[INFO] Wrote clean_0247.txt (lines=195, words=3643, chars=23406)\n",
      "[INFO] Cleaning raw_0248.txt …\n",
      "[INFO] Wrote clean_0248.txt (lines=213, words=4431, chars=29488)\n",
      "[INFO] Cleaning raw_0249.txt …\n",
      "[INFO] Wrote clean_0249.txt (lines=257, words=6835, chars=44472)\n",
      "[INFO] Cleaning raw_0250.txt …\n",
      "[INFO] Wrote clean_0250.txt (lines=243, words=6235, chars=40846)\n",
      "[INFO] Cleaning raw_0251.txt …\n",
      "[INFO] Wrote clean_0251.txt (lines=417, words=8489, chars=56624)\n",
      "[INFO] Cleaning raw_0252.txt …\n",
      "[INFO] Wrote clean_0252.txt (lines=170, words=3759, chars=25115)\n",
      "[INFO] Cleaning raw_0253.txt …\n",
      "[INFO] Wrote clean_0253.txt (lines=162, words=3613, chars=23610)\n",
      "[INFO] Cleaning raw_0254.txt …\n",
      "[INFO] Wrote clean_0254.txt (lines=168, words=4300, chars=27776)\n",
      "[INFO] Cleaning raw_0255.txt …\n",
      "[INFO] Wrote clean_0255.txt (lines=402, words=10133, chars=67525)\n",
      "[INFO] Cleaning raw_0256.txt …\n",
      "[INFO] Wrote clean_0256.txt (lines=335, words=11511, chars=73108)\n",
      "[INFO] Cleaning raw_0257.txt …\n",
      "[INFO] Wrote clean_0257.txt (lines=180, words=3060, chars=19908)\n",
      "[INFO] Cleaning raw_0258.txt …\n",
      "[INFO] Wrote clean_0258.txt (lines=1146, words=33551, chars=223845)\n",
      "[INFO] Cleaning raw_0259.txt …\n",
      "[INFO] Wrote clean_0259.txt (lines=1461, words=36806, chars=231862)\n",
      "[INFO] Cleaning raw_0260.txt …\n",
      "[INFO] Wrote clean_0260.txt (lines=1340, words=34652, chars=217540)\n",
      "[INFO] Cleaning raw_0261.txt …\n",
      "[INFO] Wrote clean_0261.txt (lines=212, words=4511, chars=29269)\n",
      "[INFO] Cleaning raw_0262.txt …\n",
      "[INFO] Wrote clean_0262.txt (lines=320, words=6368, chars=40597)\n",
      "[INFO] Cleaning raw_0263.txt …\n",
      "[INFO] Wrote clean_0263.txt (lines=216, words=4786, chars=30804)\n",
      "[INFO] Cleaning raw_0264.txt …\n",
      "[INFO] Wrote clean_0264.txt (lines=131, words=2844, chars=18656)\n",
      "[INFO] Cleaning raw_0265.txt …\n",
      "[INFO] Wrote clean_0265.txt (lines=478, words=12617, chars=82837)\n",
      "[INFO] Cleaning raw_0266.txt …\n",
      "[INFO] Wrote clean_0266.txt (lines=345, words=9876, chars=64868)\n",
      "[INFO] Cleaning raw_0267.txt …\n",
      "[INFO] Wrote clean_0267.txt (lines=526, words=13819, chars=91140)\n",
      "[INFO] Cleaning raw_0268.txt …\n",
      "[INFO] Wrote clean_0268.txt (lines=218, words=4724, chars=30033)\n",
      "[INFO] Cleaning raw_0269.txt …\n",
      "[INFO] Wrote clean_0269.txt (lines=171, words=3741, chars=24523)\n",
      "[INFO] Cleaning raw_0270.txt …\n",
      "[INFO] Wrote clean_0270.txt (lines=741, words=25814, chars=172788)\n",
      "[INFO] Cleaning raw_0271.txt …\n",
      "[INFO] Wrote clean_0271.txt (lines=106, words=2161, chars=14220)\n",
      "[INFO] Cleaning raw_0272.txt …\n",
      "[INFO] Wrote clean_0272.txt (lines=233, words=6564, chars=42148)\n",
      "[INFO] Cleaning raw_0273.txt …\n",
      "[INFO] Wrote clean_0273.txt (lines=258, words=5292, chars=33681)\n",
      "[INFO] Cleaning raw_0274.txt …\n",
      "[INFO] Wrote clean_0274.txt (lines=451, words=13885, chars=91271)\n",
      "[INFO] Cleaning raw_0275.txt …\n",
      "[INFO] Wrote clean_0275.txt (lines=407, words=9396, chars=59991)\n",
      "[INFO] Cleaning raw_0276.txt …\n",
      "[INFO] Wrote clean_0276.txt (lines=293, words=6825, chars=44684)\n",
      "[INFO] Cleaning raw_0277.txt …\n",
      "[INFO] Wrote clean_0277.txt (lines=254, words=8707, chars=58017)\n",
      "[INFO] Cleaning raw_0278.txt …\n",
      "[INFO] Wrote clean_0278.txt (lines=145, words=3199, chars=20707)\n",
      "[INFO] Cleaning raw_0279.txt …\n",
      "[INFO] Wrote clean_0279.txt (lines=143, words=3339, chars=20974)\n",
      "[INFO] Cleaning raw_0280.txt …\n",
      "[INFO] Wrote clean_0280.txt (lines=281, words=6820, chars=44730)\n",
      "[INFO] Cleaning raw_0281.txt …\n",
      "[INFO] Wrote clean_0281.txt (lines=81, words=1512, chars=9774)\n",
      "[INFO] Cleaning raw_0282.txt …\n",
      "[INFO] Wrote clean_0282.txt (lines=1028, words=30896, chars=193327)\n",
      "[INFO] Cleaning raw_0283.txt …\n",
      "[INFO] Wrote clean_0283.txt (lines=86, words=2815, chars=17511)\n",
      "[INFO] Cleaning raw_0284.txt …\n",
      "[INFO] Wrote clean_0284.txt (lines=329, words=8614, chars=55106)\n",
      "[INFO] Cleaning raw_0285.txt …\n",
      "[INFO] Wrote clean_0285.txt (lines=275, words=10448, chars=62915)\n",
      "[INFO] Cleaning raw_0286.txt …\n",
      "[INFO] Wrote clean_0286.txt (lines=186, words=5036, chars=31345)\n",
      "[INFO] Cleaning raw_0287.txt …\n",
      "[INFO] Wrote clean_0287.txt (lines=210, words=5211, chars=34212)\n",
      "[INFO] Cleaning raw_0288.txt …\n",
      "[INFO] Wrote clean_0288.txt (lines=274, words=9883, chars=64799)\n",
      "[INFO] Cleaning raw_0289.txt …\n",
      "[INFO] Wrote clean_0289.txt (lines=104, words=3325, chars=21376)\n",
      "[INFO] Cleaning raw_0290.txt …\n",
      "[INFO] Wrote clean_0290.txt (lines=111, words=3319, chars=21367)\n",
      "[INFO] Cleaning raw_0291.txt …\n",
      "[INFO] Wrote clean_0291.txt (lines=105, words=3208, chars=20496)\n",
      "[INFO] Cleaning raw_0292.txt …\n",
      "[INFO] Wrote clean_0292.txt (lines=147, words=5348, chars=34952)\n",
      "[INFO] Cleaning raw_0293.txt …\n",
      "[INFO] Wrote clean_0293.txt (lines=91, words=1994, chars=12531)\n",
      "[INFO] Cleaning raw_0294.txt …\n",
      "[INFO] Wrote clean_0294.txt (lines=226, words=5045, chars=33715)\n",
      "[INFO] Cleaning raw_0295.txt …\n",
      "[INFO] Wrote clean_0295.txt (lines=64, words=1472, chars=8991)\n",
      "[INFO] Cleaning raw_0296.txt …\n",
      "[INFO] Wrote clean_0296.txt (lines=170, words=3905, chars=24729)\n",
      "[INFO] Cleaning raw_0297.txt …\n",
      "[INFO] Wrote clean_0297.txt (lines=333, words=8538, chars=54002)\n",
      "[INFO] Cleaning raw_0298.txt …\n",
      "[INFO] Wrote clean_0298.txt (lines=797, words=16293, chars=104620)\n",
      "[INFO] Cleaning raw_0299.txt …\n",
      "[INFO] Wrote clean_0299.txt (lines=27, words=820, chars=5018)\n",
      "[INFO] Cleaning raw_0300.txt …\n",
      "[INFO] Wrote clean_0300.txt (lines=55, words=1670, chars=10320)\n",
      "[INFO] Cleaning raw_0301.txt …\n",
      "[INFO] Wrote clean_0301.txt (lines=334, words=9115, chars=59548)\n",
      "[INFO] Manifest saved to c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Refined\\manifest_clean.csv\n",
      "[INFO] Summary: cleaned=301, skipped=0, failed=0\n",
      "[INFO] Text size reduction: 12.3%\n",
      "[INFO] Step 2 complete: 301 file(s) written to c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\temp\\clean\n"
     ]
    }
   ],
   "source": [
    "# ============== Step 2 — Text Cleaning & Normalization (One-Paragraph-One-Line) ==============\n",
    "# Objetivo:\n",
    "# - Cada párrafo queda en UNA sola línea.\n",
    "# - Preservar la separabilidad de encabezados (TÍTULO/CAPÍTULO/SECCIÓN/ART./TRANSITORIOS/DECRETO/NÚMERO)\n",
    "#   y fracciones (romanas, numéricas, alfabéticas), tratándolos como inicios de nuevo párrafo.\n",
    "# - Eliminar espacios redundantes, saltos de línea extra y artefactos comunes de extracción PDF.\n",
    "# - Estandarizar Unicode (NFC, NBSP→espacio, quitar cero-width), dehyphenation segura.\n",
    "# - Idempotente y listo para Step 3 (parsing estructural).\n",
    "#\n",
    "# Integra con utilidades de tu proyecto si existen (fallbacks seguros incluidos).\n",
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional\n",
    "import sys\n",
    "\n",
    "# ----------------------- Integraciones del proyecto (con fallbacks) -----------------------\n",
    "try:\n",
    "    RAW_TXT_DIR\n",
    "except NameError:\n",
    "    RAW_TXT_DIR = Path(\"temp/raw_txt\")\n",
    "\n",
    "try:\n",
    "    CLEAN_DIR\n",
    "except NameError:\n",
    "    CLEAN_DIR = Path(\"temp/clean\")\n",
    "\n",
    "try:\n",
    "    OUTPUT_DIR\n",
    "except NameError:\n",
    "    OUTPUT_DIR = Path(\"Refined\")\n",
    "\n",
    "try:\n",
    "    CATALOG_CSV\n",
    "except NameError:\n",
    "    CATALOG_CSV = Path(\"catalog.csv\")\n",
    "\n",
    "def _fallback_write_text(path: Path, text: str) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def _fallback_write_error(base, code, msg, extra=None) -> None:\n",
    "    sys.stderr.write(f\"[clean][{base}] {code}: {msg} | extra={extra}\\n\")\n",
    "\n",
    "def _fallback_count_stats(text: str) -> Dict[str, int]:\n",
    "    return {\n",
    "        \"lines\": text.count(\"\\n\") + (0 if text.endswith(\"\\n\") else 1 if text else 0),\n",
    "        \"words\": len(text.split()),\n",
    "        \"chars\": len(text),\n",
    "    }\n",
    "\n",
    "write_text   = globals().get(\"write_text\", _fallback_write_text)\n",
    "write_error  = globals().get(\"write_error\", _fallback_write_error)\n",
    "count_stats  = globals().get(\"count_stats\", _fallback_count_stats)\n",
    "load_catalog = globals().get(\"load_catalog\", None)  # opcional\n",
    "\n",
    "# ---------------------------- Patrones estructurales clave ----------------------------\n",
    "HEAD_RE = re.compile(\n",
    "    r'^\\s*(T[ÍI]TULO|CAP[ÍI]TULO|SECCI[ÓO]N|ART[ÍI]CULO|DECRETO|TRANSITORIOS?|N[ÚU]MERO)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# \"Art.\" o \"Art \" como atajo de ARTÍCULO\n",
    "ARTICLE_RE = re.compile(r'^\\s*Art(?:[íi]culo|\\.?)\\b', re.IGNORECASE)\n",
    "\n",
    "# Enumeraciones: romanas, numéricas, alfabéticas (a), b), etc.)\n",
    "ENUM_RE = re.compile(\n",
    "    r'^\\s*((?:I{1,4}|V?I{0,3}|X{1,3})\\s*(?:\\.|-)|\\d+\\s*(?:\\.|\\)|-)|[A-Za-zÁÉÍÓÚÜÑ]\\))'\n",
    ")\n",
    "\n",
    "# Contadores/pies de página típicos\n",
    "PAGE_RE_1 = re.compile(r'^\\s*[-–—]?\\s*\\d+\\s*[-–—]?\\s*$')\n",
    "PAGE_RE_2 = re.compile(r'^\\s*P[aá]gina\\s+\\d+(\\s+de\\s+\\d+)?\\s*$', re.IGNORECASE)\n",
    "\n",
    "# Variante espaciada de TRANSITORIOS\n",
    "TRANS_SPACED_RE = re.compile(r'^\\s*T\\s*R\\s*A\\s*N\\s*S\\s*I\\s*T\\s*O\\s*R\\s*I\\s*O\\s*S\\s*$', re.IGNORECASE)\n",
    "\n",
    "# ------------------------------ Utilidades de limpieza ------------------------------\n",
    "def _normalize_unicode(s: str) -> str:\n",
    "    s = s.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    s = s.replace('\\ufeff', '')     # BOM\n",
    "    s = s.replace('\\u00A0', ' ')    # NBSP -> espacio\n",
    "    s = re.sub('[\\u200b-\\u200d]', '', s)  # zero-width\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    return s\n",
    "\n",
    "def _strip_trailing_spaces(s: str) -> str:\n",
    "    return \"\\n\".join(ln.rstrip() for ln in s.split(\"\\n\"))\n",
    "\n",
    "def _remove_page_counters_and_repeated_headers(s: str) -> str:\n",
    "    # 1) Quitar folios/páginas\n",
    "    lines = [ln for ln in s.split(\"\\n\") if not (PAGE_RE_1.match(ln) or PAGE_RE_2.match(ln))]\n",
    "    # 2) Quitar encabezados/pies repetidos idénticos (>=3 veces) que no sean estructurales\n",
    "    counts = Counter(l for l in lines if l.strip() and not HEAD_RE.match(l) and not ENUM_RE.match(l))\n",
    "    repeated = {l for l, c in counts.items() if c >= 3}\n",
    "    return \"\\n\".join(l for l in lines if l not in repeated)\n",
    "\n",
    "def _normalize_transitorios_spaced(s: str) -> str:\n",
    "    out = []\n",
    "    for ln in s.split(\"\\n\"):\n",
    "        out.append(\"TRANSITORIOS\" if TRANS_SPACED_RE.match(ln.strip()) else ln)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _is_paragraph_starter(ln: str) -> bool:\n",
    "    if not ln.strip():\n",
    "        return True\n",
    "    return bool(HEAD_RE.match(ln) or ARTICLE_RE.match(ln) or ENUM_RE.match(ln))\n",
    "\n",
    "def _join_hard_wrap(prev: str, cur: str) -> str:\n",
    "    # Une respetando dehyphenation segura y un solo espacio\n",
    "    if prev.endswith('-') and cur and cur[:1].islower():\n",
    "        return prev[:-1] + cur\n",
    "    return (prev + ' ' + cur).replace('  ', ' ')\n",
    "\n",
    "def _paragraphs_to_one_line(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Recorre el texto línea por línea y construye párrafos de UNA línea.\n",
    "    Empieza un nuevo párrafo cuando:\n",
    "      - la línea está en blanco, o\n",
    "      - coincide con HEAD/ARTICLE/ENUM (encabezados y fracciones).\n",
    "    En caso contrario, concatena con la línea previa dentro del mismo párrafo.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    acc: Optional[str] = None\n",
    "\n",
    "    for raw_ln in s.split(\"\\n\"):\n",
    "        ln = raw_ln.strip()\n",
    "        if not ln:  # línea en blanco -> finaliza párrafo actual\n",
    "            if acc is not None and acc.strip():\n",
    "                out.append(acc.strip())\n",
    "                acc = None\n",
    "            continue\n",
    "\n",
    "        if _is_paragraph_starter(ln):\n",
    "            # Cierra el párrafo previo y empieza uno nuevo\n",
    "            if acc is not None and acc.strip():\n",
    "                out.append(acc.strip())\n",
    "            acc = ln\n",
    "        else:\n",
    "            # Continúa el párrafo actual\n",
    "            if acc is None:\n",
    "                acc = ln\n",
    "            else:\n",
    "                acc = _join_hard_wrap(acc, ln)\n",
    "\n",
    "    if acc is not None and acc.strip():\n",
    "        out.append(acc.strip())\n",
    "\n",
    "    # Sin líneas en blanco: una línea por párrafo.\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _intraline_space_normalize(s: str) -> str:\n",
    "    # Colapsa espacios internos y asegura separación mínima tras tokens legales comunes\n",
    "    s = re.sub(r'[ \\t]{2,}', ' ', s)\n",
    "    s = re.sub(r'(^|\\n)(Artículo\\s+\\d+(?:[º°]\\.?|\\.))\\s*', r'\\1\\2 ', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'(^|\\n)(Art\\.\\s*\\d+\\.?)\\s*', r'\\1\\2 ', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'(^|\\n)((?:I{1,4}|V?I{0,3}|X{1,3})\\s*(?:\\.|-))\\s*', r'\\1\\2 ', s)\n",
    "    s = re.sub(r'(^|\\n)(\\d+\\s*(?:\\.|\\)|-))\\s*', r'\\1\\2 ', s)\n",
    "    s = re.sub(r'(^|\\n)([A-Za-zÁÉÍÓÚÜÑ]\\))\\s*', r'\\1\\2 ', s)\n",
    "    return s\n",
    "\n",
    "# --------------------------------- API principal ---------------------------------\n",
    "def clean_raw_text(raw: str, title_candidate: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza conservadora con \"una línea por párrafo\".\n",
    "    Pasos:\n",
    "      1) Unicode & controles (NFC, NBSP→espacio, quitar zero-width, normalizar CR/CRLF)\n",
    "      2) Quitar folios/pies de página y encabezados/pies repetidos (>=3)\n",
    "      3) Normalizar variantes espaciadas de TRANSITORIOS\n",
    "      4) Reflujo: construir párrafos de UNA línea (HEAD/ART./ENUM inician párrafo)\n",
    "      5) Normalización intralínea y tidy final\n",
    "    \"\"\"\n",
    "    txt = _normalize_unicode(raw)\n",
    "    txt = _strip_trailing_spaces(txt)\n",
    "    txt = _remove_page_counters_and_repeated_headers(txt)\n",
    "    txt = _normalize_transitorios_spaced(txt)\n",
    "\n",
    "    # *** clave: una línea por párrafo ***\n",
    "    txt = _paragraphs_to_one_line(txt)\n",
    "\n",
    "    # Ajustes finos de espacios\n",
    "    txt = _intraline_space_normalize(txt)\n",
    "\n",
    "    # Tidy final: idempotencia y salto final único\n",
    "    txt = txt.strip() + \"\\n\"\n",
    "    return txt\n",
    "\n",
    "# ------------------------------------ Driver ------------------------------------\n",
    "def step2_clean_raw(catalog_csv: Path = CATALOG_CSV):\n",
    "    \"\"\"\n",
    "    Step 2: Limpia y normaliza archivos raw -> clean_{base}.txt\n",
    "    - Lee RAW_TXT_DIR / raw_*.txt\n",
    "    - Escribe CLEAN_DIR / clean_{base}.txt\n",
    "    - Manifiesto con estadísticas (si pandas disponible)\n",
    "    - Logs claros por archivo\n",
    "    \"\"\"\n",
    "    def log(msg: str, level: str = \"INFO\"):\n",
    "        print(f\"[{level}] {msg}\", flush=True)\n",
    "\n",
    "    print(\"Starting Step 2: Text Cleaning & Normalization (one paragraph = one line)...\")\n",
    "    CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    catalog = None\n",
    "    if load_catalog:\n",
    "        try:\n",
    "            catalog = load_catalog(catalog_csv)\n",
    "            log(f\"Catalog loaded: {catalog_csv}\")\n",
    "        except Exception as e:\n",
    "            write_error(\"global\", \"catalog_load_failed\", f\"Could not load catalog: {e}\", {\"catalog_csv\": str(catalog_csv)})\n",
    "            log(f\"Catalog load failed: {e}\", level=\"WARN\")\n",
    "\n",
    "    raw_files = sorted(RAW_TXT_DIR.glob(\"raw_*.txt\"))\n",
    "    log(f\"Found {len(raw_files)} raw text files in {RAW_TXT_DIR}\")\n",
    "\n",
    "    records: List[Dict] = []\n",
    "    total_chars_raw = 0\n",
    "    total_chars_clean = 0\n",
    "    cleaned_count = 0\n",
    "    skipped_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for raw_path in raw_files:\n",
    "        base = raw_path.stem.replace(\"raw_\", \"\")\n",
    "        fname = raw_path.name\n",
    "\n",
    "        if catalog is not None and base not in catalog:\n",
    "            write_error(base, \"catalog_missing\", \"file_num not found in catalog (clean stage)\", {\"raw_file\": fname})\n",
    "            log(f\"{fname} → skipping (not in catalog)\", level=\"WARN\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            log(f\"Cleaning {fname} …\")\n",
    "            raw_text = raw_path.read_text(encoding=\"utf-8\")\n",
    "            meta = catalog[base] if catalog is not None else None\n",
    "            title = getattr(meta, \"law_name\", None) if meta is not None else None\n",
    "\n",
    "            cleaned = clean_raw_text(raw_text, title)\n",
    "\n",
    "            out_path = CLEAN_DIR / f\"clean_{base}.txt\"\n",
    "            write_text(out_path, cleaned)\n",
    "\n",
    "            stats_raw = count_stats(raw_text)\n",
    "            stats_clean = count_stats(cleaned)\n",
    "            total_chars_raw += stats_raw.get(\"chars\", 0)\n",
    "            total_chars_clean += stats_clean.get(\"chars\", 0)\n",
    "\n",
    "            rec = {\"stage\": \"clean\", \"source_raw\": fname, \"base\": base, **stats_clean}\n",
    "            records.append(rec)\n",
    "            cleaned_count += 1\n",
    "\n",
    "            log(f\"Wrote {out_path.name} (lines={stats_clean.get('lines')}, words={stats_clean.get('words')}, chars={stats_clean.get('chars')})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            write_error(base, \"clean_failed\", f\"Exception during cleaning: {e}\", {\"raw_file\": fname})\n",
    "            log(f\"{fname} → cleaning failed: {e}\", level=\"ERROR\")\n",
    "            failed_count += 1\n",
    "\n",
    "    # Manifest opcional\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if records:\n",
    "            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(records)\n",
    "            manifest_path = OUTPUT_DIR / \"manifest_clean.csv\"\n",
    "            df.to_csv(manifest_path, index=False, encoding=\"utf-8\")\n",
    "            log(f\"Manifest saved to {manifest_path}\")\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        log(f\"Could not write manifest (pandas missing or error: {e})\", level=\"WARN\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(records) if records else pd.DataFrame()\n",
    "        except Exception:\n",
    "            df = None\n",
    "\n",
    "    # Resumen\n",
    "    reduction = (1 - (total_chars_clean / total_chars_raw)) * 100 if total_chars_raw else 0.0\n",
    "    log(f\"Summary: cleaned={cleaned_count}, skipped={skipped_count}, failed={failed_count}\")\n",
    "    if total_chars_raw:\n",
    "        log(f\"Text size reduction: {reduction:.1f}%\")\n",
    "    log(f\"Step 2 complete: {cleaned_count} file(s) written to {CLEAN_DIR}\")\n",
    "\n",
    "    return df if (df is not None and not df.empty) else None\n",
    "\n",
    "# ------------------------------- Ejecución directa --------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    _ = step2_clean_raw(CATALOG_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897daf4",
   "metadata": {},
   "source": [
    "## **Step 3: Document Structure Splitting**  ESTE PASO HACERLO EN EL JSONPARSER\n",
    "\n",
    "### **Purpose**\n",
    "Intelligently split cleaned legal documents into their constituent parts: decree sections, main law content, and transitional provisions. This separation enables specialized processing of different document types.\n",
    "\n",
    "### **Document Structure Recognition**\n",
    "Mexican legal documents typically follow this structure:\n",
    "1. **Decreto** - Government decree and preamble (before main law)\n",
    "2. **Ley** - Main law content with hierarchical structure (títulos, capítulos, artículos)  \n",
    "3. **Transitorios** - Transitional provisions and implementation rules\n",
    "\n",
    "### **Splitting Algorithm**\n",
    "- **Title Detection** - Identifies law title using catalog metadata (first two words)\n",
    "- **Hierarchy Recognition** - Detects formal legal structure markers\n",
    "- **Transitional Identification** - Locates \"Transitorios\" sections\n",
    "- **Intelligent Fallback** - Handles documents that don't follow standard format\n",
    "\n",
    "### **Quality Assurance**\n",
    "- Validates structural patterns against expected legal document format\n",
    "- Logs splitting decisions and potential issues\n",
    "- Preserves content integrity across all document sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0acf632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created successfully!\n",
      "Base directory: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\n",
      "Input PDFs: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Raw\n",
      "Catalog: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Raw\\index.csv\n",
      "Final JSON output: c:\\Users\\braul\\Documents\\_ITAMLaptop\\Datalab\\DataMakers\\Leyes\\14\\Refined\\json\n"
     ]
    }
   ],
   "source": [
    "# ============== Directory Configuration ==============\n",
    "\n",
    "# Base working directory (current notebook location)\n",
    "BASE_DIR     = Path.cwd()\n",
    "\n",
    "# === INPUT PATHS ===\n",
    "\n",
    "\n",
    "RAW_DIR      = BASE_DIR / \"Raw\"                 # Source PDF files location\n",
    "CATALOG_CSV  = RAW_DIR / \"index.csv\"           # Metadata catalog for PDFs\n",
    "\n",
    "# === OUTPUT ROOT ===\n",
    "OUTPUT_DIR   = BASE_DIR / \"Refined\"             # All processed outputs go here\n",
    "TEMP_DIR     = BASE_DIR / \"temp\"                # Temporary processing files\n",
    "\n",
    "# === INTERMEDIATE PROCESSING DIRECTORIES ===\n",
    "RAW_TXT_DIR  = OUTPUT_DIR / TEMP_DIR / \"raw_txt\"   # Step 1: Raw PDF text extraction\n",
    "CLEAN_DIR    = OUTPUT_DIR / TEMP_DIR / \"clean\"     # Step 2: Cleaned text files\n",
    "\n",
    "# === DOCUMENT TYPE SEPARATION (Step 3 outputs) ===\n",
    "LEY_DIR      = OUTPUT_DIR / \"leyes\"             # Main law content\n",
    "DECR_DIR     = OUTPUT_DIR / \"decretos\"          # Government decree sections  \n",
    "TRANS_DIR    = OUTPUT_DIR / \"transitorios\"      # Transitional provisions\n",
    "\n",
    "# === FINAL OUTPUTS ===\n",
    "JSON_DIR     = OUTPUT_DIR / \"json\"              # Structured JSON documents\n",
    "ERRORES_DIR  = OUTPUT_DIR / \"errores\"           # Error logs and validation reports\n",
    "\n",
    "# Create all necessary directories (parents=True creates nested paths)\n",
    "for d in [OUTPUT_DIR, RAW_TXT_DIR, TEMP_DIR, CLEAN_DIR, LEY_DIR, DECR_DIR, TRANS_DIR, JSON_DIR, ERRORES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directory structure created successfully!\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Input PDFs: {RAW_DIR}\")  \n",
    "print(f\"Catalog: {CATALOG_CSV}\")\n",
    "print(f\"Final JSON output: {JSON_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1353fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal structure patterns loaded:\n",
      "   Hierarchy levels: 11 types\n",
      "   Pattern matching: Case-insensitive with flexible spacing\n",
      "   Transitional sections: Supports both normal and spaced formatting\n"
     ]
    }
   ],
   "source": [
    "# ============== Legal Document Structure Recognition ==============\n",
    "\n",
    "# Hierarchical structure vocabulary for Mexican legal documents\n",
    "# These terms appear in formal legal headers in order of hierarchy\n",
    "HIER = [\n",
    "    \"disposiciones preliminares\", \"disposiciones generales\",\n",
    "    \"libro\", \"titulo\", \"capitulo\", \"seccion\",\n",
    "    \"articulo\", \"art.\",                 # ← añade “art.”\n",
    "    \"capitulo unico\", \"seccion unica\", \"titulo preliminar\",\n",
    "]\n",
    "\n",
    "# El resto del HIER_RE igual (usando norm_lower(line))\n",
    "\n",
    "\n",
    "# Build regex pattern to match hierarchical headers\n",
    "# Matches any of the hierarchy terms at the start of a line (case-insensitive)\n",
    "HIER_RE = re.compile(\n",
    "    r\"^\\s*(?:%s)\\b\" % \"|\".join(re.escape(h) for h in HIER),\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "# Flexible \"Transitorios\" pattern recognition\n",
    "# Handles both spaced (\"T R A N S I T O R I O S\") and normal spelling\n",
    "# Accounts for potential leading whitespace and case variations\n",
    "TRANS_RE = re.compile(\n",
    "    r\"^\\s*(?:\"\n",
    "    r\"t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\\s*s\"  # T R A N S I T O R I O S\n",
    "    r\"|t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\"     # T R A N S I T O R I O\n",
    "    r\"|transitorios\"\n",
    "    r\"|transitorio\"\n",
    "    r\"|articulos?\\s+transitorios\"                      # \"Artículos Transitorios\"\n",
    "    r\")\\b\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Legal structure patterns loaded:\")\n",
    "print(f\"   Hierarchy levels: {len(HIER)} types\")\n",
    "print(f\"   Pattern matching: Case-insensitive with flexible spacing\")\n",
    "print(f\"   Transitional sections: Supports both normal and spaced formatting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _contains_allcaps_prefix(line: str, prefix_caps: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if the normalized-uppercase view of 'line' contains the two-word\n",
    "    ALL-CAPS prefix 'prefix_caps' as a token span, accent-insensitive.\n",
    "    \"\"\"\n",
    "    head = _norm_caps(line)\n",
    "    pref = _norm_caps(prefix_caps)\n",
    "    if not pref:\n",
    "        return False\n",
    "    # Whole-token boundaries: not preceded/followed by A–Z/0–9\n",
    "    pat = re.compile(rf\"(?<![A-Z0-9]){re.escape(pref)}(?![A-Z0-9])\")\n",
    "    return bool(pat.search(head))\n",
    "\n",
    "def _next_nonempty_index(lines: List[str], j: int) -> Optional[int]:\n",
    "    n = len(lines)\n",
    "    while j < n and not lines[j].strip():\n",
    "        j += 1\n",
    "    return j if j < n else None\n",
    "\n",
    "def find_decreto_ley_start_two_words(\n",
    "    lines: List[str],\n",
    "    first_two_caps: str,\n",
    "    max_lookahead: int = 120,\n",
    ") -> Optional[Tuple[int, int]]:\n",
    "    n = len(lines)\n",
    "    # 1) Fase \"estricta\"\n",
    "    for i in range(n - 1):\n",
    "        if _contains_allcaps_prefix(lines[i], first_two_caps):\n",
    "            j = _next_nonempty_index(lines, i + 1)\n",
    "            if j is not None and HIER_RE.search(norm_lower(lines[j].lstrip())):\n",
    "                return i, j\n",
    "            # 2) Fase \"relajada\": ventana\n",
    "            start = (j if j is not None else i + 1)\n",
    "            end = min(n, start + max_lookahead)\n",
    "            for k in range(start, end):\n",
    "                ln = norm_lower(lines[k].lstrip())\n",
    "                if TRANS_RE.search(ln):  # si llegas primero a Transitorios, descarta este título\n",
    "                    break\n",
    "                if HIER_RE.search(ln):\n",
    "                    return i, k\n",
    "    return None\n",
    "\n",
    "\n",
    "def first_transitorios_after(lines: List[str], start: int) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Find the first index >= start+1 that looks like a 'Transitorios' heading,\n",
    "    tolerant of leading spaces and both spaced/plain spellings.\n",
    "    \"\"\"\n",
    "    for i in range(max(start + 1, 0), len(lines)):\n",
    "        if TRANS_RE.search(norm_lower(lines[i].lstrip())):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_blocks_two_word_strict(cleaned_text: str, first_two_caps: str, base: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split a cleaned text into:\n",
    "      - decreto: everything before the ALL-CAPS two-word title line\n",
    "      - ley: from the first HIER line after that title, up to 'Transitorios' (if any)\n",
    "      - transitorios: from 'Transitorios' to end (if present)\n",
    "    If the (title -> header) pair is not found, everything (up to 'Transitorios') is put into 'decreto'.\n",
    "    \"\"\"\n",
    "    lines = cleaned_text.splitlines()\n",
    "    pair = find_decreto_ley_start_two_words(lines, first_two_caps)\n",
    "\n",
    "    # Fallback: couldn't find the (title → header) pair\n",
    "    if pair is None:\n",
    "        t_idx = first_transitorios_after(lines, 0)\n",
    "        if t_idx is not None:\n",
    "            decreto = \"\\n\".join(lines[:t_idx]).strip()\n",
    "            tran    = \"\\n\".join(lines[t_idx:]).strip()\n",
    "        else:\n",
    "            decreto = cleaned_text\n",
    "            tran    = \"\"\n",
    "        write_error(base, \"two_word_pair_not_found\",\n",
    "                    \"No (ALL-CAPS two-word title line → HIER) pair; ley not split.\",\n",
    "                    {\"clean\": f\"clean_{base}.txt\"})\n",
    "        return {\"decreto\": decreto, \"ley\": \"\", \"transitorios\": tran}\n",
    "\n",
    "    title_idx, ley_start = pair\n",
    "    decreto = \"\\n\".join(lines[:title_idx]).strip()\n",
    "\n",
    "    t_idx = first_transitorios_after(lines, ley_start)\n",
    "    if t_idx is not None and t_idx > ley_start:\n",
    "        ley  = \"\\n\".join(lines[ley_start: t_idx]).strip()\n",
    "        tran = \"\\n\".join(lines[t_idx:]).strip()\n",
    "        \n",
    "    else:\n",
    "        ley  = \"\\n\".join(lines[ley_start:]).strip()\n",
    "        tran = \"\"\n",
    "    return {\"decreto\": decreto, \"ley\": ley, \"transitorios\": tran}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84361ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === STEP 3 — Atomic: split ONE cleaned file into decreto/ley/transitorios (con banderas) ===\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Asume que ya existen:\n",
    "# # - load_catalog, LawMeta\n",
    "# # - split_blocks_two_word_strict\n",
    "# # - write_text, write_error (JSON estructurado)\n",
    "# # - count_stats (devuelve dict con keys: lines, words, chars)\n",
    "\n",
    "# # Directorios base (reutiliza los de tu pipeline si ya existen)\n",
    "# CLEAN_DIR   = Path(\"temp/clean\")     # clean_{file_num}.txt\n",
    "# OUTPUT_DIR  = Path(\"Refined\")        # carpeta base de salida\n",
    "\n",
    "# DEC_DIR = OUTPUT_DIR / \"decretos\"\n",
    "# LEY_DIR = OUTPUT_DIR / \"leyes\"\n",
    "# TRA_DIR = OUTPUT_DIR / \"transitorios\"\n",
    "# ERR_DIR = OUTPUT_DIR / \"errores\"\n",
    "# MANIFEST_PARTS = OUTPUT_DIR / \"manifest_parts.csv\"\n",
    "\n",
    "# for d in (DEC_DIR, LEY_DIR, TRA_DIR, ERR_DIR):\n",
    "#     d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# def _append_manifest_rows(rows: list[dict]) -> None:\n",
    "#     \"\"\"Append rows to manifest_parts.csv (crea encabezados si no existe).\"\"\"\n",
    "#     import csv\n",
    "#     cols = [\"file_num\",\"part\",\"lines\",\"words\",\"chars\",\"path\",\"created_at\"]\n",
    "#     new_file = not MANIFEST_PARTS.exists()\n",
    "#     with MANIFEST_PARTS.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         w = csv.DictWriter(f, fieldnames=cols)\n",
    "#         if new_file:\n",
    "#             w.writeheader()\n",
    "#         for r in rows:\n",
    "#             w.writerow(r)\n",
    "\n",
    "# def process_one_file(file_num: str, catalog: dict[str, \"LawMeta\"]) -> dict:\n",
    "#     \"\"\"\n",
    "#     Procesa UN archivo clean_{file_num}.txt:\n",
    "#       - lee el limpio\n",
    "#       - hace split con split_blocks_two_word_strict\n",
    "#       - escribe partes (decreto/ley/transitorios)\n",
    "#       - anexa renglones al manifest_parts.csv\n",
    "#       - retorna dict con 'ok' y 'paths' escritos\n",
    "#     Con banderas de estado por consola y advertencias si faltan partes.\n",
    "#     \"\"\"\n",
    "#     clean_path = CLEAN_DIR / f\"clean_{file_num}.txt\"\n",
    "#     print(f\"[INFO] Procesando {clean_path.name}...\")\n",
    "\n",
    "#     if not clean_path.exists():\n",
    "#         msg = f\"No existe {clean_path.name} para split Step 3\"\n",
    "#         print(f\"[WARNING] {msg}\")\n",
    "#         write_error(file_num, \"clean_missing\", msg, {\"clean_path\": str(clean_path)})\n",
    "#         return {\"ok\": False, \"reason\": \"clean_missing\"}\n",
    "\n",
    "#     meta = catalog.get(file_num)\n",
    "#     if not meta:\n",
    "#         msg = \"file_num no encontrado en catálogo (split stage)\"\n",
    "#         print(f\"[WARNING] {msg} → {file_num}\")\n",
    "#         write_error(file_num, \"catalog_missing\", msg, {\"file_num\": file_num})\n",
    "#         return {\"ok\": False, \"reason\": \"catalog_missing\"}\n",
    "\n",
    "#     # Lee texto limpio y divide en partes\n",
    "#     cleaned_text = clean_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "#     try:\n",
    "#         parts = split_blocks_two_word_strict(\n",
    "#             cleaned_text=cleaned_text,\n",
    "#             first_two_caps=meta.first_two_caps,\n",
    "#             base=file_num,\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         msg = f\"Excepción al dividir {clean_path.name}: {type(e).__name__}: {e}\"\n",
    "#         print(f\"[ERROR] {msg}\")\n",
    "#         write_error(file_num, \"split_exception\", msg, {\"first_two_caps\": meta.first_two_caps})\n",
    "#         return {\"ok\": False, \"reason\": \"split_exception\"}\n",
    "\n",
    "#     # Helper para escribir partes + recolectar stats\n",
    "#     now = datetime.now().isoformat(timespec=\"seconds\")\n",
    "#     out_rows, out_paths = [], {}\n",
    "\n",
    "#     def _emit(part_key: str, out_dir: Path):\n",
    "#         content = (parts.get(part_key) or \"\").strip()\n",
    "#         if not content:\n",
    "#             print(f\"[WARNING] {part_key.upper()} PARA {clean_path.name} NO ENCONTRADO\")\n",
    "#             write_error(file_num, f\"{part_key}_missing\",\n",
    "#                         f\"{part_key} vacío tras split\",\n",
    "#                         {\"file\": clean_path.name, \"first_two_caps\": meta.first_two_caps})\n",
    "#             return\n",
    "#         outp = out_dir / f\"{file_num}.txt\"\n",
    "#         write_text(outp, content)\n",
    "#         stats = count_stats(content)  # dict: lines, words, chars\n",
    "#         out_rows.append({\n",
    "#             \"file_num\": file_num,\n",
    "#             \"part\": part_key,\n",
    "#             \"lines\": stats[\"lines\"],\n",
    "#             \"words\": stats[\"words\"],\n",
    "#             \"chars\": stats[\"chars\"],\n",
    "#             \"path\": str(outp),\n",
    "#             \"created_at\": now,\n",
    "#         })\n",
    "#         out_paths[part_key] = str(outp)\n",
    "#         print(f\"[OK] {part_key} guardado → {outp.name} \"\n",
    "#               f\"({stats['lines']} líneas, {stats['words']} palabras, {stats['chars']} chars)\")\n",
    "\n",
    "#     # Intentar exportar todas las partes\n",
    "#     _emit(\"decreto\", DEC_DIR)\n",
    "#     _emit(\"ley\",      LEY_DIR)\n",
    "#     _emit(\"transitorios\", TRA_DIR)\n",
    "\n",
    "#     if out_rows:\n",
    "#         _append_manifest_rows(out_rows)\n",
    "#         print(f\"[INFO] Manifest actualizado para {clean_path.name} (+{len(out_rows)} filas)\")\n",
    "#     else:\n",
    "#         print(f\"[WARNING] Ninguna parte escrita para {clean_path.name}\")\n",
    "\n",
    "#     # Auditoría si no hubo 'ley'\n",
    "#     if not (parts.get(\"ley\") or \"\").strip():\n",
    "#         msg = \"Ley vacía tras split; revisar heurística o limpieza\"\n",
    "#         print(f\"[WARNING] {msg} → {clean_path.name}\")\n",
    "#         write_error(file_num, \"ley_missing\", msg, {\"first_two_caps\": meta.first_two_caps})\n",
    "\n",
    "#     print(f\"[DONE] {clean_path.name} procesado.\")\n",
    "#     return {\"ok\": True, \"paths\": out_paths, \"has_ley\": bool((parts.get(\"ley\") or \"\").strip())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a699495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === STEP 3 — Atomic: split ONE cleaned file into decreto/ley/transitorios (prints mínimos) ===\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Asume que ya existen en el entorno:\n",
    "# - load_catalog, LawMeta\n",
    "# - split_blocks_two_word_strict\n",
    "# - write_text, write_error (JSON estructurado)\n",
    "# - count_stats (devuelve dict con keys: lines, words, chars)\n",
    "\n",
    "# Directorios base\n",
    "CLEAN_DIR   = Path(\"temp/clean\")     # clean_{file_num}.txt\n",
    "OUTPUT_DIR  = Path(\"Refined\")        # carpeta base de salida\n",
    "\n",
    "DEC_DIR = OUTPUT_DIR / \"decretos\"\n",
    "LEY_DIR = OUTPUT_DIR / \"leyes\"\n",
    "TRA_DIR = OUTPUT_DIR / \"transitorios\"\n",
    "ERR_DIR = OUTPUT_DIR / \"errores\"\n",
    "MANIFEST_PARTS = OUTPUT_DIR / \"manifest_parts.csv\"\n",
    "\n",
    "for d in (DEC_DIR, LEY_DIR, TRA_DIR, ERR_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _append_manifest_rows(rows: list[dict]) -> None:\n",
    "    \"\"\"Append rows to manifest_parts.csv (crea encabezados si no existe).\"\"\"\n",
    "    import csv\n",
    "    cols = [\"file_num\",\"part\",\"lines\",\"words\",\"chars\",\"path\",\"created_at\"]\n",
    "    new_file = not MANIFEST_PARTS.exists()\n",
    "    with MANIFEST_PARTS.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        if new_file:\n",
    "            w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def process_one_file(file_num: str, catalog: dict[str, \"LawMeta\"]) -> dict:\n",
    "    \"\"\"\n",
    "    Procesa UN archivo clean_{file_num}.txt:\n",
    "      - lee el limpio\n",
    "      - split con split_blocks_two_word_strict\n",
    "      - escribe partes (decr_/law_/tran_)\n",
    "      - agrega filas al manifest (silencioso)\n",
    "    Prints: solo INFO de procesamiento y ERROR cuando falta decreto/ley/transitorios.\n",
    "    \"\"\"\n",
    "    clean_path = CLEAN_DIR / f\"clean_{file_num}.txt\"\n",
    "    print(f\"[INFO] Procesando {clean_path.name}...\")\n",
    "\n",
    "    if not clean_path.exists():\n",
    "        # Error fatal: no hay clean; dejamos también registro estructurado\n",
    "        write_error(file_num, \"clean_missing\",\n",
    "                    f\"No existe {clean_path.name} para split Step 3\",\n",
    "                    {\"clean_path\": str(clean_path)})\n",
    "        # No añadimos más prints distintos a los solicitados\n",
    "        return {\"ok\": False, \"reason\": \"clean_missing\"}\n",
    "\n",
    "    meta = catalog.get(file_num)\n",
    "    if not meta:\n",
    "        write_error(file_num, \"catalog_missing\",\n",
    "                    \"file_num no encontrado en catálogo (split stage)\",\n",
    "                    {\"file_num\": file_num})\n",
    "        return {\"ok\": False, \"reason\": \"catalog_missing\"}\n",
    "\n",
    "    # Lee texto limpio y divide en partes\n",
    "    cleaned_text = clean_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    try:\n",
    "        parts = split_blocks_two_word_strict(\n",
    "            cleaned_text=cleaned_text,\n",
    "            first_two_caps=meta.first_two_caps,\n",
    "            base=file_num,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        write_error(file_num, \"split_exception\",\n",
    "                    f\"Excepción al dividir {clean_path.name}: {type(e).__name__}: {e}\",\n",
    "                    {\"first_two_caps\": meta.first_two_caps})\n",
    "        return {\"ok\": False, \"reason\": \"split_exception\"}\n",
    "\n",
    "    # Prefijos de salida por tipo de parte\n",
    "    prefix_map = {\n",
    "        \"decreto\": \"decr\",\n",
    "        \"ley\": \"law\",\n",
    "        \"transitorios\": \"tran\",\n",
    "    }\n",
    "    dir_map = {\n",
    "        \"decreto\": DEC_DIR,\n",
    "        \"ley\": LEY_DIR,\n",
    "        \"transitorios\": TRA_DIR,\n",
    "    }\n",
    "\n",
    "    now = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    out_rows, out_paths = [], {}\n",
    "\n",
    "    def _emit(part_key: str):\n",
    "        content = (parts.get(part_key) or \"\").strip()\n",
    "        if not content:\n",
    "            if part_key == \"ley\":\n",
    "                # Falla real del split → error y registro estructurado\n",
    "                print(f\"[ERROR] {part_key} NO encontrado \\u2192 {file_num}.txt\")\n",
    "                write_error(\n",
    "                    file_num,\n",
    "                    f\"{part_key}_missing\",\n",
    "                    f\"{part_key} vacío tras split\",\n",
    "                    {\"file\": clean_path.name, \"first_two_caps\": meta.first_two_caps},\n",
    "                )\n",
    "            else:\n",
    "                # Falta esperada en muchos casos (decreto / transitorios) → solo INFO, sin write_error\n",
    "                print(f\"[INFO] {part_key} NO encontrado \\u2192 {file_num}.txt\")\n",
    "            return\n",
    "\n",
    "        out_dir = dir_map[part_key]\n",
    "        outp = out_dir / f\"{prefix_map[part_key]}_{file_num}.txt\"\n",
    "        write_text(outp, content)\n",
    "        stats = count_stats(content)  # dict: lines, words, chars\n",
    "        out_rows.append({\n",
    "            \"file_num\": file_num,\n",
    "            \"part\": part_key,\n",
    "            \"lines\": stats[\"lines\"],\n",
    "            \"words\": stats[\"words\"],\n",
    "            \"chars\": stats[\"chars\"],\n",
    "            \"path\": str(outp),\n",
    "            \"created_at\": now,\n",
    "        })\n",
    "        out_paths[part_key] = str(outp)\n",
    "\n",
    "\n",
    "    # Exportar (silencioso si se guardó; solo avisa si falta)\n",
    "    _emit(\"decreto\")\n",
    "    _emit(\"ley\")\n",
    "    _emit(\"transitorios\")\n",
    "\n",
    "    if out_rows:\n",
    "        _append_manifest_rows(out_rows)\n",
    "\n",
    "    return {\"ok\": True, \"paths\": out_paths, \"has_ley\": bool((parts.get(\"ley\") or \"\").strip())}\n",
    "\n",
    "\n",
    "\n",
    "#! VOLVER A CORRER ESTO !!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b40485f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 3 | Inicio: Split decreto / ley / transitorios ===\n",
      "[INFO] Archivos detectados en temp\\clean: 301\n",
      "[INFO] Procesando clean_0001.txt...\n",
      "[INFO] Procesando clean_0002.txt...\n",
      "[INFO] Procesando clean_0003.txt...\n",
      "[INFO] Procesando clean_0004.txt...\n",
      "[INFO] Procesando clean_0005.txt...\n",
      "[INFO] Procesando clean_0006.txt...\n",
      "[INFO] Procesando clean_0007.txt...\n",
      "[INFO] Procesando clean_0008.txt...\n",
      "[INFO] Procesando clean_0009.txt...\n",
      "[INFO] Procesando clean_0010.txt...\n",
      "[INFO] Procesando clean_0011.txt...\n",
      "[INFO] Procesando clean_0012.txt...\n",
      "[INFO] Procesando clean_0013.txt...\n",
      "[INFO] Procesando clean_0014.txt...\n",
      "[INFO] Procesando clean_0015.txt...\n",
      "[INFO] Procesando clean_0016.txt...\n",
      "[INFO] decreto NO encontrado → 0016.txt\n",
      "[INFO] Procesando clean_0017.txt...\n",
      "[INFO] Procesando clean_0018.txt...\n",
      "[INFO] Procesando clean_0019.txt...\n",
      "[INFO] decreto NO encontrado → 0019.txt\n",
      "[INFO] transitorios NO encontrado → 0019.txt\n",
      "[INFO] Procesando clean_0020.txt...\n",
      "[INFO] Procesando clean_0021.txt...\n",
      "[INFO] Procesando clean_0022.txt...\n",
      "[INFO] Procesando clean_0023.txt...\n",
      "[INFO] Procesando clean_0024.txt...\n",
      "[INFO] Procesando clean_0025.txt...\n",
      "[INFO] Procesando clean_0026.txt...\n",
      "[INFO] Procesando clean_0027.txt...\n",
      "[INFO] Procesando clean_0028.txt...\n",
      "[INFO] Procesando clean_0029.txt...\n",
      "[INFO] Procesando clean_0030.txt...\n",
      "[INFO] Procesando clean_0031.txt...\n",
      "[INFO] Procesando clean_0032.txt...\n",
      "[INFO] Procesando clean_0033.txt...\n",
      "[INFO] Procesando clean_0034.txt...\n",
      "[INFO] Procesando clean_0035.txt...\n",
      "[INFO] Procesando clean_0036.txt...\n",
      "[INFO] Procesando clean_0037.txt...\n",
      "[INFO] Procesando clean_0038.txt...\n",
      "[INFO] Procesando clean_0039.txt...\n",
      "[INFO] Procesando clean_0040.txt...\n",
      "[ERROR] ley NO encontrado → 0040.txt\n",
      "[INFO] Procesando clean_0041.txt...\n",
      "[INFO] Procesando clean_0042.txt...\n",
      "[INFO] Procesando clean_0043.txt...\n",
      "[INFO] Procesando clean_0044.txt...\n",
      "[INFO] Procesando clean_0045.txt...\n",
      "[INFO] Procesando clean_0046.txt...\n",
      "[INFO] Procesando clean_0047.txt...\n",
      "[INFO] Procesando clean_0048.txt...\n",
      "[INFO] Procesando clean_0049.txt...\n",
      "[INFO] Procesando clean_0050.txt...\n",
      "[INFO] Procesando clean_0051.txt...\n",
      "[INFO] Procesando clean_0052.txt...\n",
      "[INFO] Procesando clean_0053.txt...\n",
      "[INFO] Procesando clean_0054.txt...\n",
      "[INFO] Procesando clean_0055.txt...\n",
      "[INFO] decreto NO encontrado → 0055.txt\n",
      "[INFO] Procesando clean_0056.txt...\n",
      "[INFO] Procesando clean_0057.txt...\n",
      "[INFO] transitorios NO encontrado → 0057.txt\n",
      "[INFO] Procesando clean_0058.txt...\n",
      "[INFO] Procesando clean_0059.txt...\n",
      "[INFO] Procesando clean_0060.txt...\n",
      "[INFO] Procesando clean_0061.txt...\n",
      "[INFO] Procesando clean_0062.txt...\n",
      "[INFO] Procesando clean_0063.txt...\n",
      "[INFO] Procesando clean_0064.txt...\n",
      "[INFO] Procesando clean_0065.txt...\n",
      "[INFO] Procesando clean_0066.txt...\n",
      "[INFO] Procesando clean_0067.txt...\n",
      "[INFO] Procesando clean_0068.txt...\n",
      "[INFO] Procesando clean_0069.txt...\n",
      "[INFO] transitorios NO encontrado → 0069.txt\n",
      "[INFO] Procesando clean_0070.txt...\n",
      "[INFO] Procesando clean_0071.txt...\n",
      "[INFO] Procesando clean_0072.txt...\n",
      "[INFO] Procesando clean_0073.txt...\n",
      "[INFO] Procesando clean_0074.txt...\n",
      "[INFO] Procesando clean_0075.txt...\n",
      "[INFO] Procesando clean_0076.txt...\n",
      "[INFO] Procesando clean_0077.txt...\n",
      "[INFO] Procesando clean_0078.txt...\n",
      "[INFO] Procesando clean_0079.txt...\n",
      "[INFO] Procesando clean_0080.txt...\n",
      "[INFO] Procesando clean_0081.txt...\n",
      "[INFO] transitorios NO encontrado → 0081.txt\n",
      "[INFO] Procesando clean_0082.txt...\n",
      "[INFO] decreto NO encontrado → 0082.txt\n",
      "[INFO] Procesando clean_0083.txt...\n",
      "[INFO] Procesando clean_0084.txt...\n",
      "[INFO] transitorios NO encontrado → 0084.txt\n",
      "[INFO] Procesando clean_0085.txt...\n",
      "[INFO] Procesando clean_0086.txt...\n",
      "[INFO] Procesando clean_0087.txt...\n",
      "[INFO] Procesando clean_0088.txt...\n",
      "[INFO] Procesando clean_0089.txt...\n",
      "[INFO] Procesando clean_0090.txt...\n",
      "[INFO] Procesando clean_0091.txt...\n",
      "[INFO] Procesando clean_0092.txt...\n",
      "[INFO] Procesando clean_0093.txt...\n",
      "[INFO] Procesando clean_0094.txt...\n",
      "[INFO] Procesando clean_0095.txt...\n",
      "[INFO] Procesando clean_0096.txt...\n",
      "[INFO] Procesando clean_0097.txt...\n",
      "[INFO] Procesando clean_0098.txt...\n",
      "[INFO] Procesando clean_0099.txt...\n",
      "[INFO] Procesando clean_0100.txt...\n",
      "[INFO] Procesando clean_0101.txt...\n",
      "[INFO] Procesando clean_0102.txt...\n",
      "[INFO] Procesando clean_0103.txt...\n",
      "[INFO] Procesando clean_0104.txt...\n",
      "[INFO] Procesando clean_0105.txt...\n",
      "[INFO] Procesando clean_0106.txt...\n",
      "[INFO] Procesando clean_0107.txt...\n",
      "[INFO] Procesando clean_0108.txt...\n",
      "[INFO] Procesando clean_0109.txt...\n",
      "[INFO] Procesando clean_0110.txt...\n",
      "[INFO] Procesando clean_0111.txt...\n",
      "[INFO] Procesando clean_0112.txt...\n",
      "[INFO] Procesando clean_0113.txt...\n",
      "[INFO] Procesando clean_0114.txt...\n",
      "[INFO] transitorios NO encontrado → 0114.txt\n",
      "[INFO] Procesando clean_0115.txt...\n",
      "[INFO] transitorios NO encontrado → 0115.txt\n",
      "[INFO] Procesando clean_0116.txt...\n",
      "[INFO] Procesando clean_0117.txt...\n",
      "[INFO] Procesando clean_0118.txt...\n",
      "[INFO] Procesando clean_0119.txt...\n",
      "[INFO] Procesando clean_0120.txt...\n",
      "[INFO] Procesando clean_0121.txt...\n",
      "[INFO] Procesando clean_0122.txt...\n",
      "[INFO] Procesando clean_0123.txt...\n",
      "[INFO] Procesando clean_0124.txt...\n",
      "[INFO] Procesando clean_0125.txt...\n",
      "[INFO] Procesando clean_0126.txt...\n",
      "[INFO] Procesando clean_0127.txt...\n",
      "[INFO] Procesando clean_0128.txt...\n",
      "[INFO] Procesando clean_0129.txt...\n",
      "[INFO] Procesando clean_0130.txt...\n",
      "[INFO] Procesando clean_0131.txt...\n",
      "[INFO] Procesando clean_0132.txt...\n",
      "[INFO] Procesando clean_0133.txt...\n",
      "[INFO] Procesando clean_0134.txt...\n",
      "[INFO] Procesando clean_0135.txt...\n",
      "[INFO] Procesando clean_0136.txt...\n",
      "[INFO] Procesando clean_0137.txt...\n",
      "[INFO] Procesando clean_0138.txt...\n",
      "[INFO] Procesando clean_0139.txt...\n",
      "[INFO] Procesando clean_0140.txt...\n",
      "[INFO] Procesando clean_0141.txt...\n",
      "[INFO] Procesando clean_0142.txt...\n",
      "[INFO] Procesando clean_0143.txt...\n",
      "[INFO] transitorios NO encontrado → 0143.txt\n",
      "[INFO] Procesando clean_0144.txt...\n",
      "[INFO] Procesando clean_0145.txt...\n",
      "[INFO] transitorios NO encontrado → 0145.txt\n",
      "[INFO] Procesando clean_0146.txt...\n",
      "[INFO] Procesando clean_0147.txt...\n",
      "[INFO] Procesando clean_0148.txt...\n",
      "[INFO] Procesando clean_0149.txt...\n",
      "[INFO] Procesando clean_0150.txt...\n",
      "[INFO] transitorios NO encontrado → 0150.txt\n",
      "[INFO] Procesando clean_0151.txt...\n",
      "[INFO] Procesando clean_0152.txt...\n",
      "[INFO] Procesando clean_0153.txt...\n",
      "[INFO] Procesando clean_0154.txt...\n",
      "[INFO] Procesando clean_0155.txt...\n",
      "[INFO] Procesando clean_0156.txt...\n",
      "[INFO] Procesando clean_0157.txt...\n",
      "[INFO] transitorios NO encontrado → 0157.txt\n",
      "[INFO] Procesando clean_0158.txt...\n",
      "[INFO] Procesando clean_0159.txt...\n",
      "[INFO] Procesando clean_0160.txt...\n",
      "[INFO] Procesando clean_0161.txt...\n",
      "[INFO] Procesando clean_0162.txt...\n",
      "[INFO] Procesando clean_0163.txt...\n",
      "[INFO] transitorios NO encontrado → 0163.txt\n",
      "[INFO] Procesando clean_0164.txt...\n",
      "[INFO] Procesando clean_0165.txt...\n",
      "[INFO] transitorios NO encontrado → 0165.txt\n",
      "[INFO] Procesando clean_0166.txt...\n",
      "[INFO] Procesando clean_0167.txt...\n",
      "[INFO] Procesando clean_0168.txt...\n",
      "[INFO] Procesando clean_0169.txt...\n",
      "[INFO] Procesando clean_0170.txt...\n",
      "[INFO] Procesando clean_0171.txt...\n",
      "[INFO] Procesando clean_0172.txt...\n",
      "[INFO] transitorios NO encontrado → 0172.txt\n",
      "[INFO] Procesando clean_0173.txt...\n",
      "[INFO] Procesando clean_0174.txt...\n",
      "[INFO] Procesando clean_0175.txt...\n",
      "[INFO] Procesando clean_0176.txt...\n",
      "[INFO] Procesando clean_0177.txt...\n",
      "[INFO] Procesando clean_0178.txt...\n",
      "[INFO] Procesando clean_0179.txt...\n",
      "[INFO] Procesando clean_0180.txt...\n",
      "[INFO] Procesando clean_0181.txt...\n",
      "[INFO] Procesando clean_0182.txt...\n",
      "[INFO] Procesando clean_0183.txt...\n",
      "[INFO] Procesando clean_0184.txt...\n",
      "[INFO] Procesando clean_0185.txt...\n",
      "[INFO] Procesando clean_0186.txt...\n",
      "[INFO] Procesando clean_0187.txt...\n",
      "[INFO] Procesando clean_0188.txt...\n",
      "[INFO] Procesando clean_0189.txt...\n",
      "[INFO] Procesando clean_0190.txt...\n",
      "[INFO] Procesando clean_0191.txt...\n",
      "[INFO] Procesando clean_0192.txt...\n",
      "[INFO] Procesando clean_0193.txt...\n",
      "[INFO] Procesando clean_0194.txt...\n",
      "[INFO] Procesando clean_0195.txt...\n",
      "[INFO] Procesando clean_0196.txt...\n",
      "[INFO] Procesando clean_0197.txt...\n",
      "[INFO] Procesando clean_0198.txt...\n",
      "[INFO] Procesando clean_0199.txt...\n",
      "[INFO] Procesando clean_0200.txt...\n",
      "[INFO] Procesando clean_0201.txt...\n",
      "[INFO] decreto NO encontrado → 0201.txt\n",
      "[INFO] Procesando clean_0202.txt...\n",
      "[INFO] decreto NO encontrado → 0202.txt\n",
      "[INFO] Procesando clean_0203.txt...\n",
      "[INFO] Procesando clean_0204.txt...\n",
      "[INFO] Procesando clean_0205.txt...\n",
      "[INFO] Procesando clean_0206.txt...\n",
      "[INFO] Procesando clean_0207.txt...\n",
      "[INFO] Procesando clean_0208.txt...\n",
      "[INFO] Procesando clean_0209.txt...\n",
      "[INFO] Procesando clean_0210.txt...\n",
      "[INFO] Procesando clean_0211.txt...\n",
      "[INFO] Procesando clean_0212.txt...\n",
      "[INFO] Procesando clean_0213.txt...\n",
      "[INFO] Procesando clean_0214.txt...\n",
      "[INFO] Procesando clean_0215.txt...\n",
      "[INFO] Procesando clean_0216.txt...\n",
      "[INFO] Procesando clean_0217.txt...\n",
      "[INFO] Procesando clean_0218.txt...\n",
      "[INFO] Procesando clean_0219.txt...\n",
      "[INFO] Procesando clean_0220.txt...\n",
      "[INFO] Procesando clean_0221.txt...\n",
      "[INFO] Procesando clean_0222.txt...\n",
      "[INFO] Procesando clean_0223.txt...\n",
      "[INFO] Procesando clean_0224.txt...\n",
      "[INFO] Procesando clean_0225.txt...\n",
      "[INFO] Procesando clean_0226.txt...\n",
      "[INFO] Procesando clean_0227.txt...\n",
      "[INFO] Procesando clean_0228.txt...\n",
      "[INFO] Procesando clean_0229.txt...\n",
      "[INFO] Procesando clean_0230.txt...\n",
      "[INFO] Procesando clean_0231.txt...\n",
      "[INFO] Procesando clean_0232.txt...\n",
      "[INFO] Procesando clean_0233.txt...\n",
      "[INFO] Procesando clean_0234.txt...\n",
      "[INFO] Procesando clean_0235.txt...\n",
      "[INFO] Procesando clean_0236.txt...\n",
      "[INFO] Procesando clean_0237.txt...\n",
      "[INFO] Procesando clean_0238.txt...\n",
      "[INFO] Procesando clean_0239.txt...\n",
      "[INFO] Procesando clean_0240.txt...\n",
      "[INFO] Procesando clean_0241.txt...\n",
      "[INFO] Procesando clean_0242.txt...\n",
      "[INFO] Procesando clean_0243.txt...\n",
      "[INFO] Procesando clean_0244.txt...\n",
      "[INFO] Procesando clean_0245.txt...\n",
      "[INFO] Procesando clean_0246.txt...\n",
      "[INFO] Procesando clean_0247.txt...\n",
      "[INFO] Procesando clean_0248.txt...\n",
      "[INFO] Procesando clean_0249.txt...\n",
      "[INFO] decreto NO encontrado → 0249.txt\n",
      "[INFO] Procesando clean_0250.txt...\n",
      "[INFO] Procesando clean_0251.txt...\n",
      "[INFO] Procesando clean_0252.txt...\n",
      "[INFO] Procesando clean_0253.txt...\n",
      "[INFO] Procesando clean_0254.txt...\n",
      "[INFO] Procesando clean_0255.txt...\n",
      "[INFO] Procesando clean_0256.txt...\n",
      "[INFO] Procesando clean_0257.txt...\n",
      "[INFO] Procesando clean_0258.txt...\n",
      "[INFO] Procesando clean_0259.txt...\n",
      "[INFO] Procesando clean_0260.txt...\n",
      "[INFO] Procesando clean_0261.txt...\n",
      "[INFO] Procesando clean_0262.txt...\n",
      "[INFO] Procesando clean_0263.txt...\n",
      "[INFO] Procesando clean_0264.txt...\n",
      "[INFO] Procesando clean_0265.txt...\n",
      "[INFO] Procesando clean_0266.txt...\n",
      "[INFO] Procesando clean_0267.txt...\n",
      "[INFO] Procesando clean_0268.txt...\n",
      "[INFO] Procesando clean_0269.txt...\n",
      "[INFO] Procesando clean_0270.txt...\n",
      "[INFO] Procesando clean_0271.txt...\n",
      "[INFO] Procesando clean_0272.txt...\n",
      "[INFO] Procesando clean_0273.txt...\n",
      "[INFO] transitorios NO encontrado → 0273.txt\n",
      "[INFO] Procesando clean_0274.txt...\n",
      "[INFO] Procesando clean_0275.txt...\n",
      "[INFO] Procesando clean_0276.txt...\n",
      "[INFO] Procesando clean_0277.txt...\n",
      "[INFO] Procesando clean_0278.txt...\n",
      "[INFO] Procesando clean_0279.txt...\n",
      "[INFO] Procesando clean_0280.txt...\n",
      "[INFO] Procesando clean_0281.txt...\n",
      "[INFO] Procesando clean_0282.txt...\n",
      "[INFO] Procesando clean_0283.txt...\n",
      "[INFO] Procesando clean_0284.txt...\n",
      "[INFO] Procesando clean_0285.txt...\n",
      "[INFO] decreto NO encontrado → 0285.txt\n",
      "[INFO] Procesando clean_0286.txt...\n",
      "[INFO] Procesando clean_0287.txt...\n",
      "[INFO] Procesando clean_0288.txt...\n",
      "[INFO] Procesando clean_0289.txt...\n",
      "[INFO] decreto NO encontrado → 0289.txt\n",
      "[INFO] Procesando clean_0290.txt...\n",
      "[INFO] Procesando clean_0291.txt...\n",
      "[INFO] Procesando clean_0292.txt...\n",
      "[INFO] Procesando clean_0293.txt...\n",
      "[INFO] Procesando clean_0294.txt...\n",
      "[INFO] Procesando clean_0295.txt...\n",
      "[INFO] transitorios NO encontrado → 0295.txt\n",
      "[INFO] Procesando clean_0296.txt...\n",
      "[INFO] Procesando clean_0297.txt...\n",
      "[INFO] Procesando clean_0298.txt...\n",
      "[INFO] Procesando clean_0299.txt...\n",
      "[INFO] decreto NO encontrado → 0299.txt\n",
      "[INFO] transitorios NO encontrado → 0299.txt\n",
      "[INFO] Procesando clean_0300.txt...\n",
      "[INFO] Procesando clean_0301.txt...\n",
      "=== Step 3 | Fin: OK=301 | Fails=0 | Total=301 | Tiempo=19.41s ===\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3 — Orchestral: iterate over clean_*.txt and call atomic (con banderas) ===\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "CATALOG_CSV = Path(\"Raw/index.csv\")\n",
    "CLEAN_DIR   = Path(\"temp/clean\")\n",
    "ONLY_ONE    = None   # Ej. \"0008\" para solo ese; None = procesa todos\n",
    "\n",
    "# Reusa load_catalog de Step 1/2 (retorna Dict[str, LawMeta])\n",
    "catalog = load_catalog(CATALOG_CSV)\n",
    "\n",
    "def step3_split_all(only_one: str | None = ONLY_ONE) -> dict:\n",
    "    \"\"\"\n",
    "    Orquesta Step 3:\n",
    "      - si only_one: procesa solo clean_{only_one}.txt\n",
    "      - si None: procesa todos los clean_*.txt presentes\n",
    "      - retorna resumen con totales y listas de procesados/fallidos\n",
    "    \"\"\"\n",
    "    t0 = perf_counter()\n",
    "    processed, failed = [], []\n",
    "\n",
    "    print(\"=== Step 3 | Inicio: Split decreto / ley / transitorios ===\")\n",
    "    if only_one:\n",
    "        clean_path = CLEAN_DIR / f\"clean_{only_one}.txt\"\n",
    "        if not clean_path.exists():\n",
    "            print(f\"[WARNING] No existe {clean_path.name}; nada que procesar.\")\n",
    "            return {\"total\": 0, \"processed\": [], \"failed\": [only_one]}\n",
    "        print(f\"[INFO] Modo 'uno': procesando {clean_path.name}\")\n",
    "        res = process_one_file(only_one, catalog)\n",
    "        (processed if res.get(\"ok\") else failed).append(only_one)\n",
    "    else:\n",
    "        files = sorted(CLEAN_DIR.glob(\"clean_*.txt\"))\n",
    "        print(f\"[INFO] Archivos detectados en {CLEAN_DIR}: {len(files)}\")\n",
    "        if not files:\n",
    "            print(\"[WARNING] No hay archivos clean_*.txt para procesar.\")\n",
    "        for fp in files:\n",
    "            fn = fp.stem.replace(\"clean_\", \"\")\n",
    "            res = process_one_file(fn, catalog)\n",
    "            (processed if res.get(\"ok\") else failed).append(fn)\n",
    "\n",
    "    dt = perf_counter() - t0\n",
    "    summary = {\n",
    "        \"total\": len(processed) + len(failed),\n",
    "        \"processed\": processed,\n",
    "        \"failed\": failed,\n",
    "        \"seconds\": round(dt, 2),\n",
    "    }\n",
    "    print(f\"=== Step 3 | Fin: OK={len(processed)} | Fails={len(failed)} | \"\n",
    "          f\"Total={summary['total']} | Tiempo={summary['seconds']}s ===\")\n",
    "    if failed:\n",
    "        print(\"[WARNING] Archivos con fallo:\", \", \".join(failed))\n",
    "    return summary\n",
    "\n",
    "# === Ejecutar (elige: uno o todos) ===\n",
    "summary_step3 = step3_split_all(ONLY_ONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894252cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 listo: decreto / ley / transitorios escritos en Refined/ y manifiesto actualizado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # === STEP 3 — Split decreto / ley / transitorios ============================\n",
    "# # Pega esta celda justo después de la celda 54\n",
    "\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "# import csv, re, unicodedata\n",
    "\n",
    "# # ---- Config ----\n",
    "# CLEAN_DIR   = Path(\"temp/clean\")         # archivos: clean_{file_num}.txt\n",
    "# CATALOG_CSV = Path(\"Raw/index.csv\")      # columnas requeridas: file_num, law_name\n",
    "# OUT_BASE    = Path(\"Refined\")\n",
    "# ONLY_ONE    = None                       # ej. \"0008\" para solo ese; None = procesa todos\n",
    "\n",
    "# # ---- Fallbacks si no existen en celdas previas ----\n",
    "# def _exists(name:str)->bool:\n",
    "#     return name in globals()\n",
    "\n",
    "# if not _exists(\"write_text\"):\n",
    "#     def write_text(path: Path, content: str):\n",
    "#         path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "# if not _exists(\"write_error\"):\n",
    "#     def write_error(base: str, kind: str, message: str, extra=None):\n",
    "#         err_dir = OUT_BASE / \"errores\"\n",
    "#         err_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         p = err_dir / f\"{base}_errors.txt\"\n",
    "#         with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"[{kind}] {message}\")\n",
    "#             if extra:\n",
    "#                 f.write(f\" | {extra}\")\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "# if not _exists(\"count_stats\"):\n",
    "#     def count_stats(text: str):\n",
    "#         lines = text.count(\"\\n\") + (1 if text else 0)\n",
    "#         words = len(text.split())\n",
    "#         chars = len(text)\n",
    "#         return lines, words, chars\n",
    "\n",
    "# # ---- Helpers locales ----\n",
    "# def _strip_accents(s: str) -> str:\n",
    "#     return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# def _first_two_caps(law_name: str | None) -> str | None:\n",
    "#     if not law_name:\n",
    "#         return None\n",
    "#     toks = [t for t in re.split(r\"\\W+\", law_name) if t]\n",
    "#     if not toks:\n",
    "#         return None\n",
    "#     return _strip_accents(\" \".join(toks[:2])).upper()\n",
    "\n",
    "# def _load_catalog(csv_path: Path) -> dict[str, tuple[str|None, str|None]]:\n",
    "#     \"\"\"\n",
    "#     Devuelve: { file_num: (law_name, first_two_caps) }\n",
    "#     \"\"\"\n",
    "#     mapping: dict[str, tuple[str|None, str|None]] = {}\n",
    "#     with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         required = {\"file_num\",\"law_name\"}\n",
    "#         if not required.issubset(set(reader.fieldnames or [])):\n",
    "#             raise ValueError(\"Raw/index.csv debe tener columnas: file_num, law_name\")\n",
    "#         for row in reader:\n",
    "#             fn = (row.get(\"file_num\") or \"\").strip()\n",
    "#             ln = (row.get(\"law_name\") or \"\").strip() or None\n",
    "#             if fn:\n",
    "#                 mapping[fn] = (ln, _first_two_caps(ln))\n",
    "#     return mapping\n",
    "\n",
    "# def _ensure_dirs():\n",
    "#     (OUT_BASE / \"decretos\").mkdir(parents=True, exist_ok=True)\n",
    "#     (OUT_BASE / \"leyes\").mkdir(parents=True, exist_ok=True)\n",
    "#     (OUT_BASE / \"transitorios\").mkdir(parents=True, exist_ok=True)\n",
    "#     (OUT_BASE / \"errores\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# def _append_manifest(rows: list[dict]):\n",
    "#     manifest = OUT_BASE / \"manifest_parts.csv\"\n",
    "#     exists = manifest.exists()\n",
    "#     cols = [\"file_num\",\"part\",\"lines\",\"words\",\"chars\",\"path\",\"created_at\"]\n",
    "#     with manifest.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         w = csv.DictWriter(f, fieldnames=cols)\n",
    "#         if not exists:\n",
    "#             w.writeheader()\n",
    "#         for r in rows:\n",
    "#             w.writerow(r)\n",
    "\n",
    "# # ---- Núcleo de proceso por archivo ----\n",
    "# def process_one(file_num: str, clean_path: Path, first_two_caps: str | None):\n",
    "#     txt = clean_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "#     # Usa TU helper ya definido en celdas anteriores:\n",
    "#     # split_blocks_two_word_strict(cleaned_text, first_two_caps, base)\n",
    "#     parts = split_blocks_two_word_strict(txt, first_two_caps or \"\", base=file_num)\n",
    "\n",
    "#     out_rows = []\n",
    "#     now = datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "#     def drop(part_key: str, folder: str):\n",
    "#         content = (parts.get(part_key) or \"\").strip()\n",
    "#         if not content:\n",
    "#             return\n",
    "#         outp = OUT_BASE / folder / f\"{file_num}.txt\"\n",
    "#         write_text(outp, content)\n",
    "#         ln, wd, ch = count_stats(content)\n",
    "#         out_rows.append({\n",
    "#             \"file_num\": file_num,\n",
    "#             \"part\": part_key,\n",
    "#             \"lines\": ln,\n",
    "#             \"words\": wd,\n",
    "#             \"chars\": ch,\n",
    "#             \"path\": str(outp),\n",
    "#             \"created_at\": now\n",
    "#         })\n",
    "\n",
    "#     drop(\"decreto\", \"decretos\")\n",
    "#     drop(\"ley\", \"leyes\")\n",
    "#     drop(\"transitorios\", \"transitorios\")\n",
    "\n",
    "#     if out_rows:\n",
    "#         _append_manifest(out_rows)\n",
    "\n",
    "#     # Si no se logró separar \"ley\", deja una nota para auditoría\n",
    "#     if not parts.get(\"ley\"):\n",
    "#         write_error(file_num, \"ley_missing\",\n",
    "#                     \"Ley vacía tras split; revisar heurística o limpieza\",\n",
    "#                     {\"first_two_caps\": first_two_caps})\n",
    "\n",
    "# # ---- Run ----\n",
    "# _ensure_dirs()\n",
    "# catalog = _load_catalog(CATALOG_CSV)\n",
    "\n",
    "# if ONLY_ONE:\n",
    "#     fp = CLEAN_DIR / f\"clean_{ONLY_ONE}.txt\"\n",
    "#     if not fp.exists():\n",
    "#         print(f\"[WARN] No existe {fp}\")\n",
    "#     else:\n",
    "#         _, ftc = catalog.get(ONLY_ONE, (None, None))\n",
    "#         process_one(ONLY_ONE, fp, ftc)\n",
    "# else:\n",
    "#     for fp in sorted(CLEAN_DIR.glob(\"clean_*.txt\")):\n",
    "#         fn = fp.stem.replace(\"clean_\",\"\")\n",
    "#         _, ftc = catalog.get(fn, (None, None))\n",
    "#         process_one(fn, fp, ftc)\n",
    "\n",
    "# print(\"Step 3 listo: decreto / ley / transitorios escritos en Refined/ y manifiesto actualizado.\")\n",
    "\n",
    "# #! REVISAR POR QUÉ NO FUNCIONAN LO DEL manifest_parts: lines, words, chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b80da",
   "metadata": {},
   "source": [
    "## **Step 4: Hierarchical JSON Structure Generation**\n",
    "\n",
    "### **Purpose**\n",
    "Transform cleaned law text into structured JSON format that preserves the hierarchical organization of legal documents. This enables programmatic analysis, search, and processing of legal content.\n",
    "\n",
    "### **Structural Parsing**\n",
    "- **Hierarchy Detection** - Identifies libros, títulos, capítulos, secciones, artículos\n",
    "- **Content Organization** - Builds nested tree structure reflecting legal document hierarchy  \n",
    "- **Article Analysis** - Parses individual articles with content and annotations\n",
    "- **Metadata Integration** - Includes source information and validation metadata\n",
    "\n",
    "### **Advanced Features**\n",
    "- **Automatic Repair** - Detects and fixes embedded article headers within content\n",
    "- **Sequence Validation** - Identifies gaps or jumps in article numbering\n",
    "- **Error Reporting** - Comprehensive logging of parsing issues and structural problems\n",
    "- **Quality Metrics** - Statistical analysis of parsed content for validation\n",
    "\n",
    "### **Output Format**\n",
    "Structured JSON with nested hierarchy, article content, annotations, and comprehensive metadata for downstream processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd3e0c",
   "metadata": {},
   "source": [
    "### **Article Sequence Validation**\n",
    "\n",
    "The parser includes intelligent detection of article numbering gaps (e.g., jumping from \"Artículo 12\" to \"Artículo 15\" without 13-14). This helps identify:\n",
    "\n",
    "- **Missing Content** - Articles that may have been lost during PDF extraction\n",
    "- **Structural Issues** - Formatting problems that affect article detection  \n",
    "- **Document Quality** - Overall completeness of the legal document\n",
    "\n",
    "The system automatically attempts repairs for embedded article headers and provides detailed reporting of any remaining gaps for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "307efba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, Set\n",
    "\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824d4f4",
   "metadata": {},
   "source": [
    "### **Configuration: Allowed Suffixes**\n",
    "\n",
    "The `ALLOWED_SUFFIX` dictionary contains valid suffixes for each hierarchical level, derived from exploratory analysis of the legal corpus. \n",
    "\n",
    "**Future Enhancement**: This configuration should be generated dynamically for each legislature to account for:\n",
    "- Regional variations in legal terminology\n",
    "- Historical changes in numbering conventions  \n",
    "- Document-specific structural patterns\n",
    "\n",
    "**Current Implementation**: Hard-coded based on analysis of existing documents. See exploration notebooks for suffix derivation methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c54d8",
   "metadata": {},
   "source": [
    "### **Enhancement Opportunity: Typo-Resistant Suffix Matching**\n",
    "\n",
    "**Current Limitation**: Exact string matching for legal suffixes may miss valid entries due to:\n",
    "- OCR scanning errors in PDF extraction\n",
    "- Typographical variations in source documents\n",
    "- Accent mark inconsistencies\n",
    "\n",
    "**Proposed Improvements**:\n",
    "1. **Fuzzy String Matching** - Use edit distance algorithms for approximate matching\n",
    "2. **Phonetic Matching** - Handle accent mark variations and similar sounds\n",
    "3. **Machine Learning** - Train classifiers on known good/bad suffix patterns\n",
    "4. **Manual Review Interface** - Flag uncertain matches for human validation\n",
    "\n",
    "This would significantly improve parsing accuracy for lower-quality source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7af8a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Allowed suffixes and header patterns (unchanged lists provided by you)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "ALLOWED_SUFFIX = {\n",
    "    \"libro\": [\"cuarto\",\"decimo\",\"noveno\",\"octavo\",\"primero\",\"quinto\",\"segundo\",\"septimo\",\"sexto\",\"tercero\"],\n",
    "    \"titulo\": [\"catorce\",\"cuarto\",\"decimo\",\"decimo bis\",\"decimoctavo\",\"decimocuarto\",\"decimonoveno\",\"decimoprimero\",\"decimoquinto\",\"decimosegundo\",\"decimoseptimo\",\"decimosexto\",\"decimotercero\",\"dieciseis\",\"doce\",\"duodecimo\",\"i\",\"ii\",\"iii\",\"iv\",\"ix\",\"noveno\",\"octavo\",\"octavo bis\",\"once\",\"preliminar\",\"primero\",\"quince\",\"quinto\",\"quinto bis\",\"segundo\",\"segundo bis\",\"septimo\",\"septimo bis\",\"sexto\",\"tercero\",\"tercero bis\",\"trece\",\"trece bis\",\"undecimo\",\"unico\",\"v\",\"vi\",\"vigesimo\",\"vigesimocuarto\",\"vigesimoprimero\",\"vigesimosegundo\",\"vigesimotercero\",\"vii\",\"viii\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xv\",\"xvi\",\"especial\"],\n",
    "    \"capitulo\": [\"cuarto\",\"cuarto bis\",\"decimo\",\"duodecimo\",\"especial\",\"i\",\"i bis\",\"ii\",\"ii bis\",\"iii\",\"iii bis\",\"iii ter\",\"iv\",\"iv bis\",\"iv ter\",\"ix\",\"ix bis\",\"ix ter\",\"noveno\",\"octavo\",\"primero\",\"quinto\",\"segundo\",\"septimo\",\"sexto\",\"tercero\",\"undecimo\",\"unico\",\"v\",\"v bis\",\"v ter\",\"vi\",\"vi bis\",\"vigesimo\",\"vii\",\"vii bis\",\"viii\",\"viii bis\",\"x\",\"x bis\",\"xi\",\"xii\",\"xii bis\",\"xiii\",\"xiii bis\",\"xiv\",\"xix\",\"xv\",\"xv bis\",\"xv quater\",\"xv ter\",\"xvi\",\"xvi bis\",\"xvii\",\"xviii\",\"xx\",\"xxi\",\"xxii\",\"xxiii\",\"xxiv\",\"xxix\",\"xxv\",\"xxvi\",\"xxvii\",\"xxviii\", \"decimocuarto\",\"decimoquinto\",\"decimosexto\",\"decimoseptimo\",\"decimooctavo\",\"decimonoveno\",\"decimoprimero\",\"decimosegundo\",\"decimotercero\",\"vigesima\"],\n",
    "    \"seccion\": [\"1a\",\"2a\",\"3a\",\"4a\",\"5a\",\"6a\",\"7a\",\"8a\",\"a\",\"b\",\"cuarta\",\"decima\",\"decima bis\",\"decimo\",\"i\",\"ii\",\"iii\",\"iv\",\"ix\",\"novena\",\"octava\",\"primera\",\"primera bis\",\"quinta\",\"segunda\",\"segunda bis\",\"septima\",\"sexta\",\"tercera\",\"unica\",\"v\",\"vi\",\"vii\",\"vii bis\",\"viii\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xix\",\"xv\",\"xvi\",\"xvii\",\"xviii\",\"xx\",\"xxi\",\"xxii\",\"uno\",\"dos\",\"tres\",\"cuatro\",\"cinco\",\"seis\",\"siete\",\"ocho\",\"nueve\",\"diez\",\"once\",\"decimoprimera\",\"decimosegunda\",\"decimotercera\",\"decimocuarta\",\"decimoquinta\",\"decimosexta\",\"decimoseptima\",\"decimoctava\",\"decimonovena\",\"vigesima\"],\n",
    "}\n",
    "\n",
    "ROMAN_RE = re.compile(r'^(?i:xxiv|xxiii|xxii|xxi|xx|xix|xviii|xvii|xvi|xv|xiv|xiii|xii|xi|x|ix|viii|vii|vi|v|iv|iii|ii|i)$')\n",
    "\n",
    "HDR_WORDS = {\n",
    "    \"libro\":    re.compile(r'^\\s*(LIBRO)\\b', re.IGNORECASE),\n",
    "    \"titulo\":   re.compile(r'^\\s*(T[ÍI]TULO)\\b', re.IGNORECASE),\n",
    "    \"capitulo\": re.compile(r'^\\s*(CAP[ÍI]TULO)\\b', re.IGNORECASE),\n",
    "    \"seccion\":  re.compile(r'^\\s*(SECCI[ÓO]N)\\b', re.IGNORECASE),\n",
    "}\n",
    "\n",
    "LEVEL = {\"libro\": 1, \"preliminar\": 1, \"titulo\": 2, \"capitulo\": 3, \"seccion\": 4, \"articulo\": 5}\n",
    "\n",
    "# Inline \"Artículo\" header finder used for repair (accepts \"Artículo\" or \"Art.\")\n",
    "ARTICULO_INLINE_RE = re.compile(\n",
    "    r'(?i)(?<!\\w)(?:art[íi]culo|art\\.)\\s*'\n",
    "    r'(?P<sufijo>'               # full suffix capture as text \"7\", \"7 bis\", \"7-A\"\n",
    "    r'\\d+(?:\\s*(?:bis|ter|quater|quinquies|sexies|septies|octies|nonies|decies|undecies|duodecies|terdecies|[A-Za-z\\-]+)?)?'\n",
    "    r')\\s*\\.(?:-)?\\s*'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def collapse_ws(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return collapse_ws(unidecode(s).lower())\n",
    "\n",
    "def edit_distance(a: str, b: str) -> int:\n",
    "    la, lb = len(a), len(b)\n",
    "    dp = list(range(lb+1))\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            cur = dp[j]\n",
    "            cost = 0 if ca == cb else 1\n",
    "            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)\n",
    "            prev = cur\n",
    "    return dp[lb]\n",
    "\n",
    "def is_articulo_token(token: str) -> bool:\n",
    "    return edit_distance(norm(token), \"articulo\") <= 2 or norm(token) in {\"art\", \"art.\"}\n",
    "\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return [t for t in re.split(r'\\s+', s.strip()) if t]\n",
    "\n",
    "def allowed_suffix_for(tipo: str, candidate_tokens: List[str]) -> Tuple[Optional[str], int]:\n",
    "    if not candidate_tokens:\n",
    "        return None, 0\n",
    "    raw = candidate_tokens[:3]\n",
    "    cleaned = [re.sub(r'[.\\-:—]+$', '', tok).strip() for tok in raw]\n",
    "\n",
    "    allowed = set(ALLOWED_SUFFIX.get(tipo, []))\n",
    "    allowed_norm = {norm(x) for x in allowed} | {norm(x.replace(' ', '')) for x in allowed}\n",
    "\n",
    "    max_span = min(3, len(cleaned))\n",
    "    for k in range(max_span, 0, -1):\n",
    "        span_clean = cleaned[:k]\n",
    "        as_is_clean = collapse_ws(' '.join(span_clean))\n",
    "        as_norm = norm(as_is_clean)\n",
    "        as_join_norm = norm(''.join(span_clean))\n",
    "\n",
    "        if tipo == \"seccion\":\n",
    "            if re.fullmatch(r'\\d+[aª]$', as_norm) or re.fullmatch(r'\\d+$', as_norm):\n",
    "                return as_is_clean, k\n",
    "\n",
    "        if tipo in (\"libro\", \"titulo\", \"capitulo\", \"seccion\"):\n",
    "            if ROMAN_RE.match(as_norm) or re.fullmatch(r'\\d+$', as_norm):\n",
    "                return as_is_clean, k\n",
    "\n",
    "        if as_norm in allowed_norm or as_join_norm in allowed_norm:\n",
    "            return as_is_clean, k\n",
    "    return None, 0\n",
    "\n",
    "def split_header_rest(line: str, header_word_span: Tuple[int,int]) -> str:\n",
    "    return line[header_word_span[1]:].strip()\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    cleaned = re.sub(r'[<>:\"/\\\\|?*\\r\\n\\t]+', ' ', name)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    if not cleaned or cleaned in {\".\", \"..\"}:\n",
    "        cleaned = \"ley\"\n",
    "    return cleaned\n",
    "\n",
    "# -------------------------------- Inline notes logic ----------------------------------\n",
    "\n",
    "NOTE_PARENS_RE = re.compile(r'\\(([^()]*)\\)')  # one level\n",
    "\n",
    "def strip_inline_notes(line: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove '(...)' segments ONLY if inside they contain 'no.' (case-insensitive).\n",
    "    Return (cleaned_line, [notes_without_brackets]).\n",
    "    \"\"\"\n",
    "    if not line:\n",
    "        return line, []\n",
    "    notes: List[str] = []\n",
    "    kept_parts: List[str] = []\n",
    "    idx = 0\n",
    "    for m in NOTE_PARENS_RE.finditer(line):\n",
    "        start, end = m.span()\n",
    "        content = m.group(1) or \"\"\n",
    "        if re.search(r'(?i)\\bno\\.', content, flags=re.IGNORECASE):\n",
    "            kept_parts.append(line[idx:start])\n",
    "            notes.append(collapse_ws(content.strip()))\n",
    "            idx = end\n",
    "        else:\n",
    "            kept_parts.append(line[idx:end])\n",
    "            idx = end\n",
    "    kept_parts.append(line[idx:])\n",
    "    cleaned = ''.join(kept_parts).strip()\n",
    "    return cleaned, notes\n",
    "\n",
    "# ----------------------------------- Data model ---------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    tipo: str\n",
    "    sufijo: str\n",
    "    nombre: Optional[str]\n",
    "    nota: List[str] = field(default_factory=list)\n",
    "    contenido: Union[str, List['Node']] = field(default_factory=list)\n",
    "    start: int = 0\n",
    "    end: int = 0\n",
    "    line: int = 0\n",
    "    level: int = 0\n",
    "    header_line_text: Optional[str] = None\n",
    "\n",
    "    def to_json_obj(self) -> Dict[str, Any]:\n",
    "        base = {\"tipo\": self.tipo, \"sufijo\": self.sufijo}\n",
    "        if self.tipo != \"articulo\":\n",
    "            base[\"nombre\"] = self.nombre if self.nombre is not None else None\n",
    "            base[\"nota\"] = self.nota[:] if self.nota else []\n",
    "            base[\"contenido\"] = [c.to_json_obj() for c in (self.contenido or [])]\n",
    "        else:\n",
    "            base[\"nota\"] = self.nota[:] if self.nota else []\n",
    "            base[\"contenido\"] = self.contenido if isinstance(self.contenido, str) else \"\"\n",
    "        return base\n",
    "\n",
    "# -------------------------------- Counting utility ------------------------------------\n",
    "\n",
    "def count_unidades(nodes: List[Node]) -> Dict[str, int]:\n",
    "    counts = {\"libros\":0,\"titulos\":0,\"capitulos\":0,\"secciones\":0,\"articulos\":0}\n",
    "    def _walk(n: Node):\n",
    "        if n.tipo == \"libro\": counts[\"libros\"] += 1\n",
    "        elif n.tipo == \"titulo\": counts[\"titulos\"] += 1\n",
    "        elif n.tipo == \"capitulo\": counts[\"capitulos\"] += 1\n",
    "        elif n.tipo == \"seccion\": counts[\"secciones\"] += 1\n",
    "        elif n.tipo == \"articulo\": counts[\"articulos\"] += 1\n",
    "        if isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _walk(c)\n",
    "    for n in nodes: _walk(n)\n",
    "    return counts\n",
    "\n",
    "# -------------------- Header detection & Artículo parsing -----------------------------\n",
    "\n",
    "def detect_container_header(line: str) -> Optional[Tuple[str, Tuple[int,int]]]:\n",
    "    for tipo, pat in HDR_WORDS.items():\n",
    "        m = pat.match(line)\n",
    "        if m:\n",
    "            return tipo, m.span(1)\n",
    "    return None\n",
    "\n",
    "def parse_container_header(clean_line: str, tipo: str, header_span: Tuple[int,int]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    rest = split_header_rest(clean_line, header_span)\n",
    "    if not rest:\n",
    "        return None, None\n",
    "    tokens = tokenize(rest)\n",
    "    if not tokens:\n",
    "        return None, None\n",
    "    suffix, consumed = allowed_suffix_for(tipo, tokens)\n",
    "    if not suffix:\n",
    "        return None, None\n",
    "    after_suffix = ' '.join(tokens[consumed:]).strip() if consumed < len(tokens) else \"\"\n",
    "    nombre_inline = after_suffix if after_suffix else None\n",
    "    return suffix, nombre_inline\n",
    "\n",
    "def parse_articulo_header_and_body(lines: List[str], i: int, line_starts: List[int], full_text: str) -> Optional[Tuple[Node, int]]:\n",
    "    raw_line = lines[i]\n",
    "    line, header_notes = strip_inline_notes(raw_line)\n",
    "\n",
    "    m = re.match(r'^\\s*(\\S+)', line)\n",
    "    if not m:\n",
    "        return None\n",
    "    first = m.group(1)\n",
    "    if not is_articulo_token(first):\n",
    "        return None\n",
    "\n",
    "    rest = line[m.end():]\n",
    "    if not re.match(r'^\\s*\\d+', rest):\n",
    "        return None\n",
    "\n",
    "    term_idx = None\n",
    "    k = 0\n",
    "    while k < len(rest):\n",
    "        if rest[k] == '.':\n",
    "            k2 = k + 1\n",
    "            if k2 < len(rest) and rest[k2] == '-':\n",
    "                k2 += 1\n",
    "            if k2 >= len(rest) or rest[k2].isspace():\n",
    "                term_idx = k\n",
    "                break\n",
    "        k += 1\n",
    "    if term_idx is None:\n",
    "        return None\n",
    "\n",
    "    candidate_suffix = rest[:term_idx].strip()\n",
    "    if not re.search(r'\\d', candidate_suffix):\n",
    "        return None\n",
    "\n",
    "    jstart = term_idx + 1\n",
    "    if jstart < len(rest) and rest[jstart] == '-':\n",
    "        jstart += 1\n",
    "    while jstart < len(rest) and rest[jstart].isspace():\n",
    "        jstart += 1\n",
    "    after = rest[jstart:]\n",
    "\n",
    "    body_lines: List[str] = []\n",
    "    if after.strip():\n",
    "        body_clean, body_notes = strip_inline_notes(after.rstrip())\n",
    "        body_lines.append(body_clean)\n",
    "        header_notes.extend(body_notes)\n",
    "\n",
    "    j = i + 1\n",
    "    while j < len(lines):\n",
    "        candidate_raw = lines[j]\n",
    "        cand_clean, cand_notes = strip_inline_notes(candidate_raw.rstrip())\n",
    "\n",
    "        if detect_container_header(cand_clean) or (\n",
    "            cand_clean.strip() and is_articulo_token(cand_clean.strip().split(' ', 1)[0])\n",
    "        ):\n",
    "            if cand_clean.strip() and is_articulo_token(cand_clean.strip().split(' ', 1)[0]):\n",
    "                lrest = cand_clean.strip()[len(cand_clean.strip().split(' ', 1)[0]):]\n",
    "                if not re.match(r'^\\s*\\d+', lrest):\n",
    "                    header_notes.extend(cand_notes)\n",
    "                    body_lines.append(cand_clean)\n",
    "                    j += 1\n",
    "                    continue\n",
    "            break\n",
    "        header_notes.extend(cand_notes)\n",
    "        body_lines.append(cand_clean)\n",
    "        j += 1\n",
    "\n",
    "    start_char = line_starts[i]\n",
    "    end_char = line_starts[j] if j < len(lines) else len(full_text)\n",
    "    content_text = \"\\n\".join([ln.rstrip() for ln in body_lines if ln.strip()]).strip()\n",
    "\n",
    "    node = Node(\n",
    "        tipo=\"articulo\",\n",
    "        sufijo=candidate_suffix,\n",
    "        nombre=None,\n",
    "        nota=[n for n in header_notes if n],\n",
    "        contenido=content_text,\n",
    "        start=start_char,\n",
    "        end=end_char,\n",
    "        line=i+1,\n",
    "        level=LEVEL[\"articulo\"],\n",
    "        header_line_text=raw_line.strip()\n",
    "    )\n",
    "    return node, j\n",
    "\n",
    "# ----------------------------- Article repair utilities --------------------------------\n",
    "\n",
    "def parse_article_base_int(sufijo: str) -> Optional[int]:\n",
    "    m = re.search(r'(\\d+)', sufijo)\n",
    "    if m:\n",
    "        try:\n",
    "            return int(m.group(1))\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_article_base_and_variant(sufijo: str) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"\n",
    "    Return (base_int, variant_text_normalized_without_spaces) where variant can be ''.\n",
    "    \"\"\"\n",
    "    m = re.search(r'(\\d+)\\s*(.*)$', sufijo.strip())\n",
    "    if not m:\n",
    "        return None, \"\"\n",
    "    base = parse_article_base_int(sufijo)\n",
    "    tail = (m.group(2) or \"\").strip()\n",
    "    tail_norm = norm(tail).replace(\" \", \"\")\n",
    "    return base, tail_norm  # '' if no variant\n",
    "\n",
    "def find_embedded_article_headers(text: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Return list of (start_idx, end_idx_after_header, sufijo_text) matches for inline headers.\n",
    "    \"\"\"\n",
    "    matches: List[Tuple[int,int,str]] = []\n",
    "    for mm in ARTICULO_INLINE_RE.finditer(text):\n",
    "        start = mm.start()\n",
    "        end = mm.end()\n",
    "        suf = collapse_ws(mm.group(\"sufijo\"))\n",
    "        matches.append((start, end, suf))\n",
    "    return matches\n",
    "\n",
    "def split_embedded_articles_in_list(nodes: List[Node]) -> None:\n",
    "    \"\"\"\n",
    "    Traverse a node list; for each artículo node whose content contains inline 'Artículo ...'\n",
    "    headers, split them into separate article nodes IF AND ONLY IF the first embedded header\n",
    "    matches the expected immediate sequence: base+1 OR same base with a non-empty variant.\n",
    "    We accept a chain of subsequent embedded headers only if they continue +1 steps.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(nodes):\n",
    "        node = nodes[i]\n",
    "        # Recurse into containers first\n",
    "        if node.tipo != \"articulo\" and isinstance(node.contenido, list):\n",
    "            split_embedded_articles_in_list(node.contenido)\n",
    "\n",
    "        if node.tipo == \"articulo\" and isinstance(node.contenido, str) and node.contenido:\n",
    "            text = node.contenido\n",
    "            emb = find_embedded_article_headers(text)\n",
    "            if emb:\n",
    "                base0, var0 = parse_article_base_and_variant(node.sufijo)\n",
    "                if base0 is not None:\n",
    "                    # Evaluate first embedded header\n",
    "                    s0, e0, suf0 = emb[0]\n",
    "                    b1, v1 = parse_article_base_and_variant(suf0)\n",
    "                    ok_first = False\n",
    "                    # Allowed start: base+1 or same base with non-empty variant (and not identical suffix)\n",
    "                    if b1 is not None:\n",
    "                        if b1 == base0 + 1:\n",
    "                            ok_first = True\n",
    "                        elif b1 == base0:\n",
    "                            if v1 and norm(suf0) != norm(node.sufijo):\n",
    "                                ok_first = True\n",
    "                    if ok_first and s0 >= 1:\n",
    "                        # Build a chain of accepted matches: consecutive +1 steps\n",
    "                        accepted = [(s0, e0, suf0, b1)]\n",
    "                        expected_next = b1 + 1\n",
    "                        for k in range(1, len(emb)):\n",
    "                            sk, ek, sufk = emb[k]\n",
    "                            bk, vk = parse_article_base_and_variant(sufk)\n",
    "                            if bk is None:\n",
    "                                break\n",
    "                            if bk == expected_next:\n",
    "                                accepted.append((sk, ek, sufk, bk))\n",
    "                                expected_next += 1\n",
    "                            else:\n",
    "                                # stop at first non-consecutive\n",
    "                                break\n",
    "\n",
    "                        # Perform split\n",
    "                        new_nodes: List[Node] = []\n",
    "                        # part before first embedded header stays in current node\n",
    "                        before = text[:accepted[0][0]].rstrip()\n",
    "                        node.contenido = before\n",
    "\n",
    "                        # For each accepted embedded header, create a new Node with its body\n",
    "                        for idx_acc, (sk, ek, sufk, bk) in enumerate(accepted):\n",
    "                            body_start = ek\n",
    "                            body_end = accepted[idx_acc + 1][0] if idx_acc + 1 < len(accepted) else len(text)\n",
    "                            body = text[body_start:body_end].strip()\n",
    "                            new_nodes.append(Node(\n",
    "                                tipo=\"articulo\",\n",
    "                                sufijo=sufk,\n",
    "                                nombre=None,\n",
    "                                nota=[],\n",
    "                                contenido=body,\n",
    "                                start=0, end=0,\n",
    "                                line=node.line,  # best-effort\n",
    "                                level=LEVEL[\"articulo\"],\n",
    "                                header_line_text=f\"Artículo {sufk}.\"\n",
    "                            ))\n",
    "\n",
    "                        # Insert new nodes right after the original\n",
    "                        nodes[i+1:i+1] = new_nodes\n",
    "                        # Skip past inserted items\n",
    "                        i += len(new_nodes)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------ Main parser -------------------------------------\n",
    "\n",
    "def parse_law_text(text: str, issues: List[Dict[str,Any]]) -> Tuple[str, List[Node], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Parse a full law TXT into a title and list of top-level nodes.\n",
    "    Also returns per-tipo invalid suffixes encountered: {\"libro\":[...],\"titulo\":[...],...}\n",
    "    \"\"\"\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    title = next((ln.strip() for ln in lines if ln.strip()), \"Sin título\")\n",
    "\n",
    "    line_starts = []\n",
    "    pos = 0\n",
    "    for ln in lines:\n",
    "        line_starts.append(pos)\n",
    "        pos += len(ln) + 1\n",
    "\n",
    "    root_nodes: List[Node] = []\n",
    "    stack: List[Node] = []\n",
    "\n",
    "    # Track invalid suffix samples per tipo\n",
    "    invalid_suffixes: Dict[str, Set[str]] = {k: set() for k in [\"libro\",\"titulo\",\"capitulo\",\"seccion\"]}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        raw_line = lines[i]\n",
    "\n",
    "        # Optional PRELIMINAR block at top\n",
    "        if not root_nodes and not stack:\n",
    "            prelim_clean, prelim_notes = strip_inline_notes(raw_line)\n",
    "            if re.match(r'^\\s*disposiciones\\s+preliminares\\b.*$', unidecode(prelim_clean), re.IGNORECASE):\n",
    "                node = Node(\n",
    "                    tipo=\"preliminar\",\n",
    "                    sufijo=\"\",\n",
    "                    nombre=prelim_clean.strip(),\n",
    "                    nota=prelim_notes,\n",
    "                    contenido=[],\n",
    "                    start=line_starts[i],\n",
    "                    end=0,\n",
    "                    line=i+1,\n",
    "                    level=LEVEL[\"preliminar\"],\n",
    "                    header_line_text=raw_line.strip()\n",
    "                )\n",
    "                root_nodes.append(node); stack.append(node)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # 1) Try artículo first\n",
    "        parsed_art = parse_articulo_header_and_body(lines, i, line_starts, text)\n",
    "        if parsed_art:\n",
    "            art_node, j = parsed_art\n",
    "            parent = stack[-1] if stack else None\n",
    "            if parent is None:\n",
    "                root_nodes.append(art_node)\n",
    "            else:\n",
    "                if isinstance(parent.contenido, list):\n",
    "                    parent.contenido.append(art_node)\n",
    "                else:\n",
    "                    parent.contenido = [art_node]\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # 2) Try container header (strip notes first)\n",
    "        clean_line, inline_notes = strip_inline_notes(raw_line)\n",
    "        det = detect_container_header(clean_line)\n",
    "        if det:\n",
    "            tipo, hdr_span = det\n",
    "            suffix, nombre_inline = parse_container_header(clean_line, tipo, hdr_span)\n",
    "\n",
    "            if not suffix:\n",
    "                # Capture candidate \"invalid\" suffix sample for this tipo\n",
    "                rest = split_header_rest(clean_line, hdr_span)\n",
    "                tokens = tokenize(rest)\n",
    "                sample_tokens = [re.sub(r'[.\\-:—,;]+$', '', t).strip() for t in tokens[:3]]\n",
    "                sample = collapse_ws(' '.join([t for t in sample_tokens if t]))\n",
    "                if sample:\n",
    "                    invalid_suffixes.get(tipo, set()).add(sample)\n",
    "\n",
    "                issues.append({\n",
    "                    \"location\": f\"line {i+1}\",\n",
    "                    \"message\": f\"{tipo.title()} sin sufijo válido (línea ignorada)\",\n",
    "                    \"issue_type\": \"warning\",\n",
    "                    \"line_text\": raw_line.strip()\n",
    "                })\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Name-on-next-line rule (strip notes there too)\n",
    "            nombre = nombre_inline\n",
    "            notes_for_node = list(inline_notes)\n",
    "            if nombre is None:\n",
    "                peek = i + 1\n",
    "                while peek < len(lines) and not lines[peek].strip():\n",
    "                    peek += 1\n",
    "                if peek < len(lines):\n",
    "                    next_line_clean, next_line_notes = strip_inline_notes(lines[peek].strip())\n",
    "                    is_header = bool(detect_container_header(next_line_clean))\n",
    "                    is_article_hdr = False\n",
    "                    if next_line_clean.strip():\n",
    "                        first_tok = next_line_clean.strip().split(' ', 1)[0]\n",
    "                        is_article_hdr = is_articulo_token(first_tok) and re.match(\n",
    "                            r'^\\s*\\d+', next_line_clean[len(first_tok):] or \"\")\n",
    "                    if not (is_header or is_article_hdr):\n",
    "                        nombre = next_line_clean if next_line_clean else None\n",
    "                        if next_line_notes:\n",
    "                            notes_for_node.extend(next_line_notes)\n",
    "\n",
    "            node = Node(\n",
    "                tipo=tipo,\n",
    "                sufijo=suffix,\n",
    "                nombre=nombre,\n",
    "                nota=notes_for_node,\n",
    "                contenido=[],\n",
    "                start=line_starts[i],\n",
    "                end=0,\n",
    "                line=i+1,\n",
    "                level=LEVEL[tipo],\n",
    "                header_line_text=raw_line.strip()\n",
    "            )\n",
    "\n",
    "            while stack and stack[-1].level >= node.level:\n",
    "                top = stack.pop()\n",
    "                top.end = line_starts[i]\n",
    "\n",
    "            if not stack:\n",
    "                root_nodes.append(node)\n",
    "            else:\n",
    "                stack[-1].contenido.append(node)\n",
    "            stack.append(node)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 3) Plain line: attach any inline notes to nearest open container\n",
    "        if inline_notes and stack:\n",
    "            for n in inline_notes:\n",
    "                if n and n not in stack[-1].nota:\n",
    "                    stack[-1].nota.append(n)\n",
    "        i += 1\n",
    "\n",
    "    # Close remaining containers at EOF\n",
    "    for n in stack[::-1]:\n",
    "        n.end = len(text)\n",
    "\n",
    "    # ---- Post-parse normalization: split inline embedded artículo headers (auto-repair) ----\n",
    "    split_embedded_articles_in_list(root_nodes)\n",
    "\n",
    "    # Finalize nodes (end positions, note de-dup)\n",
    "    def _finalize(n: Node):\n",
    "        if n.end == 0:\n",
    "            n.end = len(text)\n",
    "        if isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _finalize(c)\n",
    "        seen = set(); dedup = []\n",
    "        for x in n.nota:\n",
    "            if x not in seen:\n",
    "                seen.add(x); dedup.append(x)\n",
    "        n.nota = dedup\n",
    "\n",
    "    for n in root_nodes:\n",
    "        _finalize(n)\n",
    "\n",
    "    # Convert invalid suffix sets to lists\n",
    "    invalid_out = {k: sorted(list(v)) for k, v in invalid_suffixes.items() if v}\n",
    "\n",
    "    return title, root_nodes, invalid_out\n",
    "\n",
    "# ---------------------------- Article sequence validation -----------------------------\n",
    "\n",
    "def validate_article_sequence(nodes: List[Node], file_issues: List[Dict[str,Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Detect forward jumps in base article numbers (> +1).\n",
    "    Returns a list of jump dicts for CSV export and logs a verbose warning per jump.\n",
    "    (Runs AFTER auto-repair, so only genuine jumps remain.)\n",
    "    \"\"\"\n",
    "    arts: List[Node] = []\n",
    "    def _walk(n: Node):\n",
    "        if n.tipo == \"articulo\":\n",
    "            arts.append(n)\n",
    "        elif isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _walk(c)\n",
    "    for n in nodes: _walk(n)\n",
    "\n",
    "    jumps: List[Dict[str,Any]] = []\n",
    "    prev_base = None\n",
    "    prev_node: Optional[Node] = None\n",
    "\n",
    "    for a in arts:\n",
    "        base = parse_article_base_int(a.sufijo)\n",
    "        if base is None:\n",
    "            prev_node = a if prev_node is None else prev_node\n",
    "            continue\n",
    "        if prev_base is None:\n",
    "            prev_base = base\n",
    "            prev_node = a\n",
    "            continue\n",
    "        if base > prev_base + 1:\n",
    "            jump = {\n",
    "                \"prev_line\": prev_node.line if prev_node else \"\",\n",
    "                \"prev_sufijo\": prev_node.sufijo if prev_node else \"\",\n",
    "                \"prev_line_text\": (prev_node.header_line_text or f\"Artículo {prev_node.sufijo}.\") if prev_node else \"\",\n",
    "                \"current_line\": a.line,\n",
    "                \"current_sufijo\": a.sufijo,\n",
    "                \"current_line_text\": a.header_line_text or f\"Artículo {a.sufijo}.\",\n",
    "                \"prev_base\": prev_base,\n",
    "                \"current_base\": base,\n",
    "                \"delta\": base - prev_base\n",
    "            }\n",
    "            jumps.append(jump)\n",
    "            file_issues.append({\n",
    "                \"location\": f\"line {a.line}\",\n",
    "                \"message\": f\"Secuencia de artículos salta de {prev_base} a {base}\",\n",
    "                \"issue_type\": \"warning\",\n",
    "                \"line_text\": jump[\"current_line_text\"]\n",
    "            })\n",
    "        if base >= prev_base:\n",
    "            prev_base = base\n",
    "            prev_node = a\n",
    "\n",
    "    return jumps\n",
    "\n",
    "# --------------------------------- I/O utilities -------------------------------------\n",
    "\n",
    "def law_basename(path: Path) -> str:\n",
    "    return path.stem\n",
    "\n",
    "def inferred_title_from_file(path: Path, parsed_title: str) -> str:\n",
    "    return path.stem\n",
    "\n",
    "def write_json(data: Dict[str,Any], path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def save_errors_to_file(file_issues: List[Dict[str,Any]], file_name: str, errores_dir: Path):\n",
    "    if not file_issues:\n",
    "        return\n",
    "    errores_dir.mkdir(parents=True, exist_ok=True)\n",
    "    error_file_path = errores_dir / f\"{Path(file_name).stem}_errors.txt\"\n",
    "    with error_file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Errores y advertencias para: {file_name}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        for issue in file_issues:\n",
    "            f.write(f\"Ubicación: {issue.get('location', 'N/A')}\\n\")\n",
    "            f.write(f\"Tipo: {issue.get('issue_type', 'warning')}\\n\")\n",
    "            f.write(f\"Mensaje: {issue.get('message', '')}\\n\")\n",
    "            if issue.get(\"line_text\"):\n",
    "                f.write(f\"Línea: {issue['line_text']}\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\")\n",
    "\n",
    "# ------------------------------- Catalogue utilities ----------------------------------\n",
    "\n",
    "def load_catalog(catalog_csv: Path) -> Dict[str, Dict[str,str]]:\n",
    "    mapping: Dict[str, Dict[str,str]] = {}\n",
    "    if catalog_csv and catalog_csv.exists():\n",
    "        with catalog_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                file_num = (row.get(\"file_num\") or \"\").strip()\n",
    "                if not file_num:\n",
    "                    continue\n",
    "                mapping[file_num] = {\n",
    "                    \"law_name\": (row.get(\"law_name\") or \"\").strip(),\n",
    "                    \"link\": (row.get(\"link\") or \"\").strip(),\n",
    "                    \"num_est\": (row.get(\"num_est\") or \"\").strip(),\n",
    "                }\n",
    "    return mapping\n",
    "\n",
    "# ---------------------------------- Main pipeline -------------------------------------\n",
    "\n",
    "def walk_and_process(\n",
    "    ley_dir: Path,\n",
    "    out_dir: Path,\n",
    "    errores_dir: Optional[Path] = None,\n",
    "    catalog_csv: Optional[Path] = None\n",
    ") -> Dict[str, Any]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if errores_dir:\n",
    "        errores_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    catalog = load_catalog(catalog_csv) if catalog_csv else {}\n",
    "\n",
    "    issues_rows = []\n",
    "    jump_rows = []\n",
    "\n",
    "    manifest = {\n",
    "        \"processed_files\": [],\n",
    "        \"files\": {},\n",
    "        \"totals\": {\"libros\":0,\"titulos\":0,\"capitulos\":0,\"secciones\":0,\"articulos\":0},\n",
    "        \"warnings\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "\n",
    "    txt_files = sorted([p for p in ley_dir.glob(\"*.txt\") if p.is_file()])\n",
    "    for p in txt_files:\n",
    "        raw = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "        file_issues: List[Dict[str,Any]] = []\n",
    "\n",
    "        title, nodes, invalid_suffixes = parse_law_text(raw, file_issues)\n",
    "\n",
    "        # Validate article sequence AFTER auto-repair; only genuine jumps remain\n",
    "        jumps = validate_article_sequence(nodes, file_issues)\n",
    "        jump_rows.extend([{\n",
    "            \"file\": p.name,\n",
    "            **jr\n",
    "        } for jr in jumps])\n",
    "\n",
    "        # Save verbose errors per file\n",
    "        if errores_dir and file_issues:\n",
    "            save_errors_to_file(file_issues, p.name, errores_dir)\n",
    "\n",
    "        counts = count_unidades(nodes)\n",
    "\n",
    "        # Determine output JSON filename via catalogue (file_num -> law_name)\n",
    "        stem = p.stem  # expected like '0001'\n",
    "        law_name = catalog.get(stem, {}).get(\"law_name\") or inferred_title_from_file(p, title)\n",
    "\n",
    "        # Output JSON name = file_num.json (strict)\n",
    "        out_name = f\"{stem}.json\"\n",
    "        final_obj = {\n",
    "            \"ley\": law_name,\n",
    "            \"contenido\": [n.to_json_obj() for n in nodes]\n",
    "        }\n",
    "        out_json_path = out_dir / out_name\n",
    "        write_json(final_obj, out_json_path)\n",
    "\n",
    "        # Write invalid suffixes JSON for this file (only if there are any)\n",
    "        if invalid_suffixes:\n",
    "            invalid_path_base = errores_dir if errores_dir else out_dir\n",
    "            invalid_path = invalid_path_base / f\"{stem}_invalid_suffixes.json\"\n",
    "            write_json(invalid_suffixes, invalid_path)\n",
    "\n",
    "        # Collect issues to global CSV rows\n",
    "        for it in file_issues:\n",
    "            issues_rows.append({\n",
    "                \"file\": p.name,\n",
    "                \"location\": it.get(\"location\",\"\"),\n",
    "                \"issue_type\": it.get(\"issue_type\",\"warning\"),\n",
    "                \"message\": it.get(\"message\",\"\"),\n",
    "                \"line_text\": it.get(\"line_text\",\"\"),\n",
    "            })\n",
    "\n",
    "        manifest[\"processed_files\"].append(p.name)\n",
    "        manifest[\"files\"][p.name] = {\n",
    "            \"output\": out_json_path.name,\n",
    "            \"law_name\": law_name,\n",
    "            \"counts\": counts,\n",
    "            \"issues\": file_issues,\n",
    "        }\n",
    "        for k in counts:\n",
    "            manifest[\"totals\"][k] += counts[k]\n",
    "        manifest[\"warnings\"] += sum(1 for it in file_issues if it.get(\"issue_type\") == \"warning\")\n",
    "        manifest[\"errors\"]   += sum(1 for it in file_issues if it.get(\"issue_type\") == \"error\")\n",
    "\n",
    "    # Write manifest + CSVs\n",
    "    write_json(manifest, out_dir / \"manifest.json\")\n",
    "\n",
    "    if errores_dir:\n",
    "        # Parsing issues CSV (includes line text)\n",
    "        with (errores_dir / \"parsing_issues.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=[\"file\",\"location\",\"issue_type\",\"message\",\"line_text\"])\n",
    "            w.writeheader(); w.writerows(issues_rows)\n",
    "\n",
    "        # Article jumps CSV\n",
    "        with (errores_dir / \"articulo_jumps.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"file\",\n",
    "                    \"prev_line\",\"prev_sufijo\",\"prev_line_text\",\n",
    "                    \"current_line\",\"current_sufijo\",\"current_line_text\",\n",
    "                    \"prev_base\",\"current_base\",\"delta\"\n",
    "                ]\n",
    "            )\n",
    "            w.writeheader(); w.writerows(jump_rows)\n",
    "\n",
    "    return manifest\n",
    "\n",
    "# ------------------------------ Error summary & summary -------------------------------\n",
    "\n",
    "def create_error_summary(manifest: Dict[str, Any], errores_dir: Path):\n",
    "    if not errores_dir:\n",
    "        return\n",
    "    summary_path = errores_dir / \"error_summary.txt\"\n",
    "    with summary_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"RESUMEN CONSOLIDADO DE ERRORES Y ADVERTENCIAS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Archivos procesados: {len(manifest.get('processed_files', []))}\\n\")\n",
    "        f.write(f\"Total de advertencias: {manifest.get('warnings', 0)}\\n\")\n",
    "        f.write(f\"Total de errores: {manifest.get('errors', 0)}\\n\\n\")\n",
    "        f.write(\"DETALLES POR ARCHIVO:\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for fname, info in manifest.get(\"files\", {}).items():\n",
    "            issues = info.get(\"issues\", [])\n",
    "            if issues:\n",
    "                f.write(f\"\\n📁 {fname}:\\n\")\n",
    "                for issue in issues:\n",
    "                    f.write(f\"  • {issue.get('location', 'N/A')}: \")\n",
    "                    f.write(f\"[{issue.get('issue_type', 'warning').upper()}] \")\n",
    "                    f.write(f\"{issue.get('message', '')}\\n\")\n",
    "                    if issue.get(\"line_text\"):\n",
    "                        f.write(f\"    Línea: {issue['line_text']}\\n\")\n",
    "\n",
    "        f.write(f\"\\n\\nArchivos de error individuales guardados en: {errores_dir}\\n\")\n",
    "\n",
    "def print_summary(manifest: Dict[str, Any]):\n",
    "    print(\"=\"*72)\n",
    "    print(\" TXT → JSON Parsing Summary\")\n",
    "    print(\"=\"*72)\n",
    "    print(f\" Files processed : {len(manifest.get('processed_files', []))}\")\n",
    "    t = manifest.get(\"totals\", {})\n",
    "    print(f\" Libros         : {t.get('libros',0)}\")\n",
    "    print(f\" Títulos        : {t.get('titulos',0)}\")\n",
    "    print(f\" Capítulos      : {t.get('capitulos',0)}\")\n",
    "    print(f\" Secciones      : {t.get('secciones',0)}\")\n",
    "    print(f\" Artículos      : {t.get('articulos',0)}\")\n",
    "    print(f\" Warnings       : {manifest.get('warnings',0)}\")\n",
    "    print(f\" Errors         : {manifest.get('errors',0)}\")\n",
    "    print(\"-\"*72)\n",
    "    for fname, info in manifest.get(\"files\", {}).items():\n",
    "        c = info.get(\"counts\", {})\n",
    "        n_warn = sum(1 for it in info.get(\"issues\",[]) if it.get(\"issue_type\") == \"warning\")\n",
    "        n_err  = sum(1 for it in info.get(\"issues\",[]) if it.get(\"issue_type\") == \"error\")\n",
    "        print(f\" {fname} → {info.get('output')} ({info.get('law_name','')})\")\n",
    "        print(f\"   L:{c.get('libros',0)} T:{c.get('titulos',0)} C:{c.get('capitulos',0)} S:{c.get('secciones',0)} A:{c.get('articulos',0)} | warn:{n_warn} err:{n_err}\")\n",
    "    print(\"=\"*72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f8ea8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Processing: JSON Structure Generation\n",
      "================================================================================\n",
      "================================================================================\n",
      "PROCESSING COMPLETE - Generating Summary Reports\n",
      "================================================================================\n",
      "========================================================================\n",
      " TXT → JSON Parsing Summary\n",
      "========================================================================\n",
      " Files processed : 0\n",
      " Libros         : 0\n",
      " Títulos        : 0\n",
      " Capítulos      : 0\n",
      " Secciones      : 0\n",
      " Artículos      : 0\n",
      " Warnings       : 0\n",
      " Errors         : 0\n",
      "------------------------------------------------------------------------\n",
      "========================================================================\n",
      "\n",
      " **Pipeline Execution Complete!**\n",
      " JSON files: c:\\Users\\braul\\Documents\\OneDrive\\Leyes\\14\\Refined\\json\n",
      " Error reports: c:\\Users\\braul\\Documents\\OneDrive\\Leyes\\14\\Refined\\errores\n",
      " Processing manifest: c:\\Users\\braul\\Documents\\OneDrive\\Leyes\\14\\Refined\\json\\manifest.json\n",
      "\n",
      " **Final Results:**\n",
      "    Total articles parsed: 0\n",
      "    Warnings: 0\n",
      "    Errors: 0\n",
      "    Success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# ============== FINAL PROCESSING: JSON GENERATION & VALIDATION ==============\n",
    "\n",
    "print(\"Starting Final Processing: JSON Structure Generation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute the complete processing pipeline\n",
    "# - Reads law text files from LEY_DIR  \n",
    "# - Parses hierarchical structure (libros, títulos, capítulos, secciones, artículos)\n",
    "# - Validates article sequences and structural integrity\n",
    "# - Generates structured JSON output with comprehensive metadata\n",
    "# - Creates detailed error reports and validation logs\n",
    "\n",
    "manifest = walk_and_process(\n",
    "    ley_dir=LEY_DIR,           # Input: Cleaned law text files\n",
    "    out_dir=JSON_DIR,          # Output: Structured JSON files\n",
    "    errores_dir=ERRORES_DIR,   # Logs: Error reports and validation\n",
    "    catalog_csv=CATALOG_CSV    # Metadata: Law names and references\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING COMPLETE - Generating Summary Reports\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display comprehensive processing summary\n",
    "print_summary(manifest)\n",
    "\n",
    "# Generate consolidated error summary for quality review\n",
    "create_error_summary(manifest, ERRORES_DIR)\n",
    "\n",
    "print(\"\\n **Pipeline Execution Complete!**\")\n",
    "print(f\" JSON files: {JSON_DIR}\")\n",
    "print(f\" Error reports: {ERRORES_DIR}\")\n",
    "print(f\" Processing manifest: {JSON_DIR / 'manifest.json'}\")\n",
    "\n",
    "# Display final statistics\n",
    "total_articles = manifest.get(\"totals\", {}).get(\"articulos\", 0)\n",
    "total_warnings = manifest.get(\"warnings\", 0)\n",
    "total_errors = manifest.get(\"errors\", 0)\n",
    "\n",
    "print(f\"\\n **Final Results:**\")\n",
    "print(f\"    Total articles parsed: {total_articles:,}\")\n",
    "print(f\"    Warnings: {total_warnings}\")\n",
    "print(f\"    Errors: {total_errors}\")\n",
    "print(f\"    Success rate: {((total_articles - total_errors) / max(total_articles, 1) * 100):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
