{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d77e4e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be67bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json, re, statistics\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import statistics\n",
    "import re\n",
    "\n",
    "print(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85be6cf",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88067e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Paths ==============\n",
    "BASE_DIR     = Path.cwd()\n",
    "\n",
    "RAW_DIR      = BASE_DIR / \"Raw\"                 # PDFs live here\n",
    "OUTPUT_DIR   = BASE_DIR / \"Refined\"             # Root outputs\n",
    "TEMP_DIR   = BASE_DIR / \"temp\"             # Root outputs\n",
    "\n",
    "\n",
    "RAW_TXT_DIR  = OUTPUT_DIR / TEMP_DIR / \"raw_txt\"           # Step 1 outputs\n",
    "CLEAN_DIR    = OUTPUT_DIR / TEMP_DIR / \"clean\"             # Step 2 outputs\n",
    "\n",
    "\n",
    "LEY_DIR      = OUTPUT_DIR / \"leyes\"             # Step 3 outputs (ley)\n",
    "DECR_DIR     = OUTPUT_DIR / \"decretos\"          # Step 3 outputs (decreto)\n",
    "TRANS_DIR    = OUTPUT_DIR / \"transitorios\"      # Step 3 outputs (transitorios)\n",
    "\n",
    "JSON_DIR     = OUTPUT_DIR / \"json\"              # Step 4 outputs (json)\n",
    "ERRORES_DIR  = OUTPUT_DIR / \"errores\"           # error logs\n",
    "\n",
    "for d in [OUTPUT_DIR, RAW_TXT_DIR, TEMP_DIR, CLEAN_DIR, LEY_DIR, DECR_DIR, TRANS_DIR, JSON_DIR, ERRORES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CATALOG_CSV = RAW_DIR / \"index.csv\"             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a13c62",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5059ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Utils ==============\n",
    "def slugify(s: str) -> str:\n",
    "    s = unidecode(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"x\"\n",
    "\n",
    "def norm_lower(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", unidecode(s).lower().strip())\n",
    "\n",
    "def caps_line(s: str) -> str:\n",
    "    return unidecode(s).upper().strip()\n",
    "\n",
    "def write_text(path: Path, content: str) -> None:\n",
    "    path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "def write_error(base: str, kind: str, message: str, extra: Dict | None = None) -> None:\n",
    "    rec = {\"file\": base, \"kind\": kind, \"message\": message}\n",
    "    if extra:\n",
    "        rec.update(extra)\n",
    "    (ERRORES_DIR / f\"{slugify(base)}_{slugify(kind)}.json\").write_text(\n",
    "        json.dumps(rec, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def count_stats(text: str) -> Dict[str, int]:\n",
    "    return {\"lines\": text.count(\"\\n\") + (1 if text else 0),\n",
    "            \"words\": len(re.findall(r\"\\S+\", text)),\n",
    "            \"chars\": len(text)}\n",
    "\n",
    "def _norm_caps(s: str) -> str:\n",
    "    t = unidecode(s).upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]+\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fdd51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Catalog ==============\n",
    "@dataclass(frozen=True)\n",
    "class LawMeta:\n",
    "    num_est: str\n",
    "    file_num: str\n",
    "    law_name: str\n",
    "    link: str\n",
    "    first_two_caps: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        toks = [t for t in norm_lower(self.law_name).split() if t]\n",
    "        first_two = \" \".join(toks[:2]) if toks else \"\"\n",
    "        object.__setattr__(self, \"first_two_caps\", first_two.upper())\n",
    "\n",
    "def load_catalog(path: Path) -> Dict[str, LawMeta]:\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    req = {\"num_est\", \"file_num\", \"law_name\", \"link\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV missing columns: {miss}\")\n",
    "    out: Dict[str, LawMeta] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        meta = LawMeta(\n",
    "            num_est=(r[\"num_est\"] or \"\").strip(),\n",
    "            file_num=(r[\"file_num\"] or \"\").strip().zfill(4),\n",
    "            law_name=(r[\"law_name\"] or \"\").strip(),\n",
    "            link=(r[\"link\"] or \"\").strip(),\n",
    "        )\n",
    "        out[meta.file_num] = meta\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9794bc0",
   "metadata": {},
   "source": [
    "# Step 1 - PDF Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(pdf_path: Path) -> str:\n",
    "\n",
    "    with fitz.open(pdf_path) as pdf_file:\n",
    "        text_content = \"\"\n",
    "        \n",
    "        for page_num in range(len(pdf_file)):\n",
    "            page = pdf_file[page_num]\n",
    "            text = page.get_text()\n",
    "            text_content += text + \"\\n\\n\"\n",
    "\n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa138b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Wrote 179 raw txt files to C:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\temp\\raw_txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>base</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>0001.pdf</td>\n",
       "      <td>0001</td>\n",
       "      <td>8180</td>\n",
       "      <td>63994</td>\n",
       "      <td>410635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>0002.pdf</td>\n",
       "      <td>0002</td>\n",
       "      <td>20136</td>\n",
       "      <td>155322</td>\n",
       "      <td>953501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw</td>\n",
       "      <td>0003.pdf</td>\n",
       "      <td>0003</td>\n",
       "      <td>809</td>\n",
       "      <td>5242</td>\n",
       "      <td>35149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw</td>\n",
       "      <td>0004.pdf</td>\n",
       "      <td>0004</td>\n",
       "      <td>12262</td>\n",
       "      <td>96822</td>\n",
       "      <td>606118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>0005.pdf</td>\n",
       "      <td>0005</td>\n",
       "      <td>5726</td>\n",
       "      <td>47363</td>\n",
       "      <td>303755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stage source_pdf  base  lines   words   chars\n",
       "0   raw   0001.pdf  0001   8180   63994  410635\n",
       "1   raw   0002.pdf  0002  20136  155322  953501\n",
       "2   raw   0003.pdf  0003    809    5242   35149\n",
       "3   raw   0004.pdf  0004  12262   96822  606118\n",
       "4   raw   0005.pdf  0005   5726   47363  303755"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step1_extract_raw(catalog_csv: Path = CATALOG_CSV) -> pd.DataFrame:\n",
    "    catalog = load_catalog(catalog_csv)\n",
    "    pdfs = sorted(RAW_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "    all_recs: List[Dict] = []\n",
    "    for pdf_path in pdfs:\n",
    "        base = pdf_path.stem.zfill(4)\n",
    "        if base not in catalog:\n",
    "            write_error(base, \"catalog_missing\", \"file_num not found in catalog\", {\"pdf\": pdf_path.name})\n",
    "            # still continue, but skip\n",
    "            continue\n",
    "\n",
    "        raw_layout = read_pdf(pdf_path)\n",
    "\n",
    "        raw_out = RAW_TXT_DIR / f\"raw_{base}.txt\"\n",
    "        write_text(raw_out, raw_layout)\n",
    "\n",
    "        rec_raw = {\n",
    "            \"stage\": \"raw\",\n",
    "            \"source_pdf\": pdf_path.name,\n",
    "            \"base\": base\n",
    "            }\n",
    "        rec_raw.update(count_stats(raw_layout))\n",
    "        all_recs.append(rec_raw)\n",
    "\n",
    "    df = pd.DataFrame(all_recs)\n",
    "    if not df.empty:\n",
    "        (OUTPUT_DIR / \"manifest_raw.csv\").write_text(\n",
    "            df.to_csv(index=False, encoding=\"utf-8\"), encoding=\"utf-8\"\n",
    "        )\n",
    "    print(f\"[Step 1] Wrote {len(df)} raw txt files to {RAW_TXT_DIR.resolve()}\")\n",
    "    return df\n",
    "\n",
    "# Run Step 1\n",
    "df_raw = step1_extract_raw(CATALOG_CSV)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5070",
   "metadata": {},
   "source": [
    "# Step 2 - Cleaning Raw TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c449cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Cleaning ==============\n",
    "def clean_raw_text(raw: str, title_candidate: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto\n",
    "    \"\"\"\n",
    "    txt = raw.replace(\"\\r\", \"\")\n",
    "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
    "    number_line_re = re.compile(r'^\\s*\\d+\\s*$')  # matches only numbers\n",
    "\n",
    "    lines = [ #removes pagination numbers\n",
    "        ln.rstrip()\n",
    "        for ln in txt.split(\"\\n\")\n",
    "        if not number_line_re.match(ln)\n",
    "    ]\n",
    "\n",
    "\n",
    "    new_lines = []\n",
    "    for i, line in enumerate(lines):#SEPARATE all ARTICULOS IN NEW LINES FOR CLEAN\n",
    "        if i > 0:\n",
    "            prev = lines[i-1].strip()\n",
    "            curr = line.strip()\n",
    "            if (\n",
    "                prev.isupper() and\n",
    "                (curr.startswith(\"Artículo\") or curr.startswith(\"Articulo\") or curr.startswith(\"Art.\") or curr.startswith(\"ART.\") or curr.startswith(\"ARTICULO\") or curr.startswith(\"ARTÍCULO\"))\n",
    "            ):\n",
    "                new_lines.append(\"\")\n",
    "        new_lines.append(line)\n",
    "    lines = new_lines\n",
    "    txt = \"\\n\".join(lines)\n",
    "    txt = re.sub(r\"(\\n\\s*){2,}\", \"__PARAGRAPH_BREAK__\", txt)\n",
    "    txt = txt.replace(\"\\n\", \" \")\n",
    "    txt = txt.replace(\"__PARAGRAPH_BREAK__\", \"\\n\")\n",
    "    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n",
    "\n",
    "\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af2156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14cacbc7",
   "metadata": {},
   "source": [
    "Transitorios split? ask harry\n",
    "def split_transitorios_inline(line):\n",
    "    # Pattern to match transitorios keyword anywhere in the line\n",
    "    pattern = r\"(t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\\s*s|transitorios|articulos?\\s+transitorios|transitorio|t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\\s*)\"\n",
    "    match = re.search(pattern, norm_lower(line))\n",
    "    if match:\n",
    "        idx = match.start()\n",
    "        before = line[:idx].rstrip()\n",
    "        keyword = line[idx:match.end()].strip()\n",
    "        after = line[match.end():].lstrip()\n",
    "        result = []\n",
    "        if before:\n",
    "            result.append(before)\n",
    "        result.append(keyword)\n",
    "        if after:\n",
    "            result.append(after)\n",
    "        return result\n",
    "    else:\n",
    "        return [line]\n",
    "\n",
    "\n",
    "\n",
    "Mejor lo hare manual, son 2 txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "206d4a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] Wrote 179 cleaned txt files to C:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\temp\\clean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>source_raw</th>\n",
       "      <th>base</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean</td>\n",
       "      <td>raw_0001.txt</td>\n",
       "      <td>0001</td>\n",
       "      <td>2039</td>\n",
       "      <td>63844</td>\n",
       "      <td>399616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clean</td>\n",
       "      <td>raw_0002.txt</td>\n",
       "      <td>0002</td>\n",
       "      <td>5977</td>\n",
       "      <td>155321</td>\n",
       "      <td>927505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean</td>\n",
       "      <td>raw_0003.txt</td>\n",
       "      <td>0003</td>\n",
       "      <td>190</td>\n",
       "      <td>5242</td>\n",
       "      <td>33688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clean</td>\n",
       "      <td>raw_0004.txt</td>\n",
       "      <td>0004</td>\n",
       "      <td>3473</td>\n",
       "      <td>96822</td>\n",
       "      <td>590629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clean</td>\n",
       "      <td>raw_0005.txt</td>\n",
       "      <td>0005</td>\n",
       "      <td>1356</td>\n",
       "      <td>47363</td>\n",
       "      <td>296286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stage    source_raw  base  lines   words   chars\n",
       "0  clean  raw_0001.txt  0001   2039   63844  399616\n",
       "1  clean  raw_0002.txt  0002   5977  155321  927505\n",
       "2  clean  raw_0003.txt  0003    190    5242   33688\n",
       "3  clean  raw_0004.txt  0004   3473   96822  590629\n",
       "4  clean  raw_0005.txt  0005   1356   47363  296286"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step2_clean_raw(catalog_csv: Path = CATALOG_CSV) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 2: Clean the raw text files extracted in Step 1.\n",
    "    Reads from RAW_TXT_DIR and writes cleaned versions to CLEAN_DIR.\n",
    "    \"\"\"\n",
    "    catalog = load_catalog(catalog_csv)\n",
    "    raw_files = sorted(RAW_TXT_DIR.glob(\"raw_*.txt\"))\n",
    "\n",
    "    all_recs: List[Dict] = []\n",
    "    \n",
    "    for raw_file in raw_files:\n",
    "        # Extract base from filename (e.g., \"raw_0001.txt\" -> \"0001\")\n",
    "        base = raw_file.stem.replace(\"raw_\", \"\")\n",
    "        \n",
    "        if base not in catalog:\n",
    "            write_error(base, \"catalog_missing\", \"file_num not found in catalog (clean stage)\", \n",
    "                       {\"raw_file\": raw_file.name})\n",
    "            continue\n",
    "\n",
    "        # Read raw text\n",
    "        raw_text = raw_file.read_text(encoding=\"utf-8\")\n",
    "        \n",
    "        # Get law metadata for potential title matching\n",
    "        meta = catalog[base]\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = clean_raw_text(raw_text, meta.law_name)\n",
    "        \n",
    "        # Write cleaned text\n",
    "        clean_out = CLEAN_DIR / f\"clean_{base}.txt\"\n",
    "        write_text(clean_out, cleaned_text)\n",
    "        \n",
    "        # Create record with statistics\n",
    "        rec_clean = {\n",
    "            \"stage\": \"clean\",\n",
    "            \"source_raw\": raw_file.name,\n",
    "            \"base\": base,\n",
    "        }\n",
    "        rec_clean.update(count_stats(cleaned_text))\n",
    "        all_recs.append(rec_clean)\n",
    "\n",
    "    df = pd.DataFrame(all_recs)\n",
    "    if not df.empty:\n",
    "        (OUTPUT_DIR / \"manifest_clean.csv\").write_text(\n",
    "            df.to_csv(index=False, encoding=\"utf-8\"), encoding=\"utf-8\"\n",
    "        )\n",
    "    print(f\"[Step 2] Wrote {len(df)} cleaned txt files to {CLEAN_DIR.resolve()}\")\n",
    "    return df\n",
    "\n",
    "# Run Step 1\n",
    "df_clean = step2_clean_raw(CATALOG_CSV)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897daf4",
   "metadata": {},
   "source": [
    "# Step 3 - Splitting Decreto-Ley-Transitorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your header vocabulary (accent-insensitive via normalization below)\n",
    "HIER = [\"disposiciones preliminares\", \"libro\", \"titulo\", \"capitulo\", \"seccion\", \"articulo\", \"disposiciones generales\"]\n",
    "\n",
    "# Build a header regex against the *normalized-lower* text (see norm_lower)\n",
    "HIER_RE = re.compile(\n",
    "    r\"^\\s*(?:%s)\\b\" % \"|\".join(re.escape(h) for h in HIER),\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "# More tolerant Transitorios regex (accepts leading spaces and both spaced and plain spellings)\n",
    "TRANS_RE = re.compile(\n",
    "    r\"^\\s*(?:t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\\s*s|transitorios|articulos?\\s+transitorios|transitorio|t\\s*r\\s*a\\s*n\\s*s\\s*i\\s*t\\s*o\\s*r\\s*i\\s*o\\s*)\\b\",\n",
    "    re.I\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81ec6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _contains_allcaps_prefix(line: str, prefix_caps: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if the normalized-uppercase view of 'line' contains the two-word\n",
    "    ALL-CAPS prefix 'prefix_caps' as a token span, accent-insensitive.\n",
    "    \"\"\"\n",
    "    head = _norm_caps(line)\n",
    "    pref = _norm_caps(prefix_caps)\n",
    "    if not pref:\n",
    "        return False\n",
    "    # Whole-token boundaries: not preceded/followed by A–Z/0–9\n",
    "    pat = re.compile(rf\"(?<![A-Z0-9]){re.escape(pref)}(?![A-Z0-9])\")\n",
    "    return bool(pat.search(head))\n",
    "\n",
    "def _next_nonempty_index(lines: List[str], j: int) -> Optional[int]:\n",
    "    n = len(lines)\n",
    "    while j < n and not lines[j].strip():\n",
    "        j += 1\n",
    "    return j if j < n else None\n",
    "\n",
    "def find_decreto_ley_start_two_words(lines: List[str], first_two_caps: str) -> Optional[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Find the pair (title_idx, ley_start_idx) such that:\n",
    "      - lines[title_idx] contains the two-word title prefix (accent-insensitive)\n",
    "      - one of the next several non-empty lines is a HIER header\n",
    "    \n",
    "    Args:\n",
    "        lines: List of text lines\n",
    "        first_two_caps: Two-word prefix to search for\n",
    "        max_search_lines: Maximum number of lines to search after finding the title\n",
    "    \"\"\"\n",
    "    max_search_lines = 40\n",
    "    for i in range(len(lines) - 1):\n",
    "        if _contains_allcaps_prefix(lines[i], first_two_caps):\n",
    "            # Search through the next several non-empty lines\n",
    "            search_start = i + 1\n",
    "            lines_searched = 0\n",
    "            j = search_start\n",
    "            \n",
    "            while j < len(lines) and lines_searched < max_search_lines:\n",
    "                # Skip empty lines\n",
    "                if not lines[j].strip():\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check if this line is a HIER header\n",
    "                if HIER_RE.search(norm_lower(lines[j].lstrip())):\n",
    "                    return i, j\n",
    "                lines_searched += 1\n",
    "                j += 1\n",
    "    return None\n",
    "\n",
    "def first_transitorios_after(lines: List[str], start: int) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Find the first index >= start+1 that looks like a 'Transitorios' heading,\n",
    "    tolerant of leading spaces and both spaced/plain spellings.\n",
    "    \"\"\"\n",
    "    for i in range(max(start + 1, 0), len(lines)):\n",
    "        if TRANS_RE.search(norm_lower(lines[i].lstrip())):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_blocks_two_word_strict(cleaned_text: str, first_two_caps: str, base: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split a cleaned text into:\n",
    "      - decreto: everything before the ALL-CAPS two-word title line\n",
    "      - ley: from the first HIER line after that title, up to 'Transitorios' (if any)\n",
    "      - transitorios: from 'Transitorios' to end (if present)\n",
    "    If the (title -> header) pair is not found, everything (up to 'Transitorios') is put into 'decreto'.\n",
    "    \"\"\"\n",
    "    lines = cleaned_text.splitlines()\n",
    "    pair = find_decreto_ley_start_two_words(lines, first_two_caps)\n",
    "\n",
    "    # Fallback: couldn't find the (title → header) pair\n",
    "    if pair is None:\n",
    "        t_idx = first_transitorios_after(lines, 0)\n",
    "        if t_idx is not None:\n",
    "            decreto = \"\\n\".join(lines[:t_idx]).strip()\n",
    "            tran    = \"\\n\".join(lines[t_idx:]).strip()\n",
    "        else:\n",
    "            decreto = cleaned_text\n",
    "            tran    = \"\"\n",
    "        write_error(base, \"two_word_pair_not_found\",\n",
    "                    \"No (ALL-CAPS two-word title line → HIER) pair; ley not split.\",\n",
    "                    {\"clean\": f\"clean_{base}.txt\"})\n",
    "        return {\"decreto\": decreto, \"ley\": \"\", \"transitorios\": tran}\n",
    "\n",
    "    title_idx, ley_start = pair\n",
    "    decreto = \"\\n\".join(lines[:ley_start]).strip()\n",
    "\n",
    "    t_idx = first_transitorios_after(lines, ley_start)\n",
    "    if t_idx is not None and t_idx > ley_start:\n",
    "        ley  = \"\\n\".join(lines[ley_start: t_idx]).strip()\n",
    "        tran = \"\\n\".join(lines[t_idx:]).strip()\n",
    "    else:\n",
    "        ley  = \"\\n\".join(lines[ley_start:]).strip()\n",
    "        tran = \"\"\n",
    "    return {\"decreto\": decreto, \"ley\": ley, \"transitorios\": tran, \"decreto_start\": 0, \"ley_start\": ley_start, \"transitorios_start\": t_idx}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89723c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3] Split outputs written: ley→C:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\leyes, decreto→C:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\decretos, transitorios→C:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\transitorios\n",
      "Decretos created: 179\n",
      "Ley created: 179\n",
      "Transitorios created: 179\n",
      "All laws have transitorios section.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>part</th>\n",
       "      <th>base</th>\n",
       "      <th>start_line</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>split</td>\n",
       "      <td>decreto</td>\n",
       "      <td>0001</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>split</td>\n",
       "      <td>ley</td>\n",
       "      <td>0001</td>\n",
       "      <td>3</td>\n",
       "      <td>998</td>\n",
       "      <td>37866</td>\n",
       "      <td>245979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split</td>\n",
       "      <td>transitorios</td>\n",
       "      <td>0001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1038</td>\n",
       "      <td>25940</td>\n",
       "      <td>153421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>split</td>\n",
       "      <td>decreto</td>\n",
       "      <td>0002</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>79</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>split</td>\n",
       "      <td>ley</td>\n",
       "      <td>0002</td>\n",
       "      <td>7</td>\n",
       "      <td>5645</td>\n",
       "      <td>148623</td>\n",
       "      <td>887805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stage          part  base  start_line  lines   words   chars\n",
       "0  split       decreto  0001           0      3      38     214\n",
       "1  split           ley  0001           3    998   37866  245979\n",
       "2  split  transitorios  0001        1001   1038   25940  153421\n",
       "3  split       decreto  0002           0      7      79     479\n",
       "4  split           ley  0002           7   5645  148623  887805"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step3_split_cleaned(catalog_csv: Path = CATALOG_CSV) -> pd.DataFrame:\n",
    "    catalog = load_catalog(catalog_csv)\n",
    "    cleans = sorted(CLEAN_DIR.glob(\"clean_*.txt\"))\n",
    "\n",
    "    all_recs: List[Dict] = []\n",
    "    count_decreto = 0\n",
    "    count_ley = 0\n",
    "    count_transitorios = 0\n",
    "    missing_transitorios = []\n",
    "    for clean_file in cleans:\n",
    "        base = clean_file.stem.replace(\"clean_\", \"\")\n",
    "        meta = catalog.get(base)\n",
    "        if meta is None:\n",
    "            write_error(base, \"catalog_missing\", \"file_num not found in catalog (split stage)\",\n",
    "                        {\"clean\": clean_file.name})\n",
    "            continue\n",
    "\n",
    "        cleaned_text = clean_file.read_text(encoding=\"utf-8\")\n",
    "        parts = split_blocks_two_word_strict(cleaned_text, meta.first_two_caps, base)\n",
    "\n",
    "        # Decreto\n",
    "        if parts.get(\"decreto\", \"\").strip():\n",
    "            out = DECR_DIR / f\"decr_{base}.txt\"\n",
    "            write_text(out, parts[\"decreto\"])\n",
    "            rec = {\"stage\": \"split\", \"part\": \"decreto\", \"base\": base, \"start_line\": parts.get(\"decreto_start\", None)}\n",
    "            rec.update(count_stats(parts[\"decreto\"]))\n",
    "            all_recs.append(rec)\n",
    "            count_decreto += 1\n",
    "\n",
    "        # Ley\n",
    "        if parts.get(\"ley\", \"\").strip():\n",
    "            out = LEY_DIR / f\"{base}.txt\"\n",
    "            write_text(out, parts[\"ley\"])\n",
    "            rec = {\"stage\": \"split\", \"part\": \"ley\", \"base\": base, \"start_line\": parts.get(\"ley_start\", None)}\n",
    "            rec.update(count_stats(parts[\"ley\"]))\n",
    "            all_recs.append(rec)\n",
    "            count_ley += 1\n",
    "\n",
    "        # Transitorios\n",
    "        if parts.get(\"transitorios\", \"\").strip():\n",
    "            out = TRANS_DIR / f\"tran_{base}.txt\"\n",
    "            write_text(out, parts[\"transitorios\"])\n",
    "            rec = {\"stage\": \"split\", \"part\": \"transitorios\", \"base\": base, \"start_line\": parts.get(\"transitorios_start\", None)}\n",
    "            rec.update(count_stats(parts[\"transitorios\"]))\n",
    "            all_recs.append(rec)\n",
    "            count_transitorios += 1\n",
    "        else:\n",
    "            missing_transitorios.append(base)\n",
    "\n",
    "    df = pd.DataFrame(all_recs)\n",
    "    if not df.empty:\n",
    "        (OUTPUT_DIR / \"manifest_parts.csv\").write_text(\n",
    "            df.to_csv(index=False, encoding=\"utf-8\"), encoding=\"utf-8\"\n",
    "        )\n",
    "    print(f\"[Step 3] Split outputs written: ley→{LEY_DIR.resolve()}, decreto→{DECR_DIR.resolve()}, transitorios→{TRANS_DIR.resolve()}\")\n",
    "    print(f\"Decretos created: {count_decreto}\")\n",
    "    print(f\"Ley created: {count_ley}\")\n",
    "    print(f\"Transitorios created: {count_transitorios}\")\n",
    "\n",
    "    #check laws without transitorios\n",
    "    if missing_transitorios:\n",
    "        print(\"Laws with no transitorios section:\", missing_transitorios)\n",
    "    else:\n",
    "        print(\"All laws have transitorios section.\")\n",
    "    return df\n",
    "# Run Step 3\n",
    "df_parts = step3_split_cleaned(CATALOG_CSV)\n",
    "df_parts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2b48f",
   "metadata": {},
   "source": [
    "## **Step 4: Hierarchical JSON Structure Generation**\n",
    "\n",
    "### **Purpose**\n",
    "Transform cleaned law text into structured JSON format that preserves the hierarchical organization of legal documents. This enables programmatic analysis, search, and processing of legal content.\n",
    "\n",
    "### **Structural Parsing**\n",
    "- **Hierarchy Detection** - Identifies libros, títulos, capítulos, secciones, artículos\n",
    "- **Content Organization** - Builds nested tree structure reflecting legal document hierarchy  \n",
    "- **Article Analysis** - Parses individual articles with content and annotations\n",
    "- **Metadata Integration** - Includes source information and validation metadata\n",
    "\n",
    "### **Advanced Features**\n",
    "- **Automatic Repair** - Detects and fixes embedded article headers within content\n",
    "- **Sequence Validation** - Identifies gaps or jumps in article numbering\n",
    "- **Error Reporting** - Comprehensive logging of parsing issues and structural problems\n",
    "- **Quality Metrics** - Statistical analysis of parsed content for validation\n",
    "\n",
    "### **Output Format**\n",
    "Structured JSON with nested hierarchy, article content, annotations, and comprehensive metadata for downstream processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe17e6d",
   "metadata": {},
   "source": [
    "### **Article Sequence Validation**\n",
    "\n",
    "The parser includes intelligent detection of article numbering gaps (e.g., jumping from \"Artículo 12\" to \"Artículo 15\" without 13-14). This helps identify:\n",
    "\n",
    "- **Missing Content** - Articles that may have been lost during PDF extraction\n",
    "- **Structural Issues** - Formatting problems that affect article detection  \n",
    "- **Document Quality** - Overall completeness of the legal document\n",
    "\n",
    "The system automatically attempts repairs for embedded article headers and provides detailed reporting of any remaining gaps for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea80c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, Set\n",
    "\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2e94f",
   "metadata": {},
   "source": [
    "### **Configuration: Allowed Suffixes**\n",
    "\n",
    "The `ALLOWED_SUFFIX` dictionary contains valid suffixes for each hierarchical level, derived from exploratory analysis of the legal corpus. \n",
    "\n",
    "**Future Enhancement**: This configuration should be generated dynamically for each legislature to account for:\n",
    "- Regional variations in legal terminology\n",
    "- Historical changes in numbering conventions  \n",
    "- Document-specific structural patterns\n",
    "\n",
    "**Current Implementation**: Hard-coded based on analysis of existing documents. See exploration notebooks for suffix derivation methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd372e9",
   "metadata": {},
   "source": [
    "### **Enhancement Opportunity: Typo-Resistant Suffix Matching**\n",
    "\n",
    "**Current Limitation**: Exact string matching for legal suffixes may miss valid entries due to:\n",
    "- OCR scanning errors in PDF extraction\n",
    "- Typographical variations in source documents\n",
    "- Accent mark inconsistencies\n",
    "\n",
    "**Proposed Improvements**:\n",
    "1. **Fuzzy String Matching** - Use edit distance algorithms for approximate matching\n",
    "2. **Phonetic Matching** - Handle accent mark variations and similar sounds\n",
    "3. **Machine Learning** - Train classifiers on known good/bad suffix patterns\n",
    "4. **Manual Review Interface** - Flag uncertain matches for human validation\n",
    "\n",
    "This would significantly improve parsing accuracy for lower-quality source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Allowed suffixes and header patterns (unchanged lists provided by you)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "ALLOWED_SUFFIX = {\n",
    "    \"libro\": [\"cuarto\",\"decimo\",\"noveno\",\"octavo\",\"primero\",\"quinto\",\"segundo\",\"septimo\",\"sexto\",\"tercero\"],\n",
    "    \"titulo\": [\"catorce\",\"cuarto\",\"decimo\",\"decimo bis\",\"decimoctavo\",\"decimocuarto\",\"decimonoveno\",\"decimoprimero\",\"decimoquinto\",\"decimosegundo\",\"decimoseptimo\",\"decimosexto\",\"decimotercero\",\"dieciseis\",\"doce\",\"duodecimo\",\"i\",\"ii\",\"iii\",\"iv\",\"ix\",\"noveno\",\"octavo\",\"octavo bis\",\"once\",\"preliminar\",\"primero\",\"quince\",\"quinto\",\"quinto bis\",\"segundo\",\"segundo bis\",\"septimo\",\"septimo bis\",\"sexto\",\"tercero\",\"tercero bis\",\"trece\",\"trece bis\",\"undecimo\",\"unico\",\"v\",\"vi\",\"vigesimo\",\"vigesimocuarto\",\"vigesimoprimero\",\"vigesimosegundo\",\"vigesimotercero\",\"vii\",\"viii\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xv\",\"xvi\",\"especial\", \"decimo-primero\",\"decimo-segundo\",\"decimo-tercero\",\"decimo-cuarto\",\"decimo-quinto\",\"decimo-sexto\",\"decimo-septimo\",\"decimo-octavo\",\"decimo-noveno\",\"decimo-quinto bis\",\"decimo-segundo bis\",\"decimo-tercero bis\",\"decimo-cuarto bis\",\"decimo-sexto bis\",\"decimo-septimo bis\",\"decimo-octavo bis\",\"decimo-noveno bis\"],\n",
    "    \"capitulo\": [\"1\",\"cuarto\",\"cuarto bis\",\"decimo\",\"duodecimo\",\"especial\",\"i\",\"i bis\",\"ii\",\"ii bis\",\"iii\",\"iii bis\",\"iii ter\",\"iv\",\"iv bis\",\"iv ter\",\"ix\",\"ix bis\",\"ix ter\",\"noveno\",\"octavo\",\"primero\",\"quinto\",\"segundo\",\"septimo\",\"sexto\",\"tercero\",\"undecimo\",\"unico\",\"v\",\"v bis\",\"v ter\",\"vi\",\"vi bis\",\"vigesimo\",\"vii\",\"vii bis\",\"viii\",\"viii bis\",\"x\",\"x bis\",\"xi\",\"xii\",\"xii bis\",\"xiii\",\"xiii bis\",\"xiv\",\"xix\",\"xv\",\"xv bis\",\"xv quater\",\"xv ter\",\"xvi\",\"xvi bis\",\"xvii\",\"xviii\",\"xx\",\"xxi\",\"xxii\",\"xxiii\",\"xxiv\",\"xxix\",\"xxv\",\"xxvi\",\"xxvii\",\"xxviii\", \"decimocuarto\",\"decimoquinto\",\"decimosexto\",\"decimoseptimo\",\"decimooctavo\",\"decimonoveno\",\"decimoprimero\",\"decimosegundo\",\"decimotercero\",\"vigesima\"],\n",
    "    \"seccion\": [\"1a\",\"2a\",\"3a\",\"4a\",\"5a\",\"6a\",\"7a\",\"8a\",\"a\",\"b\",\"cuarta\",\"decima\",\"decima bis\",\"decimo\",\"i\",\"ii\",\"iii\",\"iv\",\"ix\",\"novena\",\"octava\",\"primera\",\"primera bis\",\"quinta\",\"segunda\",\"segunda bis\",\"septima\",\"sexta\",\"tercera\",\"unica\",\"v\",\"vi\",\"vii\",\"vii bis\",\"viii\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xix\",\"xv\",\"xvi\",\"xvii\",\"xviii\",\"xx\",\"xxi\",\"xxii\",\"uno\",\"dos\",\"tres\",\"cuatro\",\"cinco\",\"seis\",\"siete\",\"ocho\",\"nueve\",\"diez\",\"once\",\"decimoprimera\",\"decimosegunda\",\"decimotercera\",\"decimocuarta\",\"decimoquinta\",\"decimosexta\",\"decimoseptima\",\"decimoctava\",\"decimonovena\",\"vigesima\",\"onceava\"],\n",
    "}\n",
    "\n",
    "ROMAN_RE = re.compile(r'^(?i:xxiv|xxiii|xxii|xxi|xx|xix|xviii|xvii|xvi|xv|xiv|xiii|xii|xi|x|ix|viii|vii|vi|v|iv|iii|ii|i)$')\n",
    "\n",
    "HDR_WORDS = {\n",
    "    \"libro\":    re.compile(r'^\\s*(LIBRO)\\b', re.IGNORECASE),\n",
    "    \"titulo\":   re.compile(r'^\\s*(T[ÍI]TULO)\\b', re.IGNORECASE),\n",
    "    \"capitulo\": re.compile(r'^\\s*(CAP[ÍI]TULO)\\b', re.IGNORECASE),\n",
    "    \"seccion\":  re.compile(r'^\\s*(SECCI[ÓO]N)\\b', re.IGNORECASE),\n",
    "}\n",
    "\n",
    "LEVEL = {\"libro\": 1, \"preliminar\": 1, \"titulo\": 2, \"capitulo\": 3, \"seccion\": 4, \"articulo\": 5}\n",
    "\n",
    "# Inline \"Artículo\" header finder used for repair (accepts \"Artículo\" or \"Art.\")\n",
    "ARTICULO_INLINE_RE = re.compile(\n",
    "    r'(?i)(?<!\\w)(?:art[íi]culo|art\\.)\\s*'\n",
    "    r'(?P<sufijo>'               # full suffix capture as text \"7\", \"7 bis\", \"7-A\"\n",
    "    r'\\d+(?:\\s*(?:bis|ter|quater|quinquies|sexies|septies|octies|nonies|decies|undecies|duodecies|terdecies|[A-Za-z\\-]+)?)?'\n",
    "    r')\\s*\\.(?:-)?\\s*'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def collapse_ws(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return collapse_ws(unidecode(s).lower())\n",
    "\n",
    "def edit_distance(a: str, b: str) -> int:\n",
    "    la, lb = len(a), len(b)\n",
    "    dp = list(range(lb+1))\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            cur = dp[j]\n",
    "            cost = 0 if ca == cb else 1\n",
    "            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)\n",
    "            prev = cur\n",
    "    return dp[lb]\n",
    "\n",
    "def is_articulo_token(token: str) -> bool:\n",
    "    return edit_distance(norm(token), \"articulo\") <= 2 or norm(token) in {\"art\", \"art.\"}\n",
    "\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return [t for t in re.split(r'\\s+', s.strip()) if t]\n",
    "\n",
    "def allowed_suffix_for(tipo: str, candidate_tokens: List[str]) -> Tuple[Optional[str], int]:\n",
    "    if not candidate_tokens:\n",
    "        return None, 0\n",
    "    raw = candidate_tokens[:3]\n",
    "    cleaned = [re.sub(r'[.\\-:—]+$', '', tok).strip() for tok in raw]\n",
    "\n",
    "    allowed = set(ALLOWED_SUFFIX.get(tipo, []))\n",
    "    allowed_norm = {norm(x) for x in allowed} | {norm(x.replace(' ', '')) for x in allowed}\n",
    "\n",
    "    max_span = min(3, len(cleaned))\n",
    "    for k in range(max_span, 0, -1):\n",
    "        span_clean = cleaned[:k]\n",
    "        as_is_clean = collapse_ws(' '.join(span_clean))\n",
    "        as_norm = norm(as_is_clean)\n",
    "        as_join_norm = norm(''.join(span_clean))\n",
    "\n",
    "        if tipo == \"seccion\":\n",
    "            if re.fullmatch(r'\\d+[aª]$', as_norm) or re.fullmatch(r'\\d+$', as_norm):\n",
    "                return as_is_clean, k\n",
    "\n",
    "        if tipo in (\"libro\", \"titulo\", \"capitulo\", \"seccion\"):\n",
    "            if ROMAN_RE.match(as_norm) or re.fullmatch(r'\\d+$', as_norm):\n",
    "                return as_is_clean, k\n",
    "\n",
    "        if as_norm in allowed_norm or as_join_norm in allowed_norm:\n",
    "            return as_is_clean, k\n",
    "    return None, 0\n",
    "\n",
    "def split_header_rest(line: str, header_word_span: Tuple[int,int]) -> str:\n",
    "    return line[header_word_span[1]:].strip()\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    cleaned = re.sub(r'[<>:\"/\\\\|?*\\r\\n\\t]+', ' ', name)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    if not cleaned or cleaned in {\".\", \"..\"}:\n",
    "        cleaned = \"ley\"\n",
    "    return cleaned\n",
    "\n",
    "# -------------------------------- Inline notes logic ----------------------------------\n",
    "\n",
    "NOTE_PARENS_RE = re.compile(r'\\(([^()]*)\\)')  # one level\n",
    "\n",
    "def strip_inline_notes(line: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove '(...)' segments ONLY if inside they contain 'no.' (case-insensitive).\n",
    "    Return (cleaned_line, [notes_without_brackets]).\n",
    "    \"\"\"\n",
    "    if not line:\n",
    "        return line, []\n",
    "    notes: List[str] = []\n",
    "    kept_parts: List[str] = []\n",
    "    idx = 0\n",
    "    for m in NOTE_PARENS_RE.finditer(line):\n",
    "        start, end = m.span()\n",
    "        content = m.group(1) or \"\"\n",
    "        if re.search(r'(?i)\\bno\\.', content, flags=re.IGNORECASE):\n",
    "            kept_parts.append(line[idx:start])\n",
    "            notes.append(collapse_ws(content.strip()))\n",
    "            idx = end\n",
    "        else:\n",
    "            kept_parts.append(line[idx:end])\n",
    "            idx = end\n",
    "    kept_parts.append(line[idx:])\n",
    "    cleaned = ''.join(kept_parts).strip()\n",
    "    return cleaned, notes\n",
    "\n",
    "# ----------------------------------- Data model ---------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    tipo: str\n",
    "    sufijo: str\n",
    "    nombre: Optional[str]\n",
    "    nota: List[str] = field(default_factory=list)\n",
    "    contenido: Union[str, List['Node']] = field(default_factory=list)\n",
    "    start: int = 0\n",
    "    end: int = 0\n",
    "    line: int = 0\n",
    "    level: int = 0\n",
    "    header_line_text: Optional[str] = None\n",
    "\n",
    "    def to_json_obj(self) -> Dict[str, Any]:\n",
    "        base = {\"tipo\": self.tipo, \"sufijo\": self.sufijo}\n",
    "        if self.tipo != \"articulo\":\n",
    "            base[\"nombre\"] = self.nombre if self.nombre is not None else None\n",
    "            base[\"nota\"] = self.nota[:] if self.nota else []\n",
    "            base[\"contenido\"] = [c.to_json_obj() for c in (self.contenido or [])]\n",
    "        else:\n",
    "            base[\"nota\"] = self.nota[:] if self.nota else []\n",
    "            base[\"contenido\"] = self.contenido if isinstance(self.contenido, str) else \"\"\n",
    "        return base\n",
    "\n",
    "# -------------------------------- Counting utility ------------------------------------\n",
    "\n",
    "def count_unidades(nodes: List[Node]) -> Dict[str, int]:\n",
    "    counts = {\"libros\":0,\"titulos\":0,\"capitulos\":0,\"secciones\":0,\"articulos\":0}\n",
    "    def _walk(n: Node):\n",
    "        if n.tipo == \"libro\": counts[\"libros\"] += 1\n",
    "        elif n.tipo == \"titulo\": counts[\"titulos\"] += 1\n",
    "        elif n.tipo == \"capitulo\": counts[\"capitulos\"] += 1\n",
    "        elif n.tipo == \"seccion\": counts[\"secciones\"] += 1\n",
    "        elif n.tipo == \"articulo\": counts[\"articulos\"] += 1\n",
    "        if isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _walk(c)\n",
    "    for n in nodes: _walk(n)\n",
    "    return counts\n",
    "\n",
    "# -------------------- Header detection & Artículo parsing -----------------------------\n",
    "\n",
    "def detect_container_header(line: str) -> Optional[Tuple[str, Tuple[int,int]]]:\n",
    "    for tipo, pat in HDR_WORDS.items():\n",
    "        m = pat.match(line)\n",
    "        if m:\n",
    "            return tipo, m.span(1)\n",
    "    return None\n",
    "\n",
    "def parse_container_header(clean_line: str, tipo: str, header_span: Tuple[int,int]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    rest = split_header_rest(clean_line, header_span)\n",
    "    if not rest:\n",
    "        return None, None\n",
    "    tokens = tokenize(rest)\n",
    "    if not tokens:\n",
    "        return None, None\n",
    "    suffix, consumed = allowed_suffix_for(tipo, tokens)\n",
    "    if not suffix:\n",
    "        return None, None\n",
    "    after_suffix = ' '.join(tokens[consumed:]).strip() if consumed < len(tokens) else \"\"\n",
    "    nombre_inline = after_suffix if after_suffix else None\n",
    "    return suffix, nombre_inline\n",
    "\n",
    "def parse_articulo_header_and_body(lines: List[str], i: int, line_starts: List[int], full_text: str) -> Optional[Tuple[Node, int]]:\n",
    "    raw_line = lines[i]\n",
    "    line, header_notes = strip_inline_notes(raw_line)\n",
    "\n",
    "    m = re.match(r'^\\s*(\\S+)', line)\n",
    "    if not m:\n",
    "        return None\n",
    "    first = m.group(1)\n",
    "    if not is_articulo_token(first):\n",
    "        return None\n",
    "\n",
    "    rest = line[m.end():]\n",
    "    if not re.match(r'^\\s*\\d+', rest):\n",
    "        return None\n",
    "\n",
    "    term_idx = None\n",
    "    k = 0\n",
    "    while k < len(rest):\n",
    "        if rest[k] == '.':\n",
    "            k2 = k + 1\n",
    "            if k2 < len(rest) and rest[k2] == '-':\n",
    "                k2 += 1\n",
    "            if k2 >= len(rest) or rest[k2].isspace():\n",
    "                term_idx = k\n",
    "                break\n",
    "        k += 1\n",
    "    if term_idx is None:\n",
    "        return None\n",
    "\n",
    "    candidate_suffix = rest[:term_idx].strip()\n",
    "    if not re.search(r'\\d', candidate_suffix):\n",
    "        return None\n",
    "\n",
    "    jstart = term_idx + 1\n",
    "    if jstart < len(rest) and rest[jstart] == '-':\n",
    "        jstart += 1\n",
    "    while jstart < len(rest) and rest[jstart].isspace():\n",
    "        jstart += 1\n",
    "    after = rest[jstart:]\n",
    "\n",
    "    body_lines: List[str] = []\n",
    "    if after.strip():\n",
    "        body_clean, body_notes = strip_inline_notes(after.rstrip())\n",
    "        body_lines.append(body_clean)\n",
    "        header_notes.extend(body_notes)\n",
    "\n",
    "    j = i + 1\n",
    "    while j < len(lines):\n",
    "        candidate_raw = lines[j]\n",
    "        cand_clean, cand_notes = strip_inline_notes(candidate_raw.rstrip())\n",
    "\n",
    "        if detect_container_header(cand_clean) or (\n",
    "            cand_clean.strip() and is_articulo_token(cand_clean.strip().split(' ', 1)[0])\n",
    "        ):\n",
    "            if cand_clean.strip() and is_articulo_token(cand_clean.strip().split(' ', 1)[0]):\n",
    "                lrest = cand_clean.strip()[len(cand_clean.strip().split(' ', 1)[0]):]\n",
    "                if not re.match(r'^\\s*\\d+', lrest):\n",
    "                    header_notes.extend(cand_notes)\n",
    "                    body_lines.append(cand_clean)\n",
    "                    j += 1\n",
    "                    continue\n",
    "            break\n",
    "        header_notes.extend(cand_notes)\n",
    "        body_lines.append(cand_clean)\n",
    "        j += 1\n",
    "\n",
    "    start_char = line_starts[i]\n",
    "    end_char = line_starts[j] if j < len(lines) else len(full_text)\n",
    "    content_text = \"\\n\".join([ln.rstrip() for ln in body_lines if ln.strip()]).strip()\n",
    "\n",
    "    node = Node(\n",
    "        tipo=\"articulo\",\n",
    "        sufijo=candidate_suffix,\n",
    "        nombre=None,\n",
    "        nota=[n for n in header_notes if n],\n",
    "        contenido=content_text,\n",
    "        start=start_char,\n",
    "        end=end_char,\n",
    "        line=i+1,\n",
    "        level=LEVEL[\"articulo\"],\n",
    "        header_line_text=raw_line.strip()\n",
    "    )\n",
    "    return node, j\n",
    "\n",
    "# ----------------------------- Article repair utilities --------------------------------\n",
    "\n",
    "def parse_article_base_int(sufijo: str) -> Optional[int]:\n",
    "    m = re.search(r'(\\d+)', sufijo)\n",
    "    if m:\n",
    "        try:\n",
    "            return int(m.group(1))\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_article_base_and_variant(sufijo: str) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"\n",
    "    Return (base_int, variant_text_normalized_without_spaces) where variant can be ''.\n",
    "    \"\"\"\n",
    "    m = re.search(r'(\\d+)\\s*(.*)$', sufijo.strip())\n",
    "    if not m:\n",
    "        return None, \"\"\n",
    "    base = parse_article_base_int(sufijo)\n",
    "    tail = (m.group(2) or \"\").strip()\n",
    "    tail_norm = norm(tail).replace(\" \", \"\")\n",
    "    return base, tail_norm  # '' if no variant\n",
    "\n",
    "def find_embedded_article_headers(text: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Return list of (start_idx, end_idx_after_header, sufijo_text) matches for inline headers.\n",
    "    \"\"\"\n",
    "    matches: List[Tuple[int,int,str]] = []\n",
    "    for mm in ARTICULO_INLINE_RE.finditer(text):\n",
    "        start = mm.start()\n",
    "        end = mm.end()\n",
    "        suf = collapse_ws(mm.group(\"sufijo\"))\n",
    "        matches.append((start, end, suf))\n",
    "    return matches\n",
    "\n",
    "def split_embedded_articles_in_list(nodes: List[Node]) -> None:\n",
    "    \"\"\"\n",
    "    Traverse a node list; for each artículo node whose content contains inline 'Artículo ...'\n",
    "    headers, split them into separate article nodes IF AND ONLY IF the first embedded header\n",
    "    matches the expected immediate sequence: base+1 OR same base with a non-empty variant.\n",
    "    We accept a chain of subsequent embedded headers only if they continue +1 steps.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(nodes):\n",
    "        node = nodes[i]\n",
    "        # Recurse into containers first\n",
    "        if node.tipo != \"articulo\" and isinstance(node.contenido, list):\n",
    "            split_embedded_articles_in_list(node.contenido)\n",
    "\n",
    "        if node.tipo == \"articulo\" and isinstance(node.contenido, str) and node.contenido:\n",
    "            text = node.contenido\n",
    "            emb = find_embedded_article_headers(text)\n",
    "            if emb:\n",
    "                base0, var0 = parse_article_base_and_variant(node.sufijo)\n",
    "                if base0 is not None:\n",
    "                    # Evaluate first embedded header\n",
    "                    s0, e0, suf0 = emb[0]\n",
    "                    b1, v1 = parse_article_base_and_variant(suf0)\n",
    "                    ok_first = False\n",
    "                    # Allowed start: base+1 or same base with non-empty variant (and not identical suffix)\n",
    "                    if b1 is not None:\n",
    "                        if b1 == base0 + 1:\n",
    "                            ok_first = True\n",
    "                        elif b1 == base0:\n",
    "                            if v1 and norm(suf0) != norm(node.sufijo):\n",
    "                                ok_first = True\n",
    "                    if ok_first and s0 >= 1:\n",
    "                        # Build a chain of accepted matches: consecutive +1 steps\n",
    "                        accepted = [(s0, e0, suf0, b1)]\n",
    "                        expected_next = b1 + 1\n",
    "                        for k in range(1, len(emb)):\n",
    "                            sk, ek, sufk = emb[k]\n",
    "                            bk, vk = parse_article_base_and_variant(sufk)\n",
    "                            if bk is None:\n",
    "                                break\n",
    "                            if bk == expected_next:\n",
    "                                accepted.append((sk, ek, sufk, bk))\n",
    "                                expected_next += 1\n",
    "                            else:\n",
    "                                # stop at first non-consecutive\n",
    "                                break\n",
    "\n",
    "                        # Perform split\n",
    "                        new_nodes: List[Node] = []\n",
    "                        # part before first embedded header stays in current node\n",
    "                        before = text[:accepted[0][0]].rstrip()\n",
    "                        node.contenido = before\n",
    "\n",
    "                        # For each accepted embedded header, create a new Node with its body\n",
    "                        for idx_acc, (sk, ek, sufk, bk) in enumerate(accepted):\n",
    "                            body_start = ek\n",
    "                            body_end = accepted[idx_acc + 1][0] if idx_acc + 1 < len(accepted) else len(text)\n",
    "                            body = text[body_start:body_end].strip()\n",
    "                            new_nodes.append(Node(\n",
    "                                tipo=\"articulo\",\n",
    "                                sufijo=sufk,\n",
    "                                nombre=None,\n",
    "                                nota=[],\n",
    "                                contenido=body,\n",
    "                                start=0, end=0,\n",
    "                                line=node.line,  # best-effort\n",
    "                                level=LEVEL[\"articulo\"],\n",
    "                                header_line_text=f\"Artículo {sufk}.\"\n",
    "                            ))\n",
    "\n",
    "                        # Insert new nodes right after the original\n",
    "                        nodes[i+1:i+1] = new_nodes\n",
    "                        # Skip past inserted items\n",
    "                        i += len(new_nodes)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------ Main parser -------------------------------------\n",
    "\n",
    "def parse_law_text(text: str, issues: List[Dict[str,Any]]) -> Tuple[str, List[Node], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Parse a full law TXT into a title and list of top-level nodes.\n",
    "    Also returns per-tipo invalid suffixes encountered: {\"libro\":[...],\"titulo\":[...],...}\n",
    "    \"\"\"\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    title = next((ln.strip() for ln in lines if ln.strip()), \"Sin título\")\n",
    "\n",
    "    line_starts = []\n",
    "    pos = 0\n",
    "    for ln in lines:\n",
    "        line_starts.append(pos)\n",
    "        pos += len(ln) + 1\n",
    "\n",
    "    root_nodes: List[Node] = []\n",
    "    stack: List[Node] = []\n",
    "\n",
    "    # Track invalid suffix samples per tipo\n",
    "    invalid_suffixes: Dict[str, Set[str]] = {k: set() for k in [\"libro\",\"titulo\",\"capitulo\",\"seccion\"]}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        raw_line = lines[i]\n",
    "\n",
    "        # Optional PRELIMINAR block at top\n",
    "        if not root_nodes and not stack:\n",
    "            prelim_clean, prelim_notes = strip_inline_notes(raw_line)\n",
    "            if re.match(r'^\\s*disposiciones\\s+preliminares\\b.*$', unidecode(prelim_clean), re.IGNORECASE):\n",
    "                node = Node(\n",
    "                    tipo=\"preliminar\",\n",
    "                    sufijo=\"\",\n",
    "                    nombre=prelim_clean.strip(),\n",
    "                    nota=prelim_notes,\n",
    "                    contenido=[],\n",
    "                    start=line_starts[i],\n",
    "                    end=0,\n",
    "                    line=i+1,\n",
    "                    level=LEVEL[\"preliminar\"],\n",
    "                    header_line_text=raw_line.strip()\n",
    "                )\n",
    "                root_nodes.append(node); stack.append(node)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # 1) Try artículo first\n",
    "        parsed_art = parse_articulo_header_and_body(lines, i, line_starts, text)\n",
    "        if parsed_art:\n",
    "            art_node, j = parsed_art\n",
    "            parent = stack[-1] if stack else None\n",
    "            if parent is None:\n",
    "                root_nodes.append(art_node)\n",
    "            else:\n",
    "                if isinstance(parent.contenido, list):\n",
    "                    parent.contenido.append(art_node)\n",
    "                else:\n",
    "                    parent.contenido = [art_node]\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # 2) Try container header (strip notes first)\n",
    "        clean_line, inline_notes = strip_inline_notes(raw_line)\n",
    "        det = detect_container_header(clean_line)\n",
    "        if det:\n",
    "            tipo, hdr_span = det\n",
    "            suffix, nombre_inline = parse_container_header(clean_line, tipo, hdr_span)\n",
    "\n",
    "            if not suffix:\n",
    "                # Capture candidate \"invalid\" suffix sample for this tipo\n",
    "                rest = split_header_rest(clean_line, hdr_span)\n",
    "                tokens = tokenize(rest)\n",
    "                sample_tokens = [re.sub(r'[.\\-:—,;]+$', '', t).strip() for t in tokens[:3]]\n",
    "                sample = collapse_ws(' '.join([t for t in sample_tokens if t]))\n",
    "                if sample:\n",
    "                    invalid_suffixes.get(tipo, set()).add(sample)\n",
    "\n",
    "                issues.append({\n",
    "                    \"location\": f\"line {i+1}\",\n",
    "                    \"message\": f\"{tipo.title()} sin sufijo válido (línea ignorada)\",\n",
    "                    \"issue_type\": \"warning\",\n",
    "                    \"line_text\": raw_line.strip()\n",
    "                })\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Name-on-next-line rule (strip notes there too)\n",
    "            nombre = nombre_inline\n",
    "            notes_for_node = list(inline_notes)\n",
    "            if nombre is None:\n",
    "                peek = i + 1\n",
    "                while peek < len(lines) and not lines[peek].strip():\n",
    "                    peek += 1\n",
    "                if peek < len(lines):\n",
    "                    next_line_clean, next_line_notes = strip_inline_notes(lines[peek].strip())\n",
    "                    is_header = bool(detect_container_header(next_line_clean))\n",
    "                    is_article_hdr = False\n",
    "                    if next_line_clean.strip():\n",
    "                        first_tok = next_line_clean.strip().split(' ', 1)[0]\n",
    "                        is_article_hdr = is_articulo_token(first_tok) and re.match(\n",
    "                            r'^\\s*\\d+', next_line_clean[len(first_tok):] or \"\")\n",
    "                    if not (is_header or is_article_hdr):\n",
    "                        nombre = next_line_clean if next_line_clean else None\n",
    "                        if next_line_notes:\n",
    "                            notes_for_node.extend(next_line_notes)\n",
    "\n",
    "            node = Node(\n",
    "                tipo=tipo,\n",
    "                sufijo=suffix,\n",
    "                nombre=nombre,\n",
    "                nota=notes_for_node,\n",
    "                contenido=[],\n",
    "                start=line_starts[i],\n",
    "                end=0,\n",
    "                line=i+1,\n",
    "                level=LEVEL[tipo],\n",
    "                header_line_text=raw_line.strip()\n",
    "            )\n",
    "\n",
    "            while stack and stack[-1].level >= node.level:\n",
    "                top = stack.pop()\n",
    "                top.end = line_starts[i]\n",
    "\n",
    "            if not stack:\n",
    "                root_nodes.append(node)\n",
    "            else:\n",
    "                stack[-1].contenido.append(node)\n",
    "            stack.append(node)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 3) Plain line: attach any inline notes to nearest open container\n",
    "        if inline_notes and stack:\n",
    "            for n in inline_notes:\n",
    "                if n and n not in stack[-1].nota:\n",
    "                    stack[-1].nota.append(n)\n",
    "        i += 1\n",
    "\n",
    "    # Close remaining containers at EOF\n",
    "    for n in stack[::-1]:\n",
    "        n.end = len(text)\n",
    "\n",
    "    # ---- Post-parse normalization: split inline embedded artículo headers (auto-repair) ----\n",
    "    split_embedded_articles_in_list(root_nodes)\n",
    "\n",
    "    # Finalize nodes (end positions, note de-dup)\n",
    "    def _finalize(n: Node):\n",
    "        if n.end == 0:\n",
    "            n.end = len(text)\n",
    "        if isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _finalize(c)\n",
    "        seen = set(); dedup = []\n",
    "        for x in n.nota:\n",
    "            if x not in seen:\n",
    "                seen.add(x); dedup.append(x)\n",
    "        n.nota = dedup\n",
    "\n",
    "    for n in root_nodes:\n",
    "        _finalize(n)\n",
    "\n",
    "    # Convert invalid suffix sets to lists\n",
    "    invalid_out = {k: sorted(list(v)) for k, v in invalid_suffixes.items() if v}\n",
    "\n",
    "    return title, root_nodes, invalid_out\n",
    "\n",
    "# ---------------------------- Article sequence validation -----------------------------\n",
    "\n",
    "def validate_article_sequence(nodes: List[Node], file_issues: List[Dict[str,Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Detect forward jumps in base article numbers (> +1).\n",
    "    Returns a list of jump dicts for CSV export and logs a verbose warning per jump.\n",
    "    (Runs AFTER auto-repair, so only genuine jumps remain.)\n",
    "    \"\"\"\n",
    "    arts: List[Node] = []\n",
    "    def _walk(n: Node):\n",
    "        if n.tipo == \"articulo\":\n",
    "            arts.append(n)\n",
    "        elif isinstance(n.contenido, list):\n",
    "            for c in n.contenido: _walk(c)\n",
    "    for n in nodes: _walk(n)\n",
    "\n",
    "    jumps: List[Dict[str,Any]] = []\n",
    "    prev_base = None\n",
    "    prev_node: Optional[Node] = None\n",
    "\n",
    "    for a in arts:\n",
    "        base = parse_article_base_int(a.sufijo)\n",
    "        if base is None:\n",
    "            prev_node = a if prev_node is None else prev_node\n",
    "            continue\n",
    "        if prev_base is None:\n",
    "            prev_base = base\n",
    "            prev_node = a\n",
    "            continue\n",
    "        if base > prev_base + 1:\n",
    "            jump = {\n",
    "                \"prev_line\": prev_node.line if prev_node else \"\",\n",
    "                \"prev_sufijo\": prev_node.sufijo if prev_node else \"\",\n",
    "                \"prev_line_text\": (prev_node.header_line_text or f\"Artículo {prev_node.sufijo}.\") if prev_node else \"\",\n",
    "                \"current_line\": a.line,\n",
    "                \"current_sufijo\": a.sufijo,\n",
    "                \"current_line_text\": a.header_line_text or f\"Artículo {a.sufijo}.\",\n",
    "                \"prev_base\": prev_base,\n",
    "                \"current_base\": base,\n",
    "                \"delta\": base - prev_base\n",
    "            }\n",
    "            jumps.append(jump)\n",
    "            file_issues.append({\n",
    "                \"location\": f\"line {a.line}\",\n",
    "                \"message\": f\"Secuencia de artículos salta de {prev_base} a {base}\",\n",
    "                \"issue_type\": \"warning\",\n",
    "                \"line_text\": jump[\"current_line_text\"]\n",
    "            })\n",
    "        if base >= prev_base:\n",
    "            prev_base = base\n",
    "            prev_node = a\n",
    "\n",
    "    return jumps\n",
    "\n",
    "# --------------------------------- I/O utilities -------------------------------------\n",
    "\n",
    "def law_basename(path: Path) -> str:\n",
    "    return path.stem\n",
    "\n",
    "def inferred_title_from_file(path: Path, parsed_title: str) -> str:\n",
    "    return path.stem\n",
    "\n",
    "def write_json(data: Dict[str,Any], path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def save_errors_to_file(file_issues: List[Dict[str,Any]], file_name: str, errores_dir: Path):\n",
    "    if not file_issues:\n",
    "        return\n",
    "    errores_dir.mkdir(parents=True, exist_ok=True)\n",
    "    error_file_path = errores_dir / f\"{Path(file_name).stem}_errors.txt\"\n",
    "    with error_file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Errores y advertencias para: {file_name}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        for issue in file_issues:\n",
    "            f.write(f\"Ubicación: {issue.get('location', 'N/A')}\\n\")\n",
    "            f.write(f\"Tipo: {issue.get('issue_type', 'warning')}\\n\")\n",
    "            f.write(f\"Mensaje: {issue.get('message', '')}\\n\")\n",
    "            if issue.get(\"line_text\"):\n",
    "                f.write(f\"Línea: {issue['line_text']}\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\")\n",
    "\n",
    "# ------------------------------- Catalogue utilities ----------------------------------\n",
    "\n",
    "def load_catalog(catalog_csv: Path) -> Dict[str, Dict[str,str]]:\n",
    "    mapping: Dict[str, Dict[str,str]] = {}\n",
    "    if catalog_csv and catalog_csv.exists():\n",
    "        with catalog_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                file_num = (row.get(\"file_num\") or \"\").strip()\n",
    "                if not file_num:\n",
    "                    continue\n",
    "                mapping[file_num] = {\n",
    "                    \"law_name\": (row.get(\"law_name\") or \"\").strip(),\n",
    "                    \"link\": (row.get(\"link\") or \"\").strip(),\n",
    "                    \"num_est\": (row.get(\"num_est\") or \"\").strip(),\n",
    "                }\n",
    "    return mapping\n",
    "\n",
    "# ---------------------------------- Main pipeline -------------------------------------\n",
    "\n",
    "def walk_and_process(\n",
    "    ley_dir: Path,\n",
    "    out_dir: Path,\n",
    "    errores_dir: Optional[Path] = None,\n",
    "    catalog_csv: Optional[Path] = None\n",
    ") -> Dict[str, Any]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if errores_dir:\n",
    "        errores_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    catalog = load_catalog(catalog_csv) if catalog_csv else {}\n",
    "\n",
    "    issues_rows = []\n",
    "    jump_rows = []\n",
    "\n",
    "    manifest = {\n",
    "        \"processed_files\": [],\n",
    "        \"files\": {},\n",
    "        \"totals\": {\"libros\":0,\"titulos\":0,\"capitulos\":0,\"secciones\":0,\"articulos\":0},\n",
    "        \"warnings\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "\n",
    "    txt_files = sorted([p for p in ley_dir.glob(\"*.txt\") if p.is_file()])\n",
    "    for p in txt_files:\n",
    "        raw = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "        file_issues: List[Dict[str,Any]] = []\n",
    "\n",
    "        title, nodes, invalid_suffixes = parse_law_text(raw, file_issues)\n",
    "\n",
    "        # Validate article sequence AFTER auto-repair; only genuine jumps remain\n",
    "        jumps = validate_article_sequence(nodes, file_issues)\n",
    "        jump_rows.extend([{\n",
    "            \"file\": p.name,\n",
    "            **jr\n",
    "        } for jr in jumps])\n",
    "\n",
    "        # Save verbose errors per file\n",
    "        if errores_dir and file_issues:\n",
    "            save_errors_to_file(file_issues, p.name, errores_dir)\n",
    "\n",
    "        counts = count_unidades(nodes)\n",
    "\n",
    "        # Determine output JSON filename via catalogue (file_num -> law_name)\n",
    "        stem = p.stem  # expected like '0001'\n",
    "        law_name = catalog.get(stem, {}).get(\"law_name\") or inferred_title_from_file(p, title)\n",
    "\n",
    "        # Output JSON name = file_num.json (strict)\n",
    "        out_name = f\"{stem}.json\"\n",
    "        final_obj = {\n",
    "            \"ley\": law_name,\n",
    "            \"contenido\": [n.to_json_obj() for n in nodes]\n",
    "        }\n",
    "        out_json_path = out_dir / out_name\n",
    "        write_json(final_obj, out_json_path)\n",
    "\n",
    "        # Write invalid suffixes JSON for this file (only if there are any)\n",
    "        if invalid_suffixes:\n",
    "            invalid_path_base = errores_dir if errores_dir else out_dir\n",
    "            invalid_path = invalid_path_base / f\"{stem}_invalid_suffixes.json\"\n",
    "            write_json(invalid_suffixes, invalid_path)\n",
    "\n",
    "        # Collect issues to global CSV rows\n",
    "        for it in file_issues:\n",
    "            issues_rows.append({\n",
    "                \"file\": p.name,\n",
    "                \"location\": it.get(\"location\",\"\"),\n",
    "                \"issue_type\": it.get(\"issue_type\",\"warning\"),\n",
    "                \"message\": it.get(\"message\",\"\"),\n",
    "                \"line_text\": it.get(\"line_text\",\"\"),\n",
    "            })\n",
    "\n",
    "        manifest[\"processed_files\"].append(p.name)\n",
    "        manifest[\"files\"][p.name] = {\n",
    "            \"output\": out_json_path.name,\n",
    "            \"law_name\": law_name,\n",
    "            \"counts\": counts,\n",
    "            \"issues\": file_issues,\n",
    "        }\n",
    "        for k in counts:\n",
    "            manifest[\"totals\"][k] += counts[k]\n",
    "        manifest[\"warnings\"] += sum(1 for it in file_issues if it.get(\"issue_type\") == \"warning\")\n",
    "        manifest[\"errors\"]   += sum(1 for it in file_issues if it.get(\"issue_type\") == \"error\")\n",
    "\n",
    "    # Write manifest + CSVs\n",
    "    write_json(manifest, out_dir / \"manifest.json\")\n",
    "\n",
    "    if errores_dir:\n",
    "        # Parsing issues CSV (includes line text)\n",
    "        with (errores_dir / \"parsing_issues.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=[\"file\",\"location\",\"issue_type\",\"message\",\"line_text\"])\n",
    "            w.writeheader(); w.writerows(issues_rows)\n",
    "\n",
    "        # Article jumps CSV\n",
    "        with (errores_dir / \"articulo_jumps.csv\").open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"file\",\n",
    "                    \"prev_line\",\"prev_sufijo\",\"prev_line_text\",\n",
    "                    \"current_line\",\"current_sufijo\",\"current_line_text\",\n",
    "                    \"prev_base\",\"current_base\",\"delta\"\n",
    "                ]\n",
    "            )\n",
    "            w.writeheader(); w.writerows(jump_rows)\n",
    "\n",
    "    return manifest\n",
    "\n",
    "# ------------------------------ Error summary & summary -------------------------------\n",
    "\n",
    "def create_error_summary(manifest: Dict[str, Any], errores_dir: Path):\n",
    "    if not errores_dir:\n",
    "        return\n",
    "    summary_path = errores_dir / \"error_summary.txt\"\n",
    "    with summary_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"RESUMEN CONSOLIDADO DE ERRORES Y ADVERTENCIAS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Archivos procesados: {len(manifest.get('processed_files', []))}\\n\")\n",
    "        f.write(f\"Total de advertencias: {manifest.get('warnings', 0)}\\n\")\n",
    "        f.write(f\"Total de errores: {manifest.get('errors', 0)}\\n\\n\")\n",
    "        f.write(\"DETALLES POR ARCHIVO:\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for fname, info in manifest.get(\"files\", {}).items():\n",
    "            issues = info.get(\"issues\", [])\n",
    "            if issues:\n",
    "                f.write(f\"\\n📁 {fname}:\\n\")\n",
    "                for issue in issues:\n",
    "                    f.write(f\"  • {issue.get('location', 'N/A')}: \")\n",
    "                    f.write(f\"[{issue.get('issue_type', 'warning').upper()}] \")\n",
    "                    f.write(f\"{issue.get('message', '')}\\n\")\n",
    "                    if issue.get(\"line_text\"):\n",
    "                        f.write(f\"    Línea: {issue['line_text']}\\n\")\n",
    "\n",
    "        f.write(f\"\\n\\nArchivos de error individuales guardados en: {errores_dir}\\n\")\n",
    "\n",
    "def print_summary(manifest: Dict[str, Any]):\n",
    "    print(\"=\"*72)\n",
    "    print(\" TXT → JSON Parsing Summary\")\n",
    "    print(\"=\"*72)\n",
    "    print(f\" Files processed : {len(manifest.get('processed_files', []))}\")\n",
    "    t = manifest.get(\"totals\", {})\n",
    "    print(f\" Libros         : {t.get('libros',0)}\")\n",
    "    print(f\" Títulos        : {t.get('titulos',0)}\")\n",
    "    print(f\" Capítulos      : {t.get('capitulos',0)}\")\n",
    "    print(f\" Secciones      : {t.get('secciones',0)}\")\n",
    "    print(f\" Artículos      : {t.get('articulos',0)}\")\n",
    "    print(f\" Warnings       : {manifest.get('warnings',0)}\")\n",
    "    print(f\" Errors         : {manifest.get('errors',0)}\")\n",
    "    print(\"-\"*72)\n",
    "    for fname, info in manifest.get(\"files\", {}).items():\n",
    "        c = info.get(\"counts\", {})\n",
    "        n_warn = sum(1 for it in info.get(\"issues\",[]) if it.get(\"issue_type\") == \"warning\")\n",
    "        n_err  = sum(1 for it in info.get(\"issues\",[]) if it.get(\"issue_type\") == \"error\")\n",
    "        print(f\" {fname} → {info.get('output')} ({info.get('law_name','')})\")\n",
    "        print(f\"   L:{c.get('libros',0)} T:{c.get('titulos',0)} C:{c.get('capitulos',0)} S:{c.get('secciones',0)} A:{c.get('articulos',0)} | warn:{n_warn} err:{n_err}\")\n",
    "    print(\"=\"*72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92e9a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Processing: JSON Structure Generation\n",
      "================================================================================\n",
      "================================================================================\n",
      "PROCESSING COMPLETE - Generating Summary Reports\n",
      "================================================================================\n",
      "========================================================================\n",
      " TXT → JSON Parsing Summary\n",
      "========================================================================\n",
      " Files processed : 179\n",
      " Libros         : 15\n",
      " Títulos        : 515\n",
      " Capítulos      : 2048\n",
      " Secciones      : 409\n",
      " Artículos      : 17900\n",
      " Warnings       : 62\n",
      " Errors         : 0\n",
      "------------------------------------------------------------------------\n",
      " 0001.txt → 0001.json (Constitución Política del Estado de Nuevo León)\n",
      "   L:0 T:8 C:25 S:15 A:215 | warn:0 err:0\n",
      " 0002.txt → 0002.json (Código Civil para el Estado de Nuevo León)\n",
      "   L:4 T:43 C:194 S:0 A:3073 | warn:4 err:0\n",
      " 0003.txt → 0003.json (Código de Ética para el Congreso del Estado de Nuevo León)\n",
      "   L:0 T:0 C:7 S:5 A:38 | warn:0 err:0\n",
      " 0004.txt → 0004.json (Código de Procedimientos Civiles del Estado de Nuevo León)\n",
      "   L:4 T:18 C:73 S:4 A:1293 | warn:2 err:0\n",
      " 0005.txt → 0005.json (Código Fiscal del Estado de Nuevo León)\n",
      "   L:0 T:5 C:8 S:8 A:267 | warn:0 err:0\n",
      " 0006.txt → 0006.json (Código Penal para el Estado de Nuevo León)\n",
      "   L:2 T:24 C:104 S:0 A:598 | warn:3 err:0\n",
      " 0007.txt → 0007.json (Ley Ambiental del Estado de Nuevo León)\n",
      "   L:0 T:6 C:29 S:18 A:313 | warn:0 err:0\n",
      " 0008.txt → 0008.json (Ley  de Acceso de las Mujeres a una Vida Libre de Violencia)\n",
      "   L:0 T:0 C:10 S:0 A:92 | warn:0 err:0\n",
      " 0009.txt → 0009.json (Ley de Administración Financiera para el Estado de Nuevo Leon)\n",
      "   L:0 T:0 C:13 S:18 A:174 | warn:0 err:0\n",
      " 0010.txt → 0010.json (Ley de Adquisiciones, Arrendamientos y Contratación de Servicios del Estado de Nuevo León)\n",
      "   L:0 T:0 C:10 S:0 A:102 | warn:0 err:0\n",
      " 0011.txt → 0011.json (Ley de Agua Potable y Saneamiento para el Estado de Nuevo Leon)\n",
      "   L:0 T:0 C:13 S:0 A:78 | warn:0 err:0\n",
      " 0012.txt → 0012.json (Ley de Amnistía para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:12 | warn:0 err:0\n",
      " 0013.txt → 0013.json (Ley de Aparcería Agrícola del Estado de Nuevo Leon)\n",
      "   L:0 T:0 C:0 S:0 A:42 | warn:0 err:0\n",
      " 0014.txt → 0014.json (Ley de Archivos para el Estado de Nuevo León)\n",
      "   L:0 T:7 C:25 S:0 A:103 | warn:2 err:0\n",
      " 0015.txt → 0015.json (Ley de Asentamientos Humanos, Ordenamiento Territorial y Desarrollo Urbano para el Estado de Nuevo León)\n",
      "   L:0 T:12 C:58 S:37 A:432 | warn:7 err:0\n",
      " 0016.txt → 0016.json (Ley de Asociaciones Público Privadas para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:13 S:24 A:130 | warn:0 err:0\n",
      " 0017.txt → 0017.json (Ley de Cambio Climático del Estado de Nuevo León)\n",
      "   L:0 T:0 C:14 S:3 A:58 | warn:2 err:0\n",
      " 0018.txt → 0018.json (Ley de Ciencia, Tecnología e Innovación del Estado de Nuevo León)\n",
      "   L:0 T:3 C:10 S:0 A:47 | warn:0 err:0\n",
      " 0019.txt → 0019.json (Ley de Coordinación Hacendaria del Estado de Nuevo León)\n",
      "   L:0 T:5 C:7 S:0 A:39 | warn:0 err:0\n",
      " 0020.txt → 0020.json (Ley de Copropiedades Rurales)\n",
      "   L:0 T:0 C:7 S:0 A:44 | warn:0 err:0\n",
      " 0021.txt → 0021.json (Ley de Defensoría Pública para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:8 A:45 | warn:1 err:0\n",
      " 0022.txt → 0022.json (Ley de Desarrollo Forestal Sustentable del Estado de Nuevo León)\n",
      "   L:0 T:9 C:25 S:11 A:175 | warn:3 err:0\n",
      " 0023.txt → 0023.json (Ley de Desarrollo Rural Integral Sustentable del Estado de Nuevo León)\n",
      "   L:0 T:10 C:10 S:0 A:121 | warn:1 err:0\n",
      " 0024.txt → 0024.json (Ley de Desarrollo Social para el Estado de Nuevo León)\n",
      "   L:0 T:7 C:9 S:0 A:59 | warn:0 err:0\n",
      " 0025.txt → 0025.json (Ley de Educación del Estado)\n",
      "   L:0 T:0 C:8 S:23 A:144 | warn:1 err:0\n",
      " 0026.txt → 0026.json (Ley de Egresos del Estado de Nuevo León para el Ejercicio Fiscal 2023)\n",
      "   L:0 T:5 C:9 S:0 A:107 | warn:0 err:0\n",
      " 0027.txt → 0027.json (Ley de Emergencia Policial, Reglamentaria de la Fracción XVIII del Artículo 85 de la Constitución Política del Estado Libre y Soberano de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:0 | warn:0 err:0\n",
      " 0028.txt → 0028.json (Ley de Entrega Recepción para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:4 S:0 A:29 | warn:0 err:0\n",
      " 0029.txt → 0029.json (Ley de Expropiación por Causa de Utilidad Pública)\n",
      "   L:0 T:0 C:0 S:0 A:20 | warn:0 err:0\n",
      " 0030.txt → 0030.json (Ley de Fiscalización Superior del Estado de Nuevo León)\n",
      "   L:0 T:7 C:19 S:4 A:109 | warn:0 err:0\n",
      " 0031.txt → 0031.json (Ley de Fomento a la Inversión y al Empleo para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:7 S:0 A:61 | warn:0 err:0\n",
      " 0032.txt → 0032.json (Ley de Fomento a la Micro, Pequeña y Mediana Empresa para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:0 A:40 | warn:0 err:0\n",
      " 0033.txt → 0033.json (Ley de Fomento a las Actividades  Agropecuarias del Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:8 | warn:0 err:0\n",
      " 0034.txt → 0034.json (Ley de Fomento al Turismo del Estado de Nuevo León)\n",
      "   L:0 T:4 C:11 S:0 A:73 | warn:0 err:0\n",
      " 0035.txt → 0035.json (Ley de Fomento de la Sociedad Civil Organizada para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:9 S:0 A:47 | warn:1 err:0\n",
      " 0036.txt → 0036.json (Ley de Fomento para la Construcción de Edificios de Estacionamiento de Vehículos)\n",
      "   L:0 T:0 C:0 S:0 A:8 | warn:0 err:0\n",
      " 0037.txt → 0037.json (Ley de Gobierno Municipal del Estado de Nuevo León)\n",
      "   L:0 T:11 C:43 S:6 A:240 | warn:0 err:0\n",
      " 0038.txt → 0038.json (Ley de Hacienda del Estado de Nuevo León)\n",
      "   L:0 T:4 C:8 S:14 A:321 | warn:1 err:0\n",
      " 0039.txt → 0039.json (Ley de Hacienda para los Municipios del Estado de Nuevo León)\n",
      "   L:0 T:5 C:4 S:0 A:173 | warn:0 err:0\n",
      " 0040.txt → 0040.json (Ley de Indulto para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:0 | warn:0 err:0\n",
      " 0041.txt → 0041.json (Ley de Infraestructura Física Educativa y Deportiva de Nuevo León)\n",
      "   L:0 T:0 C:6 S:0 A:35 | warn:0 err:0\n",
      " 0044.txt → 0044.json (Ley de Instituciones Asistenciales Públicas y Privadas para las Personas Adultas Mayores en el Estado de Nuevo León)\n",
      "   L:0 T:7 C:9 S:0 A:40 | warn:0 err:0\n",
      " 0045.txt → 0045.json (Ley de Instituciones Asistenciales que Tienen bajo su Guarda, Custodia o Ambas a Niñas, Niños y Adolescentes en el Estado de Nuevo León)\n",
      "   L:0 T:9 C:7 S:0 A:44 | warn:0 err:0\n",
      " 0046.txt → 0046.json (Ley de Juicio Político del Estado de Nuevo León)\n",
      "   L:0 T:2 C:2 S:0 A:32 | warn:0 err:0\n",
      " 0047.txt → 0047.json (Ley de Justicia Administrativa para el Estado y Municipios de Nuevo León)\n",
      "   L:0 T:2 C:21 S:0 A:208 | warn:1 err:0\n",
      " 0048.txt → 0048.json (Ley de Justicia Cívica para el Estado de Nuevo León)\n",
      "   L:0 T:11 C:23 S:0 A:92 | warn:0 err:0\n",
      " 0049.txt → 0049.json (Ley de la Beneficencia Privada para el Estado de Nuevo León)\n",
      "   L:0 T:7 C:7 S:0 A:130 | warn:0 err:0\n",
      " 0050.txt → 0050.json (Ley de la Comisión Estatal de Derechos Humanos del Estado de Nuevo León)\n",
      "   L:0 T:6 C:13 S:0 A:72 | warn:1 err:0\n",
      " 0051.txt → 0051.json (Ley de la Corporación para el Desarrollo Agropecuario de Nuevo León)\n",
      "   L:0 T:0 C:9 S:0 A:29 | warn:0 err:0\n",
      " 0052.txt → 0052.json (Ley de la Corporación para el Desarrollo Turístico de Nuevo León)\n",
      "   L:0 T:0 C:9 S:0 A:29 | warn:0 err:0\n",
      " 0053.txt → 0053.json (Ley de la Institución Policial Estatal Fuerza Civil)\n",
      "   L:0 T:0 C:4 S:0 A:44 | warn:1 err:0\n",
      " 0054.txt → 0054.json (Ley de la Juventud para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:0 A:48 | warn:0 err:0\n",
      " 0055.txt → 0055.json (Ley de los Derechos de las Personas Adultas Mayores en el Estado de Nuevo León)\n",
      "   L:0 T:8 C:20 S:0 A:70 | warn:0 err:0\n",
      " 0056.txt → 0056.json (Ley de los Derechos de las Personas Indígenas y Afromexicanas en el Estado de Nuevo León)\n",
      "   L:0 T:1 C:7 S:0 A:49 | warn:1 err:0\n",
      " 0057.txt → 0057.json (Ley de los Derechos de Niñas, Niños y Adolescentes para el Estado de Nuevo León)\n",
      "   L:0 T:7 C:31 S:0 A:203 | warn:0 err:0\n",
      " 0058.txt → 0058.json (Ley de Mecanismos Alternativos para la Solución de Controversias para el Estado de Nuevo León)\n",
      "   L:0 T:6 C:6 S:0 A:55 | warn:0 err:0\n",
      " 0059.txt → 0059.json (Ley de Movilidad Sostenible, de Accesibilidad y Seguridad Vial para el Estado de Nuevo León)\n",
      "   L:0 T:11 C:24 S:13 A:282 | warn:2 err:0\n",
      " 0060.txt → 0060.json (Ley de Nomenclatura del Estado y Municipios de Nuevo León)\n",
      "   L:0 T:0 C:4 S:0 A:7 | warn:0 err:0\n",
      " 0061.txt → 0061.json (Ley de Obras Públicas para el Estado y Municipios de Nuevo León)\n",
      "   L:0 T:6 C:11 S:0 A:129 | warn:0 err:0\n",
      " 0062.txt → 0062.json (Ley de Participación Ciudadana para el Estado de Nuevo León)\n",
      "   L:5 T:6 C:16 S:11 A:127 | warn:0 err:0\n",
      " 0063.txt → 0063.json (Ley de Pensiones, Seguro de Vida y Otros Beneficios a los Veteranos de la Revolución)\n",
      "   L:0 T:0 C:0 S:0 A:9 | warn:0 err:0\n",
      " 0064.txt → 0064.json (Ley de Planeación Estratégica del Estado de Nuevo León)\n",
      "   L:0 T:0 C:7 S:0 A:31 | warn:0 err:0\n",
      " 0065.txt → 0065.json (Ley de Prestación de Servicios para la Atención, Cuidado y Desarrollo Integral Infantil del Estado de Nuevo León)\n",
      "   L:0 T:0 C:16 S:0 A:79 | warn:0 err:0\n",
      " 0066.txt → 0066.json (Ley de Prevención Social de la Violencia y la Delincuencia con Participación Ciudadana del Estado de Nuevo León)\n",
      "   L:0 T:4 C:10 S:0 A:70 | warn:1 err:0\n",
      " 0067.txt → 0067.json (Ley de Prevención y Atención Integral de la Violencia Familiar en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:4 S:0 A:34 | warn:0 err:0\n",
      " 0068.txt → 0068.json (Ley de Profesiones del Estado de Nuevo León)\n",
      "   L:0 T:0 C:11 S:3 A:66 | warn:1 err:0\n",
      " 0069.txt → 0069.json (Ley de Propiedad en Condominio de Inmuebles para el Estado de Nuevo León)\n",
      "   L:0 T:8 C:11 S:0 A:64 | warn:0 err:0\n",
      " 0070.txt → 0070.json (Ley de Protección a la Salud Bucal para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:13 | warn:0 err:0\n",
      " 0071.txt → 0071.json (Ley de Protección al Parto Humanizado y a la Maternidad Digna del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:2 A:44 | warn:1 err:0\n",
      " 0072.txt → 0072.json (Ley de Protección Civil para el Estado de Nuevo León)\n",
      "   L:0 T:2 C:15 S:0 A:122 | warn:0 err:0\n",
      " 0073.txt → 0073.json (Ley de Protección contra Incendios y Materiales Peligrosos del Estado de Nuevo León)\n",
      "   L:0 T:0 C:3 S:0 A:10 | warn:0 err:0\n",
      " 0074.txt → 0074.json (Ley de Protección contra la Exposición al Humo del Tabaco del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:7 A:58 | warn:0 err:0\n",
      " 0075.txt → 0075.json (Ley de Protección de Datos Personales en Posesión de Sujetos Obligados del Estado de Nuevo León)\n",
      "   L:0 T:11 C:11 S:0 A:180 | warn:1 err:0\n",
      " 0076.txt → 0076.json (Ley de Protección y Bienestar Animal para la Sustentabilidad del Estado de Nuevo León)\n",
      "   L:0 T:0 C:17 S:0 A:165 | warn:0 err:0\n",
      " 0077.txt → 0077.json (Ley de Protección y Fomento Apícola del Estado de Nuevo León)\n",
      "   L:0 T:0 C:15 S:0 A:99 | warn:0 err:0\n",
      " 0078.txt → 0078.json (Ley de Remuneraciones de los Servidores Públicos del Estado de Nuevo León)\n",
      "   L:0 T:0 C:4 S:2 A:39 | warn:0 err:0\n",
      " 0079.txt → 0079.json (Ley de Responsabilidad Patrimonial del Estado y Municipios de Nuevo León)\n",
      "   L:0 T:0 C:6 S:0 A:36 | warn:0 err:0\n",
      " 0080.txt → 0080.json (Ley de Responsabilidades Administrativas del Estado de Nuevo León)\n",
      "   L:0 T:4 C:16 S:23 A:227 | warn:2 err:0\n",
      " 0081.txt → 0081.json (Ley de Salud Mental para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:11 S:0 A:110 | warn:1 err:0\n",
      " 0082.txt → 0082.json (Ley de Seguridad Privada para el Estado de Nuevo León)\n",
      "   L:0 T:6 C:14 S:0 A:48 | warn:0 err:0\n",
      " 0083.txt → 0083.json (Ley de Seguridad Pública para el Estado de Nuevo León)\n",
      "   L:0 T:10 C:42 S:17 A:302 | warn:0 err:0\n",
      " 0084.txt → 0084.json (Ley de Señalamientos Viales para el Estado de Nuevo León)\n",
      "   L:0 T:2 C:6 S:0 A:20 | warn:0 err:0\n",
      " 0085.txt → 0085.json (Ley de Sociedades Mutualistas del Estado de Nuevo León)\n",
      "   L:0 T:0 C:2 S:0 A:33 | warn:0 err:0\n",
      " 0086.txt → 0086.json (Ley de Transparencia y Acceso a la Información Pública del Estado de Nuevo León)\n",
      "   L:0 T:9 C:28 S:0 A:210 | warn:0 err:0\n",
      " 0087.txt → 0087.json (Ley de Víctimas del Estado de Nuevo León)\n",
      "   L:0 T:6 C:14 S:11 A:124 | warn:0 err:0\n",
      " 0088.txt → 0088.json (Ley del Catastro)\n",
      "   L:0 T:0 C:4 S:0 A:39 | warn:0 err:0\n",
      " 0089.txt → 0089.json (Ley del Derecho a la Alimentación Adecuada y Combate contra el Desperdicio de Alimentos para el Estado de Nuevo León)\n",
      "   L:0 T:4 C:11 S:0 A:37 | warn:0 err:0\n",
      " 0090.txt → 0090.json (Ley del Instituto de Evaluación Educativa de Nuevo León)\n",
      "   L:0 T:0 C:4 S:4 A:28 | warn:0 err:0\n",
      " 0091.txt → 0091.json (Ley del Instituto de la Vivienda de Nuevo León)\n",
      "   L:0 T:0 C:4 S:3 A:25 | warn:0 err:0\n",
      " 0092.txt → 0092.json (Ley del Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado de Nuevo León)\n",
      "   L:0 T:11 C:24 S:4 A:207 | warn:0 err:0\n",
      " 0093.txt → 0093.json (Ley del Instituto Estatal de Cultura Física y Deporte)\n",
      "   L:0 T:0 C:4 S:4 A:30 | warn:0 err:0\n",
      " 0094.txt → 0094.json (Ley del Instituto Estatal de la Juventud)\n",
      "   L:0 T:0 C:4 S:4 A:30 | warn:0 err:0\n",
      " 0095.txt → 0095.json (Ley del Instituto Estatal de las Mujeres)\n",
      "   L:0 T:0 C:5 S:5 A:34 | warn:0 err:0\n",
      " 0096.txt → 0096.json (Ley del Instituto Estatal de Seguridad Pública)\n",
      "   L:0 T:6 C:8 S:0 A:21 | warn:0 err:0\n",
      " 0097.txt → 0097.json (Ley del Instituto Registral y Catastral del Estado de Nuevo León)\n",
      "   L:0 T:5 C:3 S:0 A:31 | warn:0 err:0\n",
      " 0098.txt → 0098.json (Ley del Notariado del Estado de Nuevo León)\n",
      "   L:0 T:4 C:16 S:0 A:190 | warn:0 err:0\n",
      " 0099.txt → 0099.json (Ley del Organismo Público Descentralizado Denominado Sistema Integral para el Manejo Ecológico y Procesamiento de Desechos (SIMEPRODE))\n",
      "   L:0 T:0 C:1 S:0 A:19 | warn:0 err:0\n",
      " 0100.txt → 0100.json (Ley del Patrimonio Cultural del Estado de Nuevo León)\n",
      "   L:0 T:0 C:11 S:0 A:87 | warn:0 err:0\n",
      " 0101.txt → 0101.json (Ley del Periódico Oficial del Estado de Nuevo León)\n",
      "   L:0 T:0 C:4 S:0 A:26 | warn:0 err:0\n",
      " 0102.txt → 0102.json (Ley del Registro Civil para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:12 S:0 A:72 | warn:2 err:0\n",
      " 0103.txt → 0103.json (Ley del Servicio Civil del Estado de Nuevo León)\n",
      "   L:0 T:6 C:15 S:0 A:121 | warn:0 err:0\n",
      " 0104.txt → 0104.json (Ley del Servicio Profesional de Carrera de la Auditoría Superior del Estado de Nuevo León)\n",
      "   L:0 T:3 C:7 S:0 A:78 | warn:0 err:0\n",
      " 0105.txt → 0105.json (Ley del Sistema Estatal Anticorrupción para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:1 S:0 A:55 | warn:0 err:0\n",
      " 0106.txt → 0106.json (Ley Electoral para el Estado de Nuevo León)\n",
      "   L:0 T:10 C:42 S:19 A:401 | warn:0 err:0\n",
      " 0107.txt → 0107.json (Ley en Materia de Desaparición y Búsqueda de Personas para el Estado de Nuevo León)\n",
      "   L:0 T:5 C:12 S:1 A:96 | warn:0 err:0\n",
      " 0108.txt → 0108.json (Ley Estatal de Salud)\n",
      "   L:0 T:4 C:16 S:0 A:197 | warn:3 err:0\n",
      " 0109.txt → 0109.json (Ley Estatal del Deporte)\n",
      "   L:0 T:0 C:9 S:0 A:92 | warn:0 err:0\n",
      " 0110.txt → 0110.json (Ley Ganadera del Estado de Nuevo León)\n",
      "   L:0 T:10 C:24 S:0 A:191 | warn:0 err:0\n",
      " 0111.txt → 0111.json (Ley Orgánica de la Administración Pública para el Estado de Nuevo León)\n",
      "   L:0 T:6 C:3 S:0 A:55 | warn:0 err:0\n",
      " 0112.txt → 0112.json (Ley Orgánica de la Fiscalía General de Justicia del Estado de Nuevo León)\n",
      "   L:0 T:0 C:15 S:0 A:84 | warn:0 err:0\n",
      " 0113.txt → 0113.json (Ley Orgánica de la UNIVERSIDAD Autónoma de Nuevo León)\n",
      "   L:0 T:7 C:6 S:0 A:43 | warn:0 err:0\n",
      " 0114.txt → 0114.json (Ley Orgánica del Centro de Conciliación Laboral del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:3 A:55 | warn:0 err:0\n",
      " 0115.txt → 0115.json (Ley Orgánica del Hospital Universitario \"Dr . José Eleuterio González\")\n",
      "   L:0 T:0 C:0 S:0 A:14 | warn:0 err:0\n",
      " 0116.txt → 0116.json (Ley Orgánica del Poder Judicial del Estado de Nuevo León)\n",
      "   L:0 T:10 C:24 S:8 A:185 | warn:0 err:0\n",
      " 0117.txt → 0117.json (Ley Orgánica del Poder Legislativo del Estado de Nuevo León)\n",
      "   L:0 T:10 C:12 S:0 A:105 | warn:0 err:0\n",
      " 0118.txt → 0118.json (Ley Orgánica que Crea la Escuela Normal Superior del Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:0 | warn:0 err:0\n",
      " 0119.txt → 0119.json (Ley para el Impulso, Desarrollo y Promoción de la Industria Cinematográfica y Audiovisual del Estado de Nuevo León)\n",
      "   L:0 T:0 C:10 S:4 A:47 | warn:1 err:0\n",
      " 0120.txt → 0120.json (Ley para el Reconocimiento al Mérito Cívico \"Presea Estado de Nuevo León\")\n",
      "   L:0 T:0 C:6 S:0 A:35 | warn:0 err:0\n",
      " 0121.txt → 0121.json (Ley para Incentivar la Denuncia de Actos de Corrupción de Servidores Públicos del Estado de Nuevo León)\n",
      "   L:0 T:0 C:7 S:0 A:29 | warn:0 err:0\n",
      " 0122.txt → 0122.json (Ley para la Administración de Bienes Asegurados, Decomisados o Abandonados del Estado de Nuevo León)\n",
      "   L:0 T:0 C:8 S:0 A:27 | warn:0 err:0\n",
      " 0123.txt → 0123.json (Ley para la Atención, Protección e Inclusión de las Personas con la Condición del Espectro Autista y Otras Condiciones de la NEURODIVERSIDAD para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:3 A:16 | warn:0 err:0\n",
      " 0124.txt → 0124.json (Ley para la Conservación y Protección del Arbolado Urbano del Estado de Nuevo León)\n",
      "   L:0 T:0 C:15 S:0 A:79 | warn:0 err:0\n",
      " 0125.txt → 0125.json (Ley para la Construcción y Rehabilitación de Pavimentos del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:13 A:119 | warn:3 err:0\n",
      " 0126.txt → 0126.json (Ley para la Detección y Tratamiento Oportuno e Integral del Cáncer en la Infancia y la Adolescencia del Estado de Nuevo León)\n",
      "   L:0 T:7 C:13 S:0 A:44 | warn:0 err:0\n",
      " 0127.txt → 0127.json (Ley para la Igualdad entre Mujeres y Hombres del Estado de Nuevo León)\n",
      "   L:0 T:6 C:24 S:0 A:79 | warn:1 err:0\n",
      " 0128.txt → 0128.json (Ley para la Integración del Acervo Bibliográfico en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:9 S:0 A:21 | warn:0 err:0\n",
      " 0129.txt → 0129.json (Ley para la Mejora Regulatoria y la Simplificación Administrativa del Estado de Nuevo León)\n",
      "   L:0 T:6 C:18 S:6 A:104 | warn:1 err:0\n",
      " 0130.txt → 0130.json (Ley para la Prevención y Combate al Abuso del Alcohol y de Regulación para su Venta y Consumo para el Estado de Nuevo León)\n",
      "   L:0 T:7 C:14 S:0 A:115 | warn:0 err:0\n",
      " 0131.txt → 0131.json (Ley para la Promoción de Valores y Cultura de la Legalidad del Estado de Nuevo León)\n",
      "   L:0 T:0 C:2 S:0 A:24 | warn:0 err:0\n",
      " 0132.txt → 0132.json (Ley para la Protección, Apoyo y Promoción de la Lactancia Materna del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:2 A:30 | warn:1 err:0\n",
      " 0133.txt → 0133.json (Ley para la Protección de los Derechos de las Personas con Discapacidad)\n",
      "   L:0 T:0 C:19 S:0 A:77 | warn:0 err:0\n",
      " 0134.txt → 0134.json (Ley para la Protección de Personas que Intervienen en el Procedimiento Penal del Estado de Nuevo León)\n",
      "   L:0 T:3 C:0 S:0 A:29 | warn:0 err:0\n",
      " 0135.txt → 0135.json (Ley para Prevenir, Atender, Combatir y Erradicar la Trata de Personas en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:0 A:28 | warn:0 err:0\n",
      " 0136.txt → 0136.json (Ley para Prevenir, Atender y Erradicar el Acoso y la Violencia Escolar del Estado de Nuevo León)\n",
      "   L:0 T:2 C:13 S:0 A:77 | warn:0 err:0\n",
      " 0137.txt → 0137.json (Ley para Prevenir la Obesidad y el Sobrepeso en el Estado y Municipios de Nuevo León)\n",
      "   L:0 T:0 C:5 S:0 A:30 | warn:0 err:0\n",
      " 0138.txt → 0138.json (Ley para Prevenir y Eliminar la Discriminación en el Estado de Nuevo León)\n",
      "   L:0 T:5 C:12 S:5 A:102 | warn:1 err:0\n",
      " 0139.txt → 0139.json (Ley para Regular el Acceso Vial y Mejorar la Seguridad de los Vecinos en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:13 | warn:0 err:0\n",
      " 0140.txt → 0140.json (Ley para Regular el Uso de la Vía Pública en el Ejercicio de la Actividad Comercial)\n",
      "   L:0 T:0 C:4 S:0 A:21 | warn:0 err:0\n",
      " 0141.txt → 0141.json (Ley que Crea al Organismo Público Descentralizado Denominado Instituto del Agua del Estado de Nuevo León)\n",
      "   L:0 T:0 C:7 S:0 A:22 | warn:0 err:0\n",
      " 0142.txt → 0142.json (Ley que Crea al Organismo Público Descentralizado Denominado Parque Fundidora)\n",
      "   L:0 T:0 C:7 S:0 A:23 | warn:0 err:0\n",
      " 0143.txt → 0143.json (Ley que Crea al Organismo Público Descentralizado Denominado Parques y Vida Silvestre de Nuevo León)\n",
      "   L:0 T:0 C:4 S:4 A:30 | warn:0 err:0\n",
      " 0144.txt → 0144.json (Ley que Crea el Colegio de Educación Profesional Técnica del Estado de Nuevo León)\n",
      "   L:0 T:0 C:5 S:0 A:22 | warn:0 err:0\n",
      " 0145.txt → 0145.json (Ley que Crea el Colegio Militarizado \"General Mariano Escobedo\" del Estado de Nuevo León)\n",
      "   L:0 T:5 C:12 S:0 A:48 | warn:0 err:0\n",
      " 0146.txt → 0146.json (Ley que Crea el Consejo Estatal de Adopciones)\n",
      "   L:0 T:0 C:0 S:0 A:12 | warn:0 err:0\n",
      " 0147.txt → 0147.json (Ley que Crea el Consejo para la Cultura y las Artes de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:18 | warn:1 err:0\n",
      " 0148.txt → 0148.json (Ley que Crea el Instituto de Capacitación y Educación para el Trabajo del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:0 A:24 | warn:0 err:0\n",
      " 0149.txt → 0149.json (Ley que Crea el Instituto de Control Vehicular del Estado de Nuevo León)\n",
      "   L:0 T:0 C:9 S:0 A:48 | warn:0 err:0\n",
      " 0150.txt → 0150.json (Ley que Crea el Instituto de Investigación, Innovación y Estudios de Posgrado para la Educación del Estado de Nuevo León)\n",
      "   L:0 T:4 C:6 S:0 A:20 | warn:0 err:0\n",
      " 0151.txt → 0151.json (Ley que Crea el Organismo Público Descentralizado de Participación Ciudadana Denominado \"Corporación para el Desarrollo de la Zona Fronteriza de Nuevo León\")\n",
      "   L:0 T:0 C:9 S:0 A:27 | warn:0 err:0\n",
      " 0152.txt → 0152.json (Ley que Crea el Organismo Público Descentralizado Denominado Colegio de Estudios Científicos y Tecnológicos del Estado de Nuevo León, CECYTENL)\n",
      "   L:0 T:0 C:1 S:0 A:17 | warn:0 err:0\n",
      " 0153.txt → 0153.json (Ley que Crea el Organismo Público Descentralizado Denominado Museo de Historia Mexicana)\n",
      "   L:0 T:0 C:5 S:0 A:16 | warn:0 err:0\n",
      " 0154.txt → 0154.json (Ley que Crea el Organismo Público Descentralizado Denominado \"Red Estatal de Autopistas de Nuevo León\")\n",
      "   L:0 T:0 C:4 S:0 A:18 | warn:0 err:0\n",
      " 0155.txt → 0155.json (Ley que Crea el Organismo Público Descentralizado Denominado \"Sistema de Caminos de Nuevo León\")\n",
      "   L:0 T:0 C:4 S:0 A:20 | warn:0 err:0\n",
      " 0156.txt → 0156.json (Ley que Crea el Organismo Público Descentralizado Denominado Sistema de Radio y Televisión de Nuevo León)\n",
      "   L:0 T:0 C:7 S:0 A:32 | warn:1 err:0\n",
      " 0157.txt → 0157.json (Ley que Crea el Organismo Público Descentralizado Denominado Sistema de Transporte Colectivo \"Metrorrey\")\n",
      "   L:0 T:0 C:3 S:0 A:17 | warn:0 err:0\n",
      " 0158.txt → 0158.json (Ley que Crea el Organismo Público Descentralizado \"Operadora de Servicios Turísticos de Nuevo León\")\n",
      "   L:0 T:0 C:0 S:0 A:0 | warn:0 err:0\n",
      " 0159.txt → 0159.json (Ley que Crea el Organismo Público Descentralizado Servicios de Salud de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:11 | warn:0 err:0\n",
      " 0160.txt → 0160.json (Ley que Crea el Registro Estatal de Asesores Inmobiliarios del Estado de Nuevo León)\n",
      "   L:0 T:3 C:10 S:0 A:45 | warn:0 err:0\n",
      " 0161.txt → 0161.json (Ley que Crea la Escuela para Padres, Madres o Quienes Ejerzan la Tutela, Guarda o Custodia del Estado de Nuevo León)\n",
      "   L:0 T:0 C:3 S:0 A:23 | warn:0 err:0\n",
      " 0162.txt → 0162.json (Ley que Crea la Medalla de Honor \"Fray Servando Teresa de Mier\" del H. Congreso del Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:37 | warn:1 err:0\n",
      " 0163.txt → 0163.json (Ley que Crea la Unidad de Integración Educativa de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:0 | warn:0 err:0\n",
      " 0164.txt → 0164.json (Ley que Crea la UNIVERSIDAD de Ciencias de la Seguridad del Estado de Nuevo León)\n",
      "   L:0 T:0 C:6 S:7 A:40 | warn:0 err:0\n",
      " 0165.txt → 0165.json (Ley que Crea la UNIVERSIDAD Gral. Mariano Escobedo)\n",
      "   L:0 T:0 C:7 S:4 A:30 | warn:0 err:0\n",
      " 0166.txt → 0166.json (Ley que Crea la UNIVERSIDAD Politécnica de Apodaca)\n",
      "   L:0 T:4 C:10 S:0 A:42 | warn:1 err:0\n",
      " 0167.txt → 0167.json (Ley que Crea la UNIVERSIDAD Politécnica de García)\n",
      "   L:0 T:4 C:8 S:0 A:43 | warn:0 err:0\n",
      " 0168.txt → 0168.json (Ley que Crea la UNIVERSIDAD Tecnológica Bilingüe Franco Mexicana de Nuevo León)\n",
      "   L:0 T:4 C:10 S:0 A:43 | warn:0 err:0\n",
      " 0169.txt → 0169.json (Ley que Crea la UNIVERSIDAD Tecnológica Cadereyta)\n",
      "   L:0 T:0 C:6 S:6 A:30 | warn:0 err:0\n",
      " 0170.txt → 0170.json (Ley que Crea la UNIVERSIDAD Tecnológica Linares)\n",
      "   L:0 T:0 C:6 S:6 A:30 | warn:0 err:0\n",
      " 0171.txt → 0171.json (Ley que Crea la UNIVERSIDAD Tecnológica Santa Catarina)\n",
      "   L:0 T:0 C:7 S:4 A:30 | warn:0 err:0\n",
      " 0172.txt → 0172.json (Ley que Crea las Juntas de Mejoramiento Moral, Cívico y Material en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:17 | warn:0 err:0\n",
      " 0173.txt → 0173.json (Ley que Crea una Institución Pública Descentralizada con Personalidad Jurídica Propia y con Domicilio en la Ciudad de Monterrey que se Denominará \"Servicios de Agua y Drenaje de Monterrey\")\n",
      "   L:0 T:0 C:0 S:0 A:19 | warn:0 err:0\n",
      " 0174.txt → 0174.json (Ley que Regula el Procedimiento de Emisión de la Declaratoria de Ausencia por Desaparición en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:0 S:0 A:32 | warn:3 err:0\n",
      " 0175.txt → 0175.json (Ley que Regula el Uso de Vehículos Recreativos Todo Terreno en el Estado de Nuevo León)\n",
      "   L:0 T:0 C:11 S:0 A:62 | warn:1 err:0\n",
      " 0176.txt → 0176.json (Ley que Regula la Expedición de Licencias para Conducir del Estado de Nuevo León)\n",
      "   L:0 T:0 C:3 S:0 A:31 | warn:0 err:0\n",
      " 0177.txt → 0177.json (Ley que Regula las Características, Uso y Difusión del Escudo del Estado de Nuevo León)\n",
      "   L:0 T:0 C:3 S:0 A:12 | warn:0 err:0\n",
      " 0178.txt → 0178.json (Ley Reglamentaria del Artículo 95 de la Constitución Política del Estado de Nuevo León)\n",
      "   L:0 T:3 C:14 S:3 A:73 | warn:0 err:0\n",
      " 0179.txt → 0179.json (Ley Reglamentaria del Registro Público de la Propiedad y del Comercio para el Estado de Nuevo León)\n",
      "   L:0 T:0 C:21 S:0 A:77 | warn:0 err:0\n",
      " 0180.txt → 0180.json (Ley Sobre el Sistema Estatal de Asistencia Social del Estado de Nuevo León)\n",
      "   L:0 T:0 C:3 S:0 A:41 | warn:0 err:0\n",
      " 0181.txt → 0181.json (Ley Sobre Gobierno Electrónico y Fomento al Uso de las Tecnologías de la Información del Estado)\n",
      "   L:0 T:7 C:18 S:0 A:76 | warn:0 err:0\n",
      "========================================================================\n",
      "\n",
      " **Pipeline Execution Complete!**\n",
      " JSON files: c:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\json\n",
      " Error reports: c:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\errores\n",
      " Processing manifest: c:\\Users\\Edu\\OneDrive\\1.EmbeddingExploration\\1.Pipelines\\2.Processing\\Leyes\\19\\Refined\\json\\manifest.json\n",
      "\n",
      " **Final Results:**\n",
      "    Total articles parsed: 17,900\n",
      "    Warnings: 62\n",
      "    Errors: 0\n",
      "    Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# ============== FINAL PROCESSING: JSON GENERATION & VALIDATION ==============\n",
    "\n",
    "print(\"Starting Final Processing: JSON Structure Generation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute the complete processing pipeline\n",
    "# - Reads law text files from LEY_DIR  \n",
    "# - Parses hierarchical structure (libros, títulos, capítulos, secciones, artículos)\n",
    "# - Validates article sequences and structural integrity\n",
    "# - Generates structured JSON output with comprehensive metadata\n",
    "# - Creates detailed error reports and validation logs\n",
    "\n",
    "manifest = walk_and_process(\n",
    "    ley_dir=LEY_DIR,           # Input: Cleaned law text files\n",
    "    out_dir=JSON_DIR,          # Output: Structured JSON files\n",
    "    errores_dir=ERRORES_DIR,   # Logs: Error reports and validation\n",
    "    catalog_csv=CATALOG_CSV    # Metadata: Law names and references\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING COMPLETE - Generating Summary Reports\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display comprehensive processing summary\n",
    "print_summary(manifest)\n",
    "\n",
    "# Generate consolidated error summary for quality review\n",
    "create_error_summary(manifest, ERRORES_DIR)\n",
    "\n",
    "print(\"\\n **Pipeline Execution Complete!**\")\n",
    "print(f\" JSON files: {JSON_DIR}\")\n",
    "print(f\" Error reports: {ERRORES_DIR}\")\n",
    "print(f\" Processing manifest: {JSON_DIR / 'manifest.json'}\")\n",
    "\n",
    "# Display final statistics\n",
    "total_articles = manifest.get(\"totals\", {}).get(\"articulos\", 0)\n",
    "total_warnings = manifest.get(\"warnings\", 0)\n",
    "total_errors = manifest.get(\"errors\", 0)\n",
    "\n",
    "print(f\"\\n **Final Results:**\")\n",
    "print(f\"    Total articles parsed: {total_articles:,}\")\n",
    "print(f\"    Warnings: {total_warnings}\")\n",
    "print(f\"    Errors: {total_errors}\")\n",
    "print(f\"    Success rate: {((total_articles - total_errors) / max(total_articles, 1) * 100):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
